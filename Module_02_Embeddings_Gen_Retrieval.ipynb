{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a1d247-cb22-4115-89c1-e5fbdc0dc633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import ray\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f00640-1a22-492a-85eb-638f8f30c1e1",
   "metadata": {},
   "source": [
    "# Generating, storing, and retrieving embeddings with Ray Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b2a18d-abf3-4342-894e-8154fdba2a92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"hkunlp/instructor-large\"\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d5a05e-1f74-479f-a63c-bd14783c697b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = [\"What are some top attractions in Seattle?\", \n",
    "         \"What are some top attractions in Los Angeles?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f6bfa41-1a60-42da-bc56-48006ac2a86c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = model.encode(items)\n",
    "\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00820505-d734-40e8-aeb9-ee02db88c55c",
   "metadata": {},
   "source": [
    "We move the data to shared storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a090751-a222-4f7a-b20b-af586fc777fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! cp around.txt /mnt/cluster_storage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8314c12-e9ed-4fd4-a146-8120c19086fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 20:57:41,919\tINFO worker.py:1821 -- Connecting to existing Ray cluster at address: 10.0.142.230:6379...\n",
      "2026-01-20 20:57:41,932\tINFO worker.py:1998 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-v4klp1kjtnk9yrxwdcz5ah11ub.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-01-20 20:57:41,957\tINFO packaging.py:463 -- Pushing file package 'gcs://_ray_pkg_0bd8078b0063d4195929ce96a7cf436461a67169.zip' (9.62MiB) to Ray cluster...\n",
      "2026-01-20 20:57:41,997\tINFO packaging.py:476 -- Successfully pushed file package 'gcs://_ray_pkg_0bd8078b0063d4195929ce96a7cf436461a67169.zip'.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "paras_ds = ray.data.read_text(\"/mnt/cluster_storage/around.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "171f87ec-a0bd-434c-ad85-b4db342b7787",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 20:57:42,245\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_200_0\n",
      "2026-01-20 20:57:42,267\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_200_0. Full logs are in /tmp/ray/session_2026-01-20_18-18-31_241199_2386/logs/ray-data\n",
      "2026-01-20 20:57:42,267\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_200_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[Project] -> AggregateNumRows[AggregateNumRows]\n",
      "2026-01-20 20:57:42,268\tINFO streaming_executor.py:687 -- [dataset]: A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True` and `ray.data.DataContext.get_current().use_ray_tqdm = False`.\n",
      "2026-01-20 20:57:42,269\tINFO progress_bar.py:155 -- Progress bar disabled because stdout is a non-interactive terminal.\n",
      "2026-01-20 20:57:42,271\tWARNING resource_manager.py:136 -- ⚠️  Ray's object store is configured to use only 27.8% of available memory (40.0GiB out of 144.0GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "2026-01-20 20:57:42,298\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-20 20:57:42,299\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:42,300\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-20 20:57:42,301\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:42,302\tINFO progress_bar.py:213 -- === Ray Data Progress {Project} ===\n",
      "2026-01-20 20:57:42,303\tINFO progress_bar.py:215 -- Project: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:42,304\tINFO progress_bar.py:213 -- === Ray Data Progress {AggregateNumRows} ===\n",
      "2026-01-20 20:57:42,304\tINFO progress_bar.py:215 -- AggregateNumRows: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / 1\n",
      "2026-01-20 20:57:42,305\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-20 20:57:42,306\tINFO progress_bar.py:215 -- Running Dataset: dataset_200_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / 1\n",
      "2026-01-20 20:57:47,315\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 56.0B object store: Progress Completed 1 / 1\n",
      "2026-01-20 20:57:47,316\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:47,317\tINFO progress_bar.py:215 -- Project: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:47,317\tINFO progress_bar.py:215 -- AggregateNumRows: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / 1\n",
      "2026-01-20 20:57:47,318\tINFO progress_bar.py:215 -- Running Dataset: dataset_200_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / 1\n",
      "2026-01-20 20:57:48,825\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_200_0 execution finished in 6.56 seconds\n",
      "INFO:openlineage.client.client:OpenLineageClient will use `composite` transport\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1654"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras_ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95898c5f-a051-4335-89ad-92e94eac2f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 20:57:48,938\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_201_0\n",
      "2026-01-20 20:57:48,945\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_201_0. Full logs are in /tmp/ray/session_2026-01-20_18-18-31_241199_2386/logs/ray-data\n",
      "2026-01-20 20:57:48,946\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_201_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> LimitOperator[limit=4]\n",
      "2026-01-20 20:57:48,963\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-20 20:57:48,964\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:48,965\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-20 20:57:48,966\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:48,967\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=4} ===\n",
      "2026-01-20 20:57:48,968\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:48,969\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-20 20:57:48,969\tINFO progress_bar.py:215 -- Running Dataset: dataset_201_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:49,093\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_201_0 execution finished in 0.15 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': array(['Around the World in Eighty Days\\r',\n",
       "        'CHAPTER I. IN WHICH PHILEAS FOGG AND PASSEPARTOUT ACCEPT EACH OTHER, THE ONE AS MASTER, THE OTHER AS MAN\\r',\n",
       "        'Mr. Phileas Fogg lived, in 1872, at No. 7, Saville Row, Burlington Gardens, the house in which Sheridan died in 1814. He was one of the most noticeable members of the Reform Club, though he seemed always to avoid attracting attention; an enigmatical personage, about whom little was known, except that he was a polished man of the world. People said that he resembled Byron—at least that his head was Byronic; but he was a bearded, tranquil Byron, who might live on a thousand years without growing old.\\r',\n",
       "        'Certainly an Englishman, it was more doubtful whether Phileas Fogg was a Londoner. He was never seen on ’Change, nor at the Bank, nor in the counting-rooms of the “City”; no ships ever came into London docks of which he was the owner; he had no public employment; he had never been entered at any of the Inns of Court, either at the Temple, or Lincoln’s Inn, or Gray’s Inn; nor had his voice ever resounded in the Court of Chancery, or in the Exchequer, or the Queen’s Bench, or the Ecclesiastical Courts. He certainly was not a manufacturer; nor was he a merchant or a gentleman farmer. His name was strange to the scientific and learned societies, and he never was known to take part in the sage deliberations of the Royal Institution or the London Institution, the Artisan’s Association, or the Institution of Arts and Sciences. He belonged, in fact, to none of the numerous societies which swarm in the English capital, from the Harmonic to that of the Entomologists, founded mainly for the purpose of abolishing pernicious insects.\\r'],\n",
       "       dtype=object)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras_ds.take_batch(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df05d1-eebc-4a20-b8bd-e6fd6f100033",
   "metadata": {
    "tags": []
   },
   "source": [
    "To generate our emeddings, we'll use two steps\n",
    "\n",
    "1. Create a class that performs the embedding operation\n",
    "    1. We use a class because we'll want to hold on to a large, valuable piece of state -- the embedding model itself\n",
    "    1. For use with our vector databases, we'll need unique IDs to go with each document and embedding -- we'll generate UUIDs\n",
    "    1. the output from the `__call__` method will be similar to the input: a dict with the column names as keys, and vectorized types for values\n",
    "1. Call `dataset.map_batches(...)` where we connect the dataset to the processing class as well as specify resources like the number of class instances (actors) and GPUs\n",
    "    1. Specify an autoscaling actor pool -- to demo how Ray could autoscale to handle large, uneven workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66789137-7b23-4a70-aed4-e3745000df80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DocEmbedder:\n",
    "    def __init__(self):\n",
    "        self._model = SentenceTransformer(\"hkunlp/instructor-large\")\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        inputs = batch['text']\n",
    "        embeddings = self._model.encode(inputs, device='cuda:0')\n",
    "        ids = np.array([uuid.uuid1().hex for i in inputs])\n",
    "        return { 'doc' : inputs, 'vec' : embeddings, 'id' : ids }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6150cfd9-f22f-4137-a1c9-75c05f5e7685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vecs = paras_ds.map_batches(DocEmbedder, compute=ray.data.ActorPoolStrategy(size=2), num_gpus=0.125, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e13df2c-56bc-4105-8f73-a5aa419003d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 20:57:49,426\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_203_0\n",
      "2026-01-20 20:57:49,428\tINFO limit_pushdown.py:140 -- Skipping push down of limit 4 through map MapBatches[MapBatches(DocEmbedder)] because it requires 64 rows to produce stable outputs\n",
      "2026-01-20 20:57:49,432\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_203_0. Full logs are in /tmp/ray/session_2026-01-20_18-18-31_241199_2386/logs/ray-data\n",
      "2026-01-20 20:57:49,433\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_203_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> ActorPoolMapOperator[MapBatches(DocEmbedder)] -> LimitOperator[limit=4]\n",
      "2026-01-20 20:57:49,606\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-20 20:57:49,607\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:49,607\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-20 20:57:49,608\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:49,609\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(DocEmbedder)} ===\n",
      "2026-01-20 20:57:49,610\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:49,611\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=4} ===\n",
      "2026-01-20 20:57:49,612\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:49,613\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-20 20:57:49,613\tINFO progress_bar.py:215 -- Running Dataset: dataset_203_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store (pending: 0.25 GPU): Progress Completed 0 / ?\n",
      "2026-01-20 20:57:54,707\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-20 20:57:54,708\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-20 20:57:54,708\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 1 (368.6KiB); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:54,709\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:54,710\tINFO progress_bar.py:215 -- Running Dataset: dataset_203_0. Active & requested resources: 0/32 CPU, 368.6KiB/26.9GiB object store (pending: 0.25 GPU): Progress Completed 0 / ?\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py:416: UserWarning: The minimum number of concurrent actors for 'MapBatches(DocEmbedder)' is set to 2, but the operator only received 1 input(s). This means that the operator can launch at most 1 task(s), and won't fully utilize the available concurrency. You might be able to increase the number of concurrent tasks by configuring `override_num_blocks` earlier in the pipeline.\n",
      "  warnings.warn(\n",
      "2026-01-20 20:57:59,743\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-20 20:57:59,744\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-20 20:57:59,745\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.1 GPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:59,745\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:57:59,746\tINFO progress_bar.py:215 -- Running Dataset: dataset_203_0. Active & requested resources: 0/32 CPU, 0.125/2 GPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:04,840\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-20 20:58:04,841\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-20 20:58:04,842\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.1 GPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:04,843\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:04,844\tINFO progress_bar.py:215 -- Running Dataset: dataset_203_0. Active & requested resources: 0/32 CPU, 0.125/2 GPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:09,377\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_203_0 execution finished in 19.94 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'doc': array(['Around the World in Eighty Days\\r',\n",
       "        'CHAPTER I. IN WHICH PHILEAS FOGG AND PASSEPARTOUT ACCEPT EACH OTHER, THE ONE AS MASTER, THE OTHER AS MAN\\r',\n",
       "        'Mr. Phileas Fogg lived, in 1872, at No. 7, Saville Row, Burlington Gardens, the house in which Sheridan died in 1814. He was one of the most noticeable members of the Reform Club, though he seemed always to avoid attracting attention; an enigmatical personage, about whom little was known, except that he was a polished man of the world. People said that he resembled Byron—at least that his head was Byronic; but he was a bearded, tranquil Byron, who might live on a thousand years without growing old.\\r',\n",
       "        'Certainly an Englishman, it was more doubtful whether Phileas Fogg was a Londoner. He was never seen on ’Change, nor at the Bank, nor in the counting-rooms of the “City”; no ships ever came into London docks of which he was the owner; he had no public employment; he had never been entered at any of the Inns of Court, either at the Temple, or Lincoln’s Inn, or Gray’s Inn; nor had his voice ever resounded in the Court of Chancery, or in the Exchequer, or the Queen’s Bench, or the Ecclesiastical Courts. He certainly was not a manufacturer; nor was he a merchant or a gentleman farmer. His name was strange to the scientific and learned societies, and he never was known to take part in the sage deliberations of the Royal Institution or the London Institution, the Artisan’s Association, or the Institution of Arts and Sciences. He belonged, in fact, to none of the numerous societies which swarm in the English capital, from the Harmonic to that of the Entomologists, founded mainly for the purpose of abolishing pernicious insects.\\r'],\n",
       "       dtype=object),\n",
       " 'vec': array([[-0.03005561,  0.00823988, -0.05188768, ..., -0.02082619,\n",
       "          0.00214824,  0.05067572],\n",
       "        [-0.02825416, -0.01704813, -0.00829185, ..., -0.03100338,\n",
       "          0.04128142,  0.07145514],\n",
       "        [-0.01869183, -0.02350846, -0.0105991 , ..., -0.06052984,\n",
       "          0.0261689 ,  0.08915699],\n",
       "        [-0.02491501, -0.00306009, -0.02877481, ..., -0.05454246,\n",
       "          0.01828192,  0.09182645]], dtype=float32),\n",
       " 'id': array(['b2705228f64211f087c00ab2c0749841',\n",
       "        'b2705340f64211f087c00ab2c0749841',\n",
       "        'b270537cf64211f087c00ab2c0749841',\n",
       "        'b270539af64211f087c00ab2c0749841'], dtype=object)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = vecs.take_batch(4)\n",
    "\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20851404-9548-4d14-b48e-2eed6dfb2705",
   "metadata": {},
   "source": [
    "### Vector storage example: ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b3da0-1512-45a7-88b7-10918373ed19",
   "metadata": {},
   "source": [
    "Ray focuses on compute and is orthogonal to data storage, so many data stores can be used.\n",
    "\n",
    "For a simple example, we'll use ChromaDB, starting with a minimal in-memory demo so that we can see the prorgamming pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9243d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "collection = chroma_client.get_or_create_collection(name=\"my_text_chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88cecc-2943-4448-b114-7bb58da320c8",
   "metadata": {},
   "source": [
    "Insert the vectors, documents, and IDs\n",
    "\n",
    "> Note that Chroma can also accept arbitrary metadata dictionaries for each document, which you can then use in your queries (along with semantic similarity) and see in results. Metadata allows you to easily add powerful features like \"search only in chapter 3\" or \"cite source URLs for data returned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8aeaa33-d54e-4574-9286-07a0b80145e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection.upsert(\n",
    "    embeddings=sample_batch['vec'].tolist(),\n",
    "    documents=sample_batch['doc'].tolist(),\n",
    "    ids=sample_batch['id'].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3de3493-1c3a-4978-a4c2-d21dbdb28443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_query = model.encode(\"tell me about money\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc0467e1-4045-47ba-bf58-20b0512fb51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_embeddings=[test_query],\n",
    "    n_results=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cfc2a19-ca1e-4232-a973-04079625fcf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['b2705228f64211f087c00ab2c0749841',\n",
       "   'b270539af64211f087c00ab2c0749841',\n",
       "   'b2705340f64211f087c00ab2c0749841']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Around the World in Eighty Days\\r',\n",
       "   'Certainly an Englishman, it was more doubtful whether Phileas Fogg was a Londoner. He was never seen on ’Change, nor at the Bank, nor in the counting-rooms of the “City”; no ships ever came into London docks of which he was the owner; he had no public employment; he had never been entered at any of the Inns of Court, either at the Temple, or Lincoln’s Inn, or Gray’s Inn; nor had his voice ever resounded in the Court of Chancery, or in the Exchequer, or the Queen’s Bench, or the Ecclesiastical Courts. He certainly was not a manufacturer; nor was he a merchant or a gentleman farmer. His name was strange to the scientific and learned societies, and he never was known to take part in the sage deliberations of the Royal Institution or the London Institution, the Artisan’s Association, or the Institution of Arts and Sciences. He belonged, in fact, to none of the numerous societies which swarm in the English capital, from the Harmonic to that of the Entomologists, founded mainly for the purpose of abolishing pernicious insects.\\r',\n",
       "   'CHAPTER I. IN WHICH PHILEAS FOGG AND PASSEPARTOUT ACCEPT EACH OTHER, THE ONE AS MASTER, THE OTHER AS MAN\\r']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[None, None, None]],\n",
       " 'distances': [[0.41484829783439636, 0.4422388970851898, 0.4528372585773468]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c078d82-2a9a-4b21-97a0-d730b1b3758d",
   "metadata": {},
   "source": [
    "### Scaling queries with Chroma\n",
    "\n",
    "Now that we have the basics of Chroma down, let's look at scaling to large datasets.\n",
    "\n",
    "We'll create a Ray Core Actor that provides access to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e95e86ca-2690-422c-8cc3-fa261c016584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ray.remote(concurrency_groups={\"write\": 4, \"read\": 16})\n",
    "class ChromaWrapper:\n",
    "    def __init__(self):\n",
    "        self.chroma_client = chromadb.PersistentClient(path=\"/mnt/cluster_storage/vector_store\")\n",
    "        self.collection = self.chroma_client.get_or_create_collection(name=\"persistent_text_chunks\")\n",
    "\n",
    "    @ray.method(concurrency_group=\"write\")\n",
    "    def upsert(self, batch):\n",
    "        self.collection.upsert(\n",
    "            embeddings=batch['vec'].tolist(),\n",
    "            documents=batch['doc'].tolist(),\n",
    "            ids=batch['id'].tolist()\n",
    "        )\n",
    "        return len(batch['id'])\n",
    "\n",
    "    @ray.method(concurrency_group=\"read\")\n",
    "    def query(self, q):\n",
    "        return self.collection.query(query_embeddings=[q], n_results=3)\n",
    "\n",
    "chroma_server = ChromaWrapper.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867eb1e-cc4b-41dd-9852-7f04a8f1aaf7",
   "metadata": {},
   "source": [
    "We're using `map_batches` with a side-effect to write the vectors to the database. Alternative approach would include\n",
    "* writing a custom sink so we could use code like `my_dataset_with_vectors.write_cool_vectordb('collection')`\n",
    "* writing to a standard storage format like parquet and using another scripted workflow step to bulk load the database\n",
    "\n",
    "In this code example, we also demo using `map_batches` in (stateless) task form, and using a lambda, to access a running actor.\n",
    "\n",
    "> As an exercise, rewrite this to use `map_batches` with an actor (callable) class, the way we've done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f705c52c-e9d0-45b9-bf7a-064327e569a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 20:58:10,362\tINFO dataset.py:3641 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2026-01-20 20:58:10,364\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_206_0\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/anyscale/data/_internal/util/dependencies.py:42: UserWarning: Numba isn't available. Install numba>=0.61>=0.61 to get better performance for hash partitioning operations. Falling back to slower Python implementation for RayTurbo optimizations.\n",
      "  warnings.warn(\n",
      "2026-01-20 20:58:10,371\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_206_0. Full logs are in /tmp/ray/session_2026-01-20_18-18-31_241199_2386/logs/ray-data\n",
      "2026-01-20 20:58:10,372\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_206_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> ActorPoolMapOperator[MapBatches(DocEmbedder)] -> TaskPoolMapOperator[MapBatches(<lambda>)] -> HashAggregateOperator[HashAggregate(key_columns=(), num_partitions=1)] -> LimitOperator[limit=1]\n",
      "2026-01-20 20:58:10,547\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-20 20:58:10,548\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:10,550\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-20 20:58:10,551\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:10,552\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(DocEmbedder)} ===\n",
      "2026-01-20 20:58:10,553\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:10,553\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(<lambda>)} ===\n",
      "2026-01-20 20:58:10,554\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:10,555\tINFO progress_bar.py:213 -- === Ray Data Progress {HashAggregate(key_columns=(), num_partitions=1)} ===\n",
      "2026-01-20 20:58:10,556\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:10,556\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=1} ===\n",
      "2026-01-20 20:58:10,557\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:10,558\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-20 20:58:10,559\tINFO progress_bar.py:215 -- Running Dataset: dataset_206_0. Active & requested resources: 0.25/32 CPU, 0.0B/26.9GiB object store (pending: 0.25 GPU): Progress Completed 0 / ?\n",
      "2026-01-20 20:58:15,614\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-20 20:58:15,615\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-20 20:58:15,615\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 1 (368.6KiB); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:15,616\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:15,617\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:15,617\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:15,618\tINFO progress_bar.py:215 -- Running Dataset: dataset_206_0. Active & requested resources: 0.25/32 CPU, 368.6KiB/26.9GiB object store (pending: 0.25 GPU): Progress Completed 0 / ?\n",
      "2026-01-20 20:58:20,684\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-20 20:58:20,685\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-20 20:58:20,685\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.1 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:20,686\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:20,687\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:20,687\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:20,688\tINFO progress_bar.py:215 -- Running Dataset: dataset_206_0. Active & requested resources: 0.25/32 CPU, 0.125/2 GPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:25,748\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-20 20:58:25,749\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-20 20:58:25,750\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.1 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:25,750\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:25,751\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:25,752\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:25,752\tINFO progress_bar.py:215 -- Running Dataset: dataset_206_0. Active & requested resources: 0.25/32 CPU, 0.125/2 GPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:30,796\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-20 20:58:30,797\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-20 20:58:30,798\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 5.3MiB object store; [0/1 objects local]: Progress Completed 1654 / 1654\n",
      "2026-01-20 20:58:30,799\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:30,799\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:30,800\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:30,801\tINFO progress_bar.py:215 -- Running Dataset: dataset_206_0. Active & requested resources: 1.25/32 CPU, 389.3MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:35,833\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-20 20:58:35,834\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-20 20:58:35,834\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 5.3MiB object store; [0/1 objects local]: Progress Completed 1654 / 1654\n",
      "2026-01-20 20:58:35,835\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:35,836\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:35,837\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:35,838\tINFO progress_bar.py:215 -- Running Dataset: dataset_206_0. Active & requested resources: 1.25/32 CPU, 389.3MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:58:38,305\tINFO progress_bar.py:213 -- === Ray Data Progress {*- Shuffle} ===\n",
      "2026-01-20 20:58:38,306\tINFO progress_bar.py:215 -- *- Shuffle: Progress Completed 1 / 1\n",
      "2026-01-20 20:58:38,319\tINFO progress_bar.py:213 -- === Ray Data Progress {*- Aggregation} ===\n",
      "2026-01-20 20:58:38,320\tINFO progress_bar.py:215 -- *- Aggregation: Progress Completed 1 / ?\n",
      "2026-01-20 20:58:38,336\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_206_0 execution finished in 27.96 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1654"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.map_batches(lambda batch: {'batch_count': [ray.get(chroma_server.upsert.remote(batch))]}).sum('batch_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134d7ec-f01c-48b1-beb4-8e142f54dcf2",
   "metadata": {},
   "source": [
    "Since our service is running as an actor, we can quickly test out a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e9f13cb-c2fb-4c0b-b72f-9929984a934c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['63baa6b0f63311f08bd90ab2c0749841',\n",
       "   '20b7ee1ef64111f092e30a2cfe0740b3',\n",
       "   '7b91f5a0f64111f0bd500a2cfe0740b3']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['During the lecture the train had been making good progress, and towards half-past twelve it reached the northwest border of the Great Salt Lake. Thence the passengers could observe the vast extent of this interior sea, which is also called the Dead Sea, and into which flows an American Jordan. It is a picturesque expanse, framed in lofty crags in large strata, encrusted with white salt—a superb sheet of water, which was formerly of larger extent than now, its shores having encroached with the lapse of time, and thus at once reduced its breadth and increased its depth.\\r',\n",
       "   'During the lecture the train had been making good progress, and towards half-past twelve it reached the northwest border of the Great Salt Lake. Thence the passengers could observe the vast extent of this interior sea, which is also called the Dead Sea, and into which flows an American Jordan. It is a picturesque expanse, framed in lofty crags in large strata, encrusted with white salt—a superb sheet of water, which was formerly of larger extent than now, its shores having encroached with the lapse of time, and thus at once reduced its breadth and increased its depth.\\r',\n",
       "   'During the lecture the train had been making good progress, and towards half-past twelve it reached the northwest border of the Great Salt Lake. Thence the passengers could observe the vast extent of this interior sea, which is also called the Dead Sea, and into which flows an American Jordan. It is a picturesque expanse, framed in lofty crags in large strata, encrusted with white salt—a superb sheet of water, which was formerly of larger extent than now, its shores having encroached with the lapse of time, and thus at once reduced its breadth and increased its depth.\\r']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[None, None, None]],\n",
       " 'distances': [[0.22195535898208618,\n",
       "   0.22195535898208618,\n",
       "   0.22195535898208618]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utah_query_vec = model.encode(\"Describe the body of water in Utah\").tolist()\n",
    "\n",
    "ray.get(chroma_server.query.remote(utah_query_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489741a7-ecb7-46db-a861-6c7869902d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d59d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
