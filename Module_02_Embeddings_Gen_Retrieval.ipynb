{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a1d247-cb22-4115-89c1-e5fbdc0dc633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "# uuid: Generate unique identifiers for each document in the vector store\n",
    "# chromadb: Vector database for storing and querying embeddings\n",
    "# numpy: Array operations for batch processing\n",
    "# ray: Distributed computing framework\n",
    "# sentence_transformers: Pre-trained models for generating text embeddings\n",
    "\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import ray\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f00640-1a22-492a-85eb-638f8f30c1e1",
   "metadata": {},
   "source": [
    "# Module 2: Embeddings Generation and Retrieval with Ray Data\n",
    "\n",
    "[![Ray](https://img.shields.io/badge/Ray-Data-blue)](https://docs.ray.io/en/latest/data/data.html) [![ChromaDB](https://img.shields.io/badge/ChromaDB-Vector%20Store-green)](https://docs.trychroma.com/)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This module demonstrates how to generate embeddings at scale using Ray Data and store them in a vector database (ChromaDB) for semantic retrieval. These are core components of any RAG (Retrieval-Augmented Generation) system.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Generate embeddings using SentenceTransformers with Ray Data\n",
    "- Use the callable class pattern for stateful batch processing\n",
    "- Store and query vectors using ChromaDB\n",
    "- Create Ray actors for concurrent database access\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```bash\n",
    "┌────────────────────────────────────────────────────────────────────────────┐\n",
    "│                     Embedding Generation Pipeline                          │\n",
    "└────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "    Raw Text Documents                    Vector Database (ChromaDB)\n",
    "           │                                        ▲\n",
    "           ▼                                        │\n",
    "    ┌─────────────────┐                    ┌────────────────┐\n",
    "    │   Ray Data      │  ─────────────────►│ ChromaWrapper  │\n",
    "    │   read_text()   │                    │     Actor      │\n",
    "    └────────┬────────┘                    └────────────────┘\n",
    "             │                                      ▲\n",
    "             ▼                                      │\n",
    "    ┌────────────────────────────────────────────────┐\n",
    "    │                 map_batches()                  │\n",
    "    │  ┌──────────────┐    ┌──────────────┐          │\n",
    "    │  │  DocEmbedder │    │  DocEmbedder │  Actor   │\n",
    "    │  │    Actor 1   │    │    Actor 2   │  Pool    │\n",
    "    │  │   (GPU 0.1)  │    │   (GPU 0.1)  │          │\n",
    "    │  └──────────────┘    └──────────────┘          │\n",
    "    └────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b2a18d-abf3-4342-894e-8154fdba2a92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EMBEDDING MODEL SETUP\n",
    "# ============================================================================\n",
    "# We use 'all-MiniLM-L6-v2' - a lightweight, fast embedding model that produces\n",
    "# 384-dimensional vectors. This model offers a good balance between quality\n",
    "# and performance, and is more stable across different CUDA configurations.\n",
    "#\n",
    "# Alternative: 'hkunlp/instructor-large' (768-dim) for higher quality but\n",
    "# may have CUDA compatibility issues on some systems.\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Load the model for local testing (later we'll use this in distributed actors)\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f\"Loaded model: {EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0subk7s0p4w",
   "metadata": {},
   "source": [
    "## Step 1: Understanding Embeddings\n",
    "\n",
    "Embeddings are dense vector representations of text that capture semantic meaning. Similar texts produce similar vectors, enabling semantic search.\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────────────┐\n",
    "│                      Text to Embedding Conversion                          │\n",
    "└────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "    \"The quick brown fox\"              \"A fast auburn dog\"\n",
    "             │                                 │\n",
    "             ▼                                 ▼\n",
    "    ┌─────────────────┐               ┌─────────────────┐\n",
    "    │SentenceTransform│               │SentenceTransform│\n",
    "    │(all-MiniLM-L6)  │               │(all-MiniLM-L6)  │\n",
    "    └────────┬────────┘               └────────┬────────┘\n",
    "             │                                 │\n",
    "             ▼                                 ▼\n",
    "    [0.23, -0.45, 0.12, ...]         [0.21, -0.42, 0.15, ...]\n",
    "         384 dimensions                   384 dimensions\n",
    "             │                                 │\n",
    "             └──────────────┬──────────────────┘\n",
    "                            │\n",
    "                     Cosine Similarity\n",
    "                            │\n",
    "                            ▼\n",
    "                  High Similarity (0.89)\n",
    "                  (Semantically related)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d5a05e-1f74-479f-a63c-bd14783c697b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quick test: encode two sample sentences to verify the model works\n",
    "# Notice how we can encode multiple items in a single call (batch processing)\n",
    "items = [\"What are some top attractions in Seattle?\", \n",
    "         \"What are some top attractions in Los Angeles?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f6bfa41-1a60-42da-bc56-48006ac2a86c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2, 384)\n",
      "Each text is now a 384-dimensional vector\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings - each text becomes a 768-dimensional vector\n",
    "# Shape: (2, 768) = 2 items x 768 dimensions per embedding\n",
    "vectors = model.encode(items)\n",
    "\n",
    "print(f\"Shape: {vectors.shape}\")\n",
    "print(f\"Each text is now a {vectors.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00820505-d734-40e8-aeb9-ee02db88c55c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Loading Data with Ray Data\n",
    "\n",
    "For distributed processing, we need data accessible to all workers. We copy our source text to shared cluster storage (`/mnt/cluster_storage/`).\n",
    "\n",
    "**Data Source:** \"Around the World in Eighty Days\" by Jules Verne - a classic novel we'll use as our document corpus for the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a090751-a222-4f7a-b20b-af586fc777fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Copy the source text to shared cluster storage\n",
    "# This ensures all Ray workers can access the file\n",
    "! cp around.txt /mnt/cluster_storage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8314c12-e9ed-4fd4-a146-8120c19086fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 00:58:59,456\tINFO worker.py:1821 -- Connecting to existing Ray cluster at address: 10.0.9.248:6379...\n",
      "2026-01-22 00:58:59,469\tINFO worker.py:1998 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-v4klp1kjtnk9yrxwdcz5ah11ub.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-01-22 00:58:59,499\tINFO packaging.py:463 -- Pushing file package 'gcs://_ray_pkg_3dd5c48f1162e1cb703434693228d29e52183733.zip' (10.61MiB) to Ray cluster...\n",
      "2026-01-22 00:58:59,541\tINFO packaging.py:476 -- Successfully pushed file package 'gcs://_ray_pkg_3dd5c48f1162e1cb703434693228d29e52183733.zip'.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "2026-01-22 00:58:59,706\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_129_0\n",
      "2026-01-22 00:58:59,728\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_129_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 00:58:59,729\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_129_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> LimitOperator[limit=1]\n",
      "2026-01-22 00:58:59,730\tINFO streaming_executor.py:687 -- [dataset]: A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True` and `ray.data.DataContext.get_current().use_ray_tqdm = False`.\n",
      "2026-01-22 00:58:59,730\tINFO progress_bar.py:155 -- Progress bar disabled because stdout is a non-interactive terminal.\n",
      "2026-01-22 00:58:59,732\tWARNING resource_manager.py:136 -- ⚠️  Ray's object store is configured to use only 27.8% of available memory (40.0GiB out of 144.0GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "2026-01-22 00:58:59,768\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 00:58:59,769\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:58:59,770\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 00:58:59,772\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:58:59,774\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=1} ===\n",
      "2026-01-22 00:58:59,775\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:58:59,776\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 00:58:59,777\tINFO progress_bar.py:215 -- Running Dataset: dataset_129_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:04,862\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 56.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 00:59:04,863\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:04,864\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:04,865\tINFO progress_bar.py:215 -- Running Dataset: dataset_129_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:06,323\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_129_0 execution finished in 6.59 seconds\n",
      "INFO:openlineage.client.client:OpenLineageClient will use `composite` transport\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset schema: Column  Type\n",
      "------  ----\n",
      "text    string\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA WITH RAY DATA\n",
    "# ============================================================================\n",
    "# ray.data.read_text() reads a text file where each line becomes a row\n",
    "# This creates a Ray Dataset - a distributed, lazy data structure\n",
    "\n",
    "paras_ds = ray.data.read_text(\"/mnt/cluster_storage/around.txt\")\n",
    "\n",
    "# The dataset is lazy - no data is loaded until we perform an action\n",
    "print(f\"Dataset schema: {paras_ds.schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "171f87ec-a0bd-434c-ad85-b4db342b7787",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 00:59:06,386\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_130_0\n",
      "2026-01-22 00:59:06,390\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_130_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 00:59:06,391\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_130_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[Project] -> AggregateNumRows[AggregateNumRows]\n",
      "2026-01-22 00:59:06,410\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 00:59:06,411\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:06,412\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 00:59:06,413\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:06,414\tINFO progress_bar.py:213 -- === Ray Data Progress {Project} ===\n",
      "2026-01-22 00:59:06,415\tINFO progress_bar.py:215 -- Project: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:06,416\tINFO progress_bar.py:213 -- === Ray Data Progress {AggregateNumRows} ===\n",
      "2026-01-22 00:59:06,417\tINFO progress_bar.py:215 -- AggregateNumRows: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / 1\n",
      "2026-01-22 00:59:06,418\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 00:59:06,419\tINFO progress_bar.py:215 -- Running Dataset: dataset_130_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / 1\n",
      "2026-01-22 00:59:06,492\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_130_0 execution finished in 0.10 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total paragraphs in corpus: 1654\n"
     ]
    }
   ],
   "source": [
    "# count() triggers execution and returns the total number of rows\n",
    "# In this case, each paragraph/line of the book is one row\n",
    "print(f\"Total paragraphs in corpus: {paras_ds.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95898c5f-a051-4335-89ad-92e94eac2f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 00:59:06,570\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_131_0\n",
      "2026-01-22 00:59:06,574\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_131_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 00:59:06,574\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_131_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> LimitOperator[limit=4]\n",
      "2026-01-22 00:59:06,591\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 00:59:06,592\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:06,593\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 00:59:06,594\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:06,595\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=4} ===\n",
      "2026-01-22 00:59:06,596\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:06,596\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 00:59:06,597\tINFO progress_bar.py:215 -- Running Dataset: dataset_131_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:06,821\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_131_0 execution finished in 0.25 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: dict_keys(['text'])\n",
      "First 4 paragraphs:\n",
      ".... Around the World in Eighty Days\n",
      "  2. CHAPTER I. IN WHICH PHILEAS FOGG AND PASSEPARTOUT ACCEPT EACH OTHER, THE ONE AS ...\n",
      "  3. Mr. Phileas Fogg lived, in 1872, at No. 7, Saville Row, Burlington Gardens, the ...\n",
      "  4. Certainly an Englishman, it was more doubtful whether Phileas Fogg was a Londone...\n"
     ]
    }
   ],
   "source": [
    "# Preview the first 4 rows as a batch (dictionary of numpy arrays)\n",
    "# This is the format that map_batches() will receive\n",
    "sample = paras_ds.take_batch(4)\n",
    "print(f\"Batch keys: {sample.keys()}\")\n",
    "print(f\"First 4 paragraphs:\")\n",
    "for i, text in enumerate(sample['text'][:4]):\n",
    "    print(f\"  {i+1}. {text[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df05d1-eebc-4a20-b8bd-e6fd6f100033",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Step 3: Distributed Embedding Generation\n",
    "\n",
    "To generate embeddings at scale, we use the **Callable Class Pattern** with `map_batches()`. This pattern is essential for:\n",
    "\n",
    "1. **Expensive initialization** (loading ML models) happens once per actor\n",
    "2. **Batch processing** for efficient GPU utilization\n",
    "3. **Parallel execution** across multiple actors\n",
    "\n",
    "### The Callable Class Pattern\n",
    "\n",
    "```bash\n",
    "┌────────────────────────────────────────────────────────────────────────────┐\n",
    "│                        Callable Class Pattern                              │\n",
    "└────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "    class DocEmbedder:\n",
    "        \n",
    "        def __init__(self):           ◄── Called ONCE when actor starts\n",
    "            self._model = load()           (expensive model loading)\n",
    "            \n",
    "        def __call__(self, batch):    ◄── Called for EACH batch of data\n",
    "            return process(batch)          (efficient batch processing)\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────────────────────┐\n",
    "│  Actor Lifecycle:                                                            │\n",
    "│                                                                              │\n",
    "│              [Actor Created] ──► __init__() ──► [Ready to process batches]   │\n",
    "│                                              │                               │\n",
    "│                       ┌──────────────────────┴──────────────────────┐        │\n",
    "│                       │                                             │        │\n",
    "│                       ▼                                             ▼        │\n",
    "│                __call__(batch1)                             __call__(batch2)\n",
    "│                       │                                             │        │\n",
    "│                       ▼                                             ▼        │\n",
    "│                return results                               return results.  │\n",
    "└──────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Parameters:**\n",
    "- `compute=ray.data.ActorPoolStrategy(size=N)` - Creates N actor instances\n",
    "- `num_gpus=0.125` - Each actor uses 1/8 of a GPU (fractional allocation)\n",
    "- `batch_size=64` - Number of records processed per `__call__` invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66789137-7b23-4a70-aed4-e3745000df80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOCUMENT EMBEDDER - Callable Class for Distributed Processing\n",
    "# ============================================================================\n",
    "# This class follows the callable class pattern required by map_batches()\n",
    "# when using ActorPoolStrategy for stateful processing.\n",
    "\n",
    "class DocEmbedder:\n",
    "    \"\"\"\n",
    "    Generates embeddings for document batches using SentenceTransformers.\n",
    "    \n",
    "    - __init__: Loads the model ONCE when the actor starts (expensive operation)\n",
    "    - __call__: Processes batches efficiently (called many times per actor)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load the embedding model - happens once per actor instance\n",
    "        # This is why we use actors: to avoid reloading the model for each batch\n",
    "        # Using 'all-MiniLM-L6-v2' for stability across CUDA configurations\n",
    "        self._model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        # Extract text from the batch\n",
    "        inputs = batch['text']\n",
    "        \n",
    "        # Generate embeddings\n",
    "        # This lightweight model works efficiently on CPU\n",
    "        embeddings = self._model.encode(inputs)\n",
    "        \n",
    "        # Generate unique IDs for each document (required by vector databases)\n",
    "        ids = np.array([uuid.uuid1().hex for i in inputs])\n",
    "        \n",
    "        # Return a new batch with original docs, vectors, and IDs\n",
    "        return {'doc': inputs, 'vec': embeddings, 'id': ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6150cfd9-f22f-4137-a1c9-75c05f5e7685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# APPLY EMBEDDING TRANSFORMATION WITH ACTOR POOL\n",
    "# ============================================================================\n",
    "# map_batches() applies DocEmbedder to all batches in the dataset\n",
    "#\n",
    "# Parameters:\n",
    "#   - DocEmbedder: The callable class to instantiate\n",
    "#   - compute=ActorPoolStrategy(size=2): Create 2 actor instances\n",
    "#   - batch_size=64: Process 64 documents per __call__ invocation\n",
    "#\n",
    "# Note: Using CPU for 'all-MiniLM-L6-v2' model (lightweight and fast)\n",
    "# For GPU-based models, add: num_gpus=0.125\n",
    "\n",
    "vecs = paras_ds.map_batches(\n",
    "    DocEmbedder, \n",
    "    compute=ray.data.ActorPoolStrategy(size=2),  # 2 parallel actors\n",
    "    batch_size=64                                 # Documents per batch\n",
    ")\n",
    "\n",
    "# Note: This is LAZY - no computation happens until we materialize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e13df2c-56bc-4105-8f73-a5aa419003d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 00:59:07,045\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_133_0\n",
      "2026-01-22 00:59:07,046\tINFO limit_pushdown.py:140 -- Skipping push down of limit 4 through map MapBatches[MapBatches(DocEmbedder)] because it requires 64 rows to produce stable outputs\n",
      "2026-01-22 00:59:07,050\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_133_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 00:59:07,051\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_133_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> ActorPoolMapOperator[MapBatches(DocEmbedder)] -> LimitOperator[limit=4]\n",
      "2026-01-22 00:59:07,224\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 00:59:07,225\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:07,226\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 00:59:07,227\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:07,229\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(DocEmbedder)} ===\n",
      "2026-01-22 00:59:07,230\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:07,231\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=4} ===\n",
      "2026-01-22 00:59:07,231\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:07,232\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 00:59:07,233\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store (pending: 2 CPU): Progress Completed 0 / ?\n",
      "2026-01-22 00:59:12,233\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 00:59:12,234\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 00:59:12,235\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 1 (368.6KiB); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:12,235\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:12,236\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 0/32 CPU, 368.6KiB/26.9GiB object store (pending: 2 CPU): Progress Completed 0 / ?\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py:416: UserWarning: The minimum number of concurrent actors for 'MapBatches(DocEmbedder)' is set to 2, but the operator only received 1 input(s). This means that the operator can launch at most 1 task(s), and won't fully utilize the available concurrency. You might be able to increase the number of concurrent tasks by configuring `override_num_blocks` earlier in the pipeline.\n",
      "  warnings.warn(\n",
      "2026-01-22 00:59:17,246\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 00:59:17,247\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 00:59:17,247\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:17,248\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:17,249\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:22,248\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 00:59:22,249\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 00:59:22,250\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:22,251\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:22,252\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:27,256\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 00:59:27,257\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 00:59:27,258\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:27,259\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:27,259\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:32,258\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 00:59:32,259\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 00:59:32,260\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:32,261\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:32,262\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:37,266\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 00:59:37,267\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 738.3KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 00:59:37,268\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:37,269\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:37,270\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.7MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:42,272\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 00:59:42,273\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 738.3KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 00:59:42,274\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:42,275\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:42,276\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.7MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:47,276\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 00:59:47,277\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 738.3KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 00:59:47,277\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:47,278\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:47,279\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.7MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:52,376\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 00:59:52,377\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 738.3KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 00:59:52,378\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:52,378\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:52,379\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.7MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:57,477\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 00:59:57,478\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 738.3KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 00:59:57,478\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:57,479\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 00:59:57,479\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.7MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:02,572\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:00:02,573\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 738.3KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:00:02,573\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:02,574\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:02,575\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.7MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:07,668\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:00:07,669\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 738.3KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:00:07,670\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:07,670\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:07,671\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.7MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:12,670\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:00:12,671\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 738.3KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:00:12,671\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:12,672\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:12,673\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.7MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:17,770\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:00:17,771\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 738.3KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:00:17,772\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:17,773\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:17,774\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.7MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:22,869\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:00:22,870\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 738.3KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:00:22,871\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:22,871\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:22,872\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.7MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:27,877\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:00:27,878\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 738.3KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:00:27,879\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:27,879\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:27,880\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.7MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:32,980\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:00:32,980\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 738.3KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:00:32,981\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:32,982\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:32,982\tINFO progress_bar.py:215 -- Running Dataset: dataset_133_0. Active & requested resources: 1/32 CPU, 384.7MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:33,968\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_133_0 execution finished in 86.92 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: dict_keys(['doc', 'vec', 'id'])\n",
      "Document shape: (4,)\n",
      "Vector shape: (4, 384)\n",
      "ID shape: (4,)\n",
      "\n",
      "Each document is now a 384-dimensional vector\n"
     ]
    }
   ],
   "source": [
    "# Materialize a sample batch to verify the embedding pipeline works\n",
    "# This triggers actor creation, model loading, and embedding generation\n",
    "sample_batch = vecs.take_batch(4)\n",
    "\n",
    "print(f\"Batch keys: {sample_batch.keys()}\")\n",
    "print(f\"Document shape: {sample_batch['doc'].shape}\")\n",
    "print(f\"Vector shape: {sample_batch['vec'].shape}\")\n",
    "print(f\"ID shape: {sample_batch['id'].shape}\")\n",
    "print(f\"\\nEach document is now a {sample_batch['vec'].shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20851404-9548-4d14-b48e-2eed6dfb2705",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Vector Storage with ChromaDB\n",
    "\n",
    "Vector databases store embeddings and enable fast similarity search. ChromaDB is an open-source option that's easy to use.\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────────────┐\n",
    "│                          Vector Database Concept                           │\n",
    "└────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "┌────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         ChromaDB Collection                                │\n",
    "│                                                                            │\n",
    "│   ID            Document                           Vector (384-dim)        │\n",
    "│   ──────────────────────────────────────────────────────────────────────   │\n",
    "│   abc123        \"The quick brown fox...\"           [0.23, -0.45, ...]      │\n",
    "│   def456        \"A lazy dog sleeps...\"             [0.11, -0.32, ...]      │\n",
    "│   ghi789        \"The weather is nice...\"           [0.44, 0.21, ...]       │\n",
    "│                        ...                                ...              │\n",
    "└────────────────────────────────────────────────────────────────────────────┘\n",
    "    \n",
    "Query Flow:\n",
    "\n",
    "\"Tell me about animals\"  ──►  encode()  ──►  [0.19, -0.40, ...]\n",
    "                                                     │\n",
    "                                     ┌───────────────┴───────────────┐\n",
    "                                     │   Cosine Similarity Search    │\n",
    "                                     │   (find nearest neighbors)    │\n",
    "                                     └───────────────┬───────────────┘\n",
    "                                                     │\n",
    "                                                     ▼\n",
    "                         Results: [\"The quick brown fox...\", \"A lazy dog...\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b3da0-1512-45a7-88b7-10918373ed19",
   "metadata": {},
   "source": [
    "### In-Memory ChromaDB Demo\n",
    "\n",
    "Let's start with a simple in-memory ChromaDB instance to understand the API before scaling up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9243d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created collection: my_text_chunks\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE IN-MEMORY CHROMADB COLLECTION\n",
    "# ============================================================================\n",
    "# ChromaDB supports two modes:\n",
    "#   - In-memory (ephemeral): Data lost when process ends\n",
    "#   - Persistent: Data stored on disk\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Create an in-memory client (good for testing)\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "\n",
    "# Create or get a collection (like a table in traditional databases)\n",
    "collection = chroma_client.get_or_create_collection(name=\"my_text_chunks\")\n",
    "\n",
    "print(f\"Created collection: {collection.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88cecc-2943-4448-b114-7bb58da320c8",
   "metadata": {},
   "source": [
    "### Inserting Documents\n",
    "\n",
    "ChromaDB stores three components for each document:\n",
    "- **embeddings**: The vector representations\n",
    "- **documents**: The original text (for retrieval)\n",
    "- **ids**: Unique identifiers\n",
    "\n",
    "Optionally, you can also add **metadata** for filtering (e.g., \"chapter\", \"author\", \"source_url\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8aeaa33-d54e-4574-9286-07a0b80145e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 4 documents into collection\n"
     ]
    }
   ],
   "source": [
    "# Insert our sample batch into ChromaDB\n",
    "# upsert() will insert new documents or update existing ones (based on ID)\n",
    "collection.upsert(\n",
    "    embeddings=sample_batch['vec'].tolist(),  # Convert numpy arrays to lists\n",
    "    documents=sample_batch['doc'].tolist(),\n",
    "    ids=sample_batch['id'].tolist()\n",
    ")\n",
    "\n",
    "print(f\"Inserted {len(sample_batch['id'])} documents into collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3de3493-1c3a-4978-a4c2-d21dbdb28443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vector dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Create a query embedding to test similarity search\n",
    "# We encode the query using the same model used for documents\n",
    "test_query = model.encode(\"tell me about money\").tolist()\n",
    "\n",
    "print(f\"Query vector dimension: {len(test_query)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc0467e1-4045-47ba-bf58-20b0512fb51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query the collection for similar documents\n",
    "# n_results=3 returns the top 3 most similar documents\n",
    "results = collection.query(\n",
    "    query_embeddings=[test_query],  # Can query multiple embeddings at once\n",
    "    n_results=3                      # Return top 3 matches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cfc2a19-ca1e-4232-a973-04079625fcf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Results:\n",
      "  - IDs: [['9348fb98f72d11f0811d023246279789', '9348fcd8f72d11f0811d023246279789', '9348fcbaf72d11f0811d023246279789']]\n",
      "  - Distances: [[1.686969518661499, 1.757093906402588, 1.9784538745880127]]\n",
      "  - Documents: [['Around the World in Eighty Days\\r', 'Certainly an Englishman, it was more doubtful whether Phileas Fogg was a Londoner. He was never seen on ’Change, nor at the Bank, nor in the counting-rooms of the “City”; no ships ever came into London docks of which he was the owner; he had no public employment; he had never been entered at any of the Inns of Court, either at the Temple, or Lincoln’s Inn, or Gray’s Inn; nor had his voice ever resounded in the Court of Chancery, or in the Exchequer, or the Queen’s Bench, or the Ecclesiastical Courts. He certainly was not a manufacturer; nor was he a merchant or a gentleman farmer. His name was strange to the scientific and learned societies, and he never was known to take part in the sage deliberations of the Royal Institution or the London Institution, the Artisan’s Association, or the Institution of Arts and Sciences. He belonged, in fact, to none of the numerous societies which swarm in the English capital, from the Harmonic to that of the Entomologists, founded mainly for the purpose of abolishing pernicious insects.\\r', 'Mr. Phileas Fogg lived, in 1872, at No. 7, Saville Row, Burlington Gardens, the house in which Sheridan died in 1814. He was one of the most noticeable members of the Reform Club, though he seemed always to avoid attracting attention; an enigmatical personage, about whom little was known, except that he was a polished man of the world. People said that he resembled Byron—at least that his head was Byronic; but he was a bearded, tranquil Byron, who might live on a thousand years without growing old.\\r']]\n"
     ]
    }
   ],
   "source": [
    "# Display the query results\n",
    "# Results include: ids, documents, distances (similarity scores), and metadata\n",
    "print(\"Query Results:\")\n",
    "print(f\"  - IDs: {results['ids']}\")\n",
    "print(f\"  - Distances: {results['distances']}\")  # Lower = more similar\n",
    "print(f\"  - Documents: {results['documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c078d82-2a9a-4b21-97a0-d730b1b3758d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Scaling with Ray Actors\n",
    "\n",
    "For production workloads, we need:\n",
    "1. **Persistent storage** - Data survives process restarts\n",
    "2. **Concurrent access** - Multiple readers/writers\n",
    "3. **Distributed access** - Any Ray worker can query the database\n",
    "\n",
    "We wrap ChromaDB in a Ray Actor to achieve this:\n",
    "\n",
    "```bash\n",
    "┌────────────────────────────────────────────────────────────────────────────┐\n",
    "│                        ChromaWrapper Actor                                 │\n",
    "└────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "                        ┌─────────────────────────┐\n",
    "    Ray Worker 1 ──────►│                         │\n",
    "                        │    ChromaWrapper        │\n",
    "    Ray Worker 2 ──────►│       Actor             │──────► Persistent Storage\n",
    "                        │                         │        /mnt/cluster_storage/\n",
    "    Ray Worker 3 ──────►│  ┌─────┐   ┌──────┐     │\n",
    "                        │  │write│   │ read │     │\n",
    "                        │  │group│   │ group│     │\n",
    "                        │  │ (4) │   │ (16) │     │\n",
    "                        │  └─────┘   └──────┘     │\n",
    "                        └─────────────────────────┘\n",
    "    \n",
    "    Concurrency Groups:\n",
    "    - write: 4 concurrent writes (limited for consistency)\n",
    "    - read: 16 concurrent reads (higher for scalability)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e95e86ca-2690-422c-8cc3-fa261c016584",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaWrapper actor created and ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHROMAWRAPPER ACTOR - Distributed Vector Database Access\n",
    "# ============================================================================\n",
    "# This Ray actor wraps ChromaDB to provide:\n",
    "#   - Concurrent read/write access from multiple workers\n",
    "#   - Persistent storage on shared filesystem\n",
    "#   - Concurrency groups to control parallelism\n",
    "\n",
    "@ray.remote(concurrency_groups={\"write\": 4, \"read\": 16})\n",
    "class ChromaWrapper:\n",
    "    \"\"\"\n",
    "    Ray actor providing distributed access to ChromaDB.\n",
    "    \n",
    "    Concurrency groups allow:\n",
    "    - Up to 4 concurrent write operations\n",
    "    - Up to 16 concurrent read operations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Use PersistentClient to store data on disk\n",
    "        # Path must be accessible to all workers (shared storage)\n",
    "        self.chroma_client = chromadb.PersistentClient(\n",
    "            path=\"/mnt/cluster_storage/vector_store\"\n",
    "        )\n",
    "        self.collection = self.chroma_client.get_or_create_collection(\n",
    "            name=\"persistent_text_chunks\"\n",
    "        )\n",
    "\n",
    "    @ray.method(concurrency_group=\"write\")\n",
    "    def upsert(self, batch):\n",
    "        \"\"\"Insert or update documents in the collection.\"\"\"\n",
    "        self.collection.upsert(\n",
    "            embeddings=batch['vec'].tolist(),\n",
    "            documents=batch['doc'].tolist(),\n",
    "            ids=batch['id'].tolist()\n",
    "        )\n",
    "        return len(batch['id'])\n",
    "\n",
    "    @ray.method(concurrency_group=\"read\")\n",
    "    def query(self, q):\n",
    "        \"\"\"Query for similar documents.\"\"\"\n",
    "        return self.collection.query(query_embeddings=[q], n_results=3)\n",
    "\n",
    "# Create a single actor instance that all workers will share\n",
    "chroma_server = ChromaWrapper.remote()\n",
    "print(\"ChromaWrapper actor created and ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867eb1e-cc4b-41dd-9852-7f04a8f1aaf7",
   "metadata": {},
   "source": [
    "### Bulk Loading with map_batches\n",
    "\n",
    "We use `map_batches` with a lambda to send each batch to our ChromaWrapper actor. This provides a simple way to bulk-load data into the vector store.\n",
    "\n",
    "**Alternative approaches:**\n",
    "- Write a custom sink for cleaner API: `dataset.write_vectordb('collection')`\n",
    "- Write to Parquet first, then bulk load in a separate step\n",
    "- Use actor-based callable class (exercise: try implementing this!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f705c52c-e9d0-45b9-bf7a-064327e569a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:00:34,667\tINFO dataset.py:3641 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2026-01-22 01:00:34,669\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_136_0\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/anyscale/data/_internal/util/dependencies.py:42: UserWarning: Numba isn't available. Install numba>=0.61>=0.61 to get better performance for hash partitioning operations. Falling back to slower Python implementation for RayTurbo optimizations.\n",
      "  warnings.warn(\n",
      "2026-01-22 01:00:34,677\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_136_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 01:00:34,678\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_136_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> ActorPoolMapOperator[MapBatches(DocEmbedder)] -> TaskPoolMapOperator[MapBatches(<lambda>)] -> HashAggregateOperator[HashAggregate(key_columns=(), num_partitions=1)] -> LimitOperator[limit=1]\n",
      "2026-01-22 01:00:34,861\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 01:00:34,863\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:34,864\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 01:00:34,865\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:34,866\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(DocEmbedder)} ===\n",
      "2026-01-22 01:00:34,866\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:34,867\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(<lambda>)} ===\n",
      "2026-01-22 01:00:34,868\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:34,869\tINFO progress_bar.py:213 -- === Ray Data Progress {HashAggregate(key_columns=(), num_partitions=1)} ===\n",
      "2026-01-22 01:00:34,869\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:34,870\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=1} ===\n",
      "2026-01-22 01:00:34,870\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:34,871\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 01:00:34,872\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 0.25/32 CPU, 0.0B/26.9GiB object store (pending: 2 CPU): Progress Completed 0 / ?\n",
      "2026-01-22 01:00:39,949\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:00:39,950\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:00:39,951\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 1 (368.6KiB); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:39,952\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:39,953\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:39,954\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:39,955\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 0.25/32 CPU, 368.6KiB/26.9GiB object store (pending: 2 CPU): Progress Completed 0 / ?\n",
      "2026-01-22 01:00:44,980\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:00:44,980\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:00:44,981\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:44,983\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:44,984\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:44,985\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:44,986\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:50,068\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:00:50,069\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:00:50,069\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:50,070\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:50,071\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:50,072\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:50,072\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:55,172\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:00:55,175\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:00:55,177\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:55,179\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:55,181\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:55,182\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:00:55,184\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:00,195\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:01:00,196\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.6KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:01:00,197\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:00,198\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:00,198\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:00,199\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:00,200\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:05,216\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:01:05,217\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 369.1KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:01:05,217\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:05,219\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:05,219\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:05,220\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:05,221\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:10,217\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:01:10,218\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 369.1KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:01:10,219\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:10,219\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:10,220\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:10,221\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:10,222\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:15,240\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:01:15,241\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 369.1KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:01:15,243\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:15,244\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:15,245\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:15,246\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:15,247\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:20,340\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:01:20,341\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 369.1KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:01:20,341\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:20,342\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:20,343\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:20,344\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:20,345\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:25,390\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:01:25,391\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 369.1KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:01:25,392\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:25,393\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:25,394\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:25,395\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:25,396\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:30,465\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:01:30,465\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 369.1KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:01:30,466\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:30,467\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:30,468\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:30,468\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:30,469\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:35,527\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:01:35,527\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 369.1KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:01:35,528\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:35,529\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:35,529\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:35,530\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:35,531\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:40,603\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:01:40,604\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 369.1KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:01:40,605\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:40,606\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:40,607\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:40,607\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:40,608\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:45,679\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:01:45,680\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 369.1KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:01:45,681\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:45,681\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:45,682\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:45,683\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:45,683\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:50,764\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:01:50,765\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 369.1KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:01:50,766\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:50,767\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:50,767\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:50,768\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:50,769\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:55,832\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:01:55,835\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 369.1KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:01:55,837\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:55,838\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:55,840\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:55,841\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:01:55,843\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:02:00,973\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:02:00,979\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 369.1KiB object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:02:00,981\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:02:00,982\tINFO progress_bar.py:215 -- MapBatches(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:02:00,984\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:02:00,985\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:02:00,986\tINFO progress_bar.py:215 -- Running Dataset: dataset_136_0. Active & requested resources: 1.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:02:05,062\tINFO progress_bar.py:213 -- === Ray Data Progress {*- Shuffle} ===\n",
      "2026-01-22 01:02:05,064\tINFO progress_bar.py:215 -- *- Shuffle: Progress Completed 1 / 1\n",
      "2026-01-22 01:02:05,081\tINFO progress_bar.py:213 -- === Ray Data Progress {*- Aggregation} ===\n",
      "2026-01-22 01:02:05,082\tINFO progress_bar.py:215 -- *- Aggregation: Progress Completed 1 / ?\n",
      "2026-01-22 01:02:05,166\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_136_0 execution finished in 90.48 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents loaded into ChromaDB: 1654\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BULK LOAD ALL EMBEDDINGS INTO CHROMADB\n",
    "# ============================================================================\n",
    "# This uses map_batches with a lambda to send each batch to the actor\n",
    "# The lambda calls the actor's upsert method and returns the batch count\n",
    "\n",
    "total_docs = vecs.map_batches(\n",
    "    lambda batch: {'batch_count': [ray.get(chroma_server.upsert.remote(batch))]}\n",
    ").sum('batch_count')\n",
    "\n",
    "print(f\"Total documents loaded into ChromaDB: {total_docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134d7ec-f01c-48b1-beb4-8e142f54dcf2",
   "metadata": {},
   "source": [
    "### Testing the Persistent Vector Store\n",
    "\n",
    "Since our ChromaWrapper is a running actor, we can query it directly without going through a dataset pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e9f13cb-c2fb-4c0b-b72f-9929984a934c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'Describe the body of water in Utah'\n",
      "\n",
      "Top 3 most relevant passages:\n",
      "\n",
      "1. (distance: 0.9195)\n",
      "   The Salt Lake, seventy miles long and thirty-five wide, is situated three miles eight hundred feet above the sea. Quite different from Lake Asphaltite, whose depression is twelve hundred feet below th...\n",
      "\n",
      "2. (distance: 0.9195)\n",
      "   The Salt Lake, seventy miles long and thirty-five wide, is situated three miles eight hundred feet above the sea. Quite different from Lake Asphaltite, whose depression is twelve hundred feet below th...\n",
      "\n",
      "3. (distance: 0.9804)\n",
      "   During the lecture the train had been making good progress, and towards half-past twelve it reached the northwest border of the Great Salt Lake. Thence the passengers could observe the vast extent of ...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SEMANTIC SEARCH DEMO\n",
    "# ============================================================================\n",
    "# Query the persistent vector store for documents about Utah\n",
    "# The Great Salt Lake is mentioned in \"Around the World in 80 Days\"\n",
    "\n",
    "# Encode the query\n",
    "utah_query_vec = model.encode(\"Describe the body of water in Utah\").tolist()\n",
    "\n",
    "# Query the actor (returns a future, use ray.get() to retrieve the result)\n",
    "results = ray.get(chroma_server.query.remote(utah_query_vec))\n",
    "\n",
    "# Display results\n",
    "print(\"Query: 'Describe the body of water in Utah'\")\n",
    "print(\"\\nTop 3 most relevant passages:\")\n",
    "for i, doc in enumerate(results['documents'][0]):\n",
    "    distance = results['distances'][0][i]\n",
    "    print(f\"\\n{i+1}. (distance: {distance:.4f})\")\n",
    "    print(f\"   {doc[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489741a7-ecb7-46db-a861-6c7869902d01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this module, we covered:\n",
    "\n",
    "```bash\n",
    "┌────────────────────────────────────────────────────────────────────────────┐\n",
    "│                          Key Concepts Learned                              │\n",
    "└────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "1. EMBEDDINGS\n",
    "   ├── Dense vector representations of text (384 dimensions with MiniLM)\n",
    "   ├── Similar meanings → Similar vectors\n",
    "   └── Enable semantic search (beyond keyword matching)\n",
    "\n",
    "2. CALLABLE CLASS PATTERN\n",
    "   ├── __init__: Expensive setup (model loading) - runs ONCE\n",
    "   ├── __call__: Batch processing - runs MANY times\n",
    "   └── Essential for stateful actors with map_batches()\n",
    "\n",
    "3. RAY DATA INTEGRATION\n",
    "   ├── read_text() → Load data\n",
    "   ├── map_batches() → Distributed processing\n",
    "   └── ActorPoolStrategy → Parallel execution with state\n",
    "\n",
    "4. VECTOR DATABASE (ChromaDB)\n",
    "   ├── upsert() → Store documents with embeddings\n",
    "   ├── query() → Find similar documents\n",
    "   └── PersistentClient → Durable storage\n",
    "\n",
    "5. RAY ACTORS\n",
    "   ├── Wrap stateful services (databases, models)\n",
    "   ├── Concurrency groups → Control parallelism\n",
    "   └── Accessible from any Ray worker\n",
    "```\n",
    "\n",
    "**Next Steps:** In Module 3, we'll add LLM inference to generate responses using the retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cabc68",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
