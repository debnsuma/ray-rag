{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95c6f1-1a1b-4264-9dbe-856aecd8ea2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import ray\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11870fdf-54f7-4b09-83bb-7f3dbaa8a482",
   "metadata": {},
   "source": [
    "# Ray Data Pipelines for RAG Applications\n",
    "\n",
    "Our initial goal is to use Ray Data to implement a RAG pipeline for the following flow:\n",
    "\n",
    "[Read Queries] => [Generate Embeddings] => [Retrieve Matching Docs] => [Build LLM Prompts] => [Get LLM Responses] => [Store Output]\n",
    "\n",
    "Once we have that working, we'll look at a couple of other patterns for working with multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e83f2a-b56f-41b9-a192-a28dfc9f18ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDER_MODEL = 'hkunlp/instructor-large'\n",
    "CHAT_MODEL = 'Qwen/Qwen2.5-0.5B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa90229-7289-4bc8-929b-252a0f0ac0b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = ray.data.read_parquet('/mnt/cluster_storage/prompts.parquet')\n",
    "data.take_batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0248c8-7f6c-4c56-b0d8-2dde486c27df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Embedder:\n",
    "    def __init__(self, model: str):\n",
    "        self._model = SentenceTransformer(EMBEDDER_MODEL)\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch['prompt_embedding'] = self._model.encode(batch['prompt'], device='cuda:0')\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146158a-fa8e-4102-a568-4c64ee60956f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data \\\n",
    "    .map_batches(Embedder, fn_constructor_args=[EMBEDDER_MODEL], concurrency=2, num_gpus=0.1, batch_size=4) \\\n",
    "    .take_batch(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2b546-05b0-46f4-a458-7a0ed4d8bf17",
   "metadata": {},
   "source": [
    "We can implement a vector-db lookup service as an actor and use it for batch retrieval of documents matching a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb111474-cb01-44ce-bbfb-e3a2af48e785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ChromaDBReader:\n",
    "    def __init__(self, collection: str, top_n: int):        \n",
    "        chroma_client = chromadb.PersistentClient(path=\"/mnt/cluster_storage/vector_store\")\n",
    "        self._coll = chroma_client.get_collection(collection)\n",
    "        self._top_n = top_n\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        vecs = list(batch['prompt_embedding'])\n",
    "        batch['responsive_documents'] = self._coll.query(query_embeddings=vecs, n_results=self._top_n,)['documents']\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37c874-6981-40a9-b17d-fba0df7cd5d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data \\\n",
    "    .map_batches(Embedder, fn_constructor_args=[EMBEDDER_MODEL], concurrency=2, num_gpus=0.1, batch_size=4) \\\n",
    "    .map_batches(ChromaDBReader, fn_constructor_args=['persistent_text_chunks', 3], concurrency=2) \\\n",
    "    .take_batch(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfb37f0-edec-4e32-a87f-bd4fffd50a6f",
   "metadata": {},
   "source": [
    "There may be some retrieval quality issue, but those are not our concern right now.\n",
    "\n",
    "Next, we can create a component to enhance the prompt with context and instructions for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59368e-068b-4b41-9975-96168e5e3cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PromptEnhancer:\n",
    "    def __init__(self):\n",
    "        self._base_prompt = \"\"\"You are a helpful assistant who can answer questions about a text based on your existing knowledge and documents supplied here.\n",
    "        When answering questions, use the following relevant excerpts from the text:\n",
    "        { newline.join([doc for doc in docs]) } \n",
    "        If you don't have information to answer a question, please say you don't know. Don't make up an answer.\\n\"\"\"\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        original_prompts = batch['prompt']\n",
    "        enhanced_prompts = []\n",
    "        newline = '\\n'\n",
    "        \n",
    "        for ix, original_prompt in enumerate(original_prompts):\n",
    "            docs = batch['responsive_documents'][ix]\n",
    "            enhanced_prompts.append([ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                                      {\"role\": \"user\", \"content\": eval(f'f\"\"\"{self._base_prompt}\"\"\"') + original_prompt } ])\n",
    "\n",
    "        batch['enhanced_prompt'] = enhanced_prompts\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d529d5f8-f963-4618-b63e-766c6b147d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data \\\n",
    "    .map_batches(Embedder, fn_constructor_args=[EMBEDDER_MODEL], concurrency=2, num_gpus=0.1, batch_size=4) \\\n",
    "    .map_batches(ChromaDBReader, fn_constructor_args=['persistent_text_chunks', 3], concurrency=2) \\\n",
    "    .map_batches(PromptEnhancer, concurrency=2) \\\n",
    "    .take_batch(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448e778-85fd-49c1-acf4-e432e7ac409d",
   "metadata": {},
   "source": [
    "And now we can add out batch LLM processing to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c793f29-5292-4fe4-8e5d-6050b9f535e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        self.pipe = pipeline(\"text-generation\", model=model, device='cuda:0', model_kwargs={\"cache_dir\": \"/mnt/local_storage\"})\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        enhanced_prompts = [[j for j in i] for i in batch['enhanced_prompt']] # nested arrays to nested lists -- adjust as needed and/or for perf\n",
    "        batch['responses'] = self.pipe(enhanced_prompts, max_new_tokens=200, truncation=True)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408393f-fd91-41b3-a9fb-1bc193620dee",
   "metadata": {},
   "source": [
    "Since the output is getting larger at this point, for visual inspection we'll store a batch to a Python object and then print out some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c7970-8680-4363-90f4-3d92e99d989b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = data \\\n",
    "    .map_batches(Embedder, fn_constructor_args=[EMBEDDER_MODEL], concurrency=4, num_gpus=0.1, batch_size=4) \\\n",
    "    .map_batches(ChromaDBReader, fn_constructor_args=['persistent_text_chunks', 3], concurrency=2) \\\n",
    "    .map_batches(PromptEnhancer, concurrency=2) \\\n",
    "    .map_batches(Chat, concurrency=2, fn_constructor_args=[CHAT_MODEL], num_gpus=0.15, batch_size=4) \\\n",
    "    .take_batch(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be3b7fd-251e-4a2c-880b-2e32537b5821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_visual_eval(batch):\n",
    "    for r in batch['responses']:\n",
    "        print(r[0]['generated_text'][1]['content'].split('\\n')[-1])\n",
    "        print()\n",
    "        print(r[0]['generated_text'][2]['content'])\n",
    "        print('----------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44780ef2-ff3e-493b-b089-c3bcfb6b43b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_visual_eval(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb06cc-1121-489b-9108-5504e471f326",
   "metadata": {},
   "source": [
    "If we were happy with the pipeline, we might run it at larger scale and write the ouput to storage, into a database, kafka, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7670db3-2cd4-455c-98f0-e87258db44bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.data.read_parquet('/mnt/cluster_storage/prompts.parquet') \\\n",
    "    .map_batches(Embedder, fn_constructor_args=[EMBEDDER_MODEL], concurrency=4, num_gpus=0.1, batch_size=4) \\\n",
    "    .map_batches(ChromaDBReader, fn_constructor_args=['persistent_text_chunks', 3], concurrency=2) \\\n",
    "    .map_batches(PromptEnhancer, concurrency=2) \\\n",
    "    .map_batches(Chat, concurrency=2, fn_constructor_args=[CHAT_MODEL], num_gpus=0.15, batch_size=4) \\\n",
    "    .write_parquet('/mnt/cluster_storage/batch_output_1.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf1953-d379-467d-a7e7-d533aa83e7b6",
   "metadata": {},
   "source": [
    "## Multimodel pipelines with routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3bfe22-896c-4083-86f1-3fa3c1fc12b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BIGGER_CHAT_MODEL='Qwen/Qwen2.5-1.5B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e9b97d-43e2-49fd-b021-f9abb64d1612",
   "metadata": {},
   "source": [
    "The simplest way -- though possibly not the best way -- to do model routing would be\n",
    "* create a dataprocessing actor class like the `Chat` class\n",
    "* insert business logic to\n",
    "    * load multiple models in the constructor and use them for various subsets of the record batch, depending on some criteria or control flow\n",
    "    * or run the control flow and then call out to some other service -- e.g., another Actor or Actor Pool -- to do the inference\n",
    "* collect the results\n",
    "* return the updated batch\n",
    "\n",
    "However, there are some patterns which may allow for more optimization and tuning.\n",
    "\n",
    "First, we'll look at using a router actor which chooses a target model for each record (at random, in this example).\n",
    "\n",
    "We'll then pass the data in sequence to 2 different `FilteredChat` processing actors which only handle the records assigned to them.\n",
    "\n",
    "> This pattern can also be expanded. For example, perhaps we score every record with the first (small) model, then the router applied some evaluation model to rate the results and assigns unsatisfactory records to be scored (again) with the second (larger, more sophisticated, but more expensive) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ffdc6-c613-415b-b7b9-ea94c266fe3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Router:\n",
    "    def __init__(self, models):\n",
    "        self._models = models\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        batch['target_model'] = random.choices(self._models, k=len(batch['prompt']))\n",
    "        return batch\n",
    "    \n",
    "class FilteredChat:\n",
    "    def __init__(self, model: str):\n",
    "        self._model = model\n",
    "        self.pipe = pipeline(\"text-generation\", model=model, device='cuda:0', model_kwargs={\"cache_dir\": \"/mnt/local_storage\"})\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        indices = np.argwhere(batch['target_model']==self._model).flatten()\n",
    "        \n",
    "        prompts = batch['enhanced_prompt'][indices]\n",
    "        prompts = [[j for j in i] for i in prompts]     \n",
    "        responses = self.pipe(prompts, max_new_tokens=200, truncation=True)\n",
    "        \n",
    "        if not 'responses' in batch:\n",
    "            batch['responses'] = np.empty(len(batch['enhanced_prompt']), dtype=object)\n",
    "        batch['responses'][indices] = responses\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82573ae9-1b85-41d5-a000-6ed95072f7cd",
   "metadata": {},
   "source": [
    "We can also experiment with more docs from the vector store, different concurrency numbers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418eb316-4f5e-43d2-9139-e78ae44b46ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.data.read_parquet('/mnt/cluster_storage/prompts.parquet') \\\n",
    "    .repartition(4) \\\n",
    "    .map_batches(Embedder, fn_constructor_args=[EMBEDDER_MODEL], concurrency=4, num_gpus=0.1, batch_size=4) \\\n",
    "    .map_batches(ChromaDBReader, fn_constructor_args=['persistent_text_chunks', 5], concurrency=4) \\\n",
    "    .map_batches(PromptEnhancer, concurrency=4) \\\n",
    "    .map_batches(Router, concurrency=2, fn_constructor_args=[[CHAT_MODEL, BIGGER_CHAT_MODEL]]) \\\n",
    "    .map_batches(FilteredChat, concurrency=2, fn_constructor_args=[CHAT_MODEL], num_gpus=0.15, batch_size=8) \\\n",
    "    .map_batches(FilteredChat, concurrency=2, fn_constructor_args=[BIGGER_CHAT_MODEL], num_gpus=0.5, batch_size=8) \\\n",
    "    .take_batch(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c197a9-3f12-40cf-97d2-b8da8583eafb",
   "metadata": {},
   "source": [
    "One additional Ray Data pattern is to split the dataset using the `.filter` API and then send the filtered streams to dedicated `Chat` processors for the respective models.\n",
    "* This pattern might have potential benefits in keeping the batch sizes uniform at the point of LLM inference.\n",
    "\n",
    "However, Ray by default would read and pre-process the whole dataset before running the filter and then the inference on the target subset.\n",
    "* We can work around this by using `.materialize` to cache the dataset at the point where we are ready to \"branch\" it with filter operations.\n",
    "* This caching approach will use up object store memory and, most likely for a large dataset, will also spill to disk across the nodes.\n",
    "    * Caching + spilling is not necessarily a problem but we should understand the tradeoffs, especially for very large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af697ee3-07c9-4fa3-8b47-af811de0bd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ready_to_filter = ray.data.read_parquet('/mnt/cluster_storage/prompts.parquet') \\\n",
    "    .repartition(4) \\\n",
    "    .map_batches(Embedder, fn_constructor_args=[EMBEDDER_MODEL], concurrency=4, num_gpus=0.1, batch_size=4) \\\n",
    "    .map_batches(ChromaDBReader, fn_constructor_args=['persistent_text_chunks', 5], concurrency=4) \\\n",
    "    .map_batches(PromptEnhancer, concurrency=4) \\\n",
    "    .map_batches(Router, concurrency=2, fn_constructor_args=[[CHAT_MODEL, BIGGER_CHAT_MODEL]]) \\\n",
    "    .materialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620be8e0-ad91-40f9-ab67-4ed211756522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "small_model_outputs = ready_to_filter \\\n",
    "    .filter(expr=f\"target_model=='{CHAT_MODEL}'\") \\\n",
    "    .map_batches(Chat, concurrency=2, fn_constructor_args=[CHAT_MODEL], num_gpus=0.15, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a39486-5b69-424e-9337-421b37aea3dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "large_model_outputs = ready_to_filter \\\n",
    "    .filter(expr=f\"target_model=='{BIGGER_CHAT_MODEL}'\") \\\n",
    "    .map_batches(Chat, concurrency=2, fn_constructor_args=[BIGGER_CHAT_MODEL], num_gpus=0.5, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002a0e1-2e0b-4be6-9759-724b7201798c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = small_model_outputs.union(large_model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ace46-9176-4ef2-be66-03ed92826faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.take_batch(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1a145-04b8-4e07-a547-6b79bfc6a751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
