{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "m5ia14nzmyn",
   "metadata": {},
   "source": [
    "# RAG Pipeline with Ray Data\n",
    "\n",
    "[![Ray](https://img.shields.io/badge/Ray-Data-blue)](https://docs.ray.io/en/latest/data/data.html)\n",
    "[![Python](https://img.shields.io/badge/Python-3.8+-green)](https://python.org)\n",
    "\n",
    "Build a scalable **Retrieval-Augmented Generation (RAG)** pipeline using Ray Data for distributed processing.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to build a production-ready RAG pipeline that:\n",
    "- Scales across multiple nodes and GPUs\n",
    "- Uses vector databases for semantic search\n",
    "- Combines retrieval with LLM generation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand RAG Architecture**: Learn how Retrieval-Augmented Generation combines vector search with LLMs\n",
    "2. **Build Scalable Pipelines**: Use Ray Data to create distributed data processing pipelines\n",
    "3. **Implement Key Components**: Create embedding generators, vector database readers, and LLM inference stages\n",
    "4. **Chain Operations**: Connect multiple processing stages using Ray Data's `map_batches` API\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "RAG is a technique that enhances Large Language Models (LLMs) by providing them with relevant context retrieved from a knowledge base:\n",
    "\n",
    "```\n",
    "┌────────────┐      ┌────────────┐      ┌────────────┐\n",
    "│   User     │      │  Retrieved │      │    LLM     │\n",
    "│  Question  │  +   │  Context   │  =   │  Response  │\n",
    "└────────────┘      └────────────┘      └────────────┘\n",
    "```\n",
    "\n",
    "This approach helps:\n",
    "- **Reduce hallucinations**: Ground responses in actual documents\n",
    "- **Enable domain knowledge**: Answer questions about specific documents\n",
    "- **Stay up-to-date**: Use current information not in training data\n",
    "\n",
    "## Why Ray Data for RAG?\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|---------|\n",
    "| **Scalability** | Distribute processing across multiple nodes and GPUs |\n",
    "| **Streaming** | Process data without loading everything into memory |\n",
    "| **Actor Pools** | Efficiently manage stateful resources like ML models |\n",
    "| **Resource Management** | Fine-grained control over CPU, GPU, and memory |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf46bb5",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ylu4wlnafoe",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Before we begin, let's import the necessary libraries:\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|---------|\n",
    "| `sentence_transformers` | Generate text embeddings (convert text to numerical vectors) |\n",
    "| `chromadb` | Vector database for storing and querying embeddings |\n",
    "| `ray` | Distributed computing framework for scaling our pipeline |\n",
    "| `transformers` | Hugging Face library for LLM inference |\n",
    "| `numpy` | Numerical operations on arrays |\n",
    "\n",
    "> **Key Concept**: Embeddings are numerical representations of text that capture semantic meaning. Similar texts will have similar embeddings (close in vector space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc95c6f1-1a1b-4264-9dbe-856aecd8ea2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import ray\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import uuid\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11870fdf-54f7-4b09-83bb-7f3dbaa8a482",
   "metadata": {},
   "source": [
    "### RAG Pipeline Architecture\n",
    "\n",
    "Our goal is to build a complete RAG pipeline using Ray Data. Here's the high-level architecture:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         RAG PIPELINE ARCHITECTURE                                │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "                    ┌──────────────────────────────────────────┐\n",
    "                    │           DOCUMENT INGESTION              │\n",
    "                    │            (One-time Setup)               │\n",
    "                    └──────────────────────────────────────────┘\n",
    "                                       │\n",
    "    ┌──────────────┐     ┌──────────────┐     ┌──────────────┐\n",
    "    │  Source Text │ ──▶ │ DocEmbedder  │ ──▶ │   ChromaDB   │\n",
    "    │   (around.   │     │  (Generate   │     │   (Vector    │\n",
    "    │     txt)     │     │  Embeddings) │     │   Database)  │\n",
    "    └──────────────┘     └──────────────┘     └──────────────┘\n",
    "                                                     │\n",
    "                    ┌──────────────────────────────────────────┐\n",
    "                    │           QUERY PIPELINE                  │\n",
    "                    │         (Runtime Execution)               │\n",
    "                    └──────────────────────────────────────────┘\n",
    "                                       │\n",
    "    ┌──────────────┐     ┌──────────────┐     ┌──────────────┐\n",
    "    │    User      │ ──▶ │   Embedder   │ ──▶ │ ChromaDB     │\n",
    "    │   Prompts    │     │  (Vectorize  │     │   Reader     │\n",
    "    │  (Parquet)   │     │   Queries)   │     │ (Retrieve)   │\n",
    "    └──────────────┘     └──────────────┘     └──────────────┘\n",
    "                                                     │\n",
    "                                                     ▼\n",
    "                         ┌──────────────┐     ┌──────────────┐\n",
    "                         │   Prompt     │ ──▶ │    Chat      │\n",
    "                         │  Enhancer    │     │   (LLM)      │\n",
    "                         │ (Augment)    │     │  Response    │\n",
    "                         └──────────────┘     └──────────────┘\n",
    "                                                     │\n",
    "                                                     ▼\n",
    "                                          ┌──────────────────┐\n",
    "                                          │  Output (Parquet │\n",
    "                                          │   or Memory)     │\n",
    "                                          └──────────────────┘\n",
    "```\n",
    "\n",
    "### Pipeline Stages Overview\n",
    "\n",
    "| Stage | Component | Description | Resource |\n",
    "|-------|-----------|-------------|----------|\n",
    "| 1 | `ray.data.read_parquet()` | Load user questions from storage | CPU |\n",
    "| 2 | `Embedder` class | Convert questions to vectors | CPU |\n",
    "| 3 | `ChromaDBReader` class | Query vector database for similar documents | CPU |\n",
    "| 4 | `PromptEnhancer` class | Combine retrieved context with original question | CPU |\n",
    "| 5 | `Chat` class | Generate responses using an LLM | **GPU** |\n",
    "| 6 | `write_parquet()` | Save results to storage | CPU |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5128e3d",
   "metadata": {},
   "source": [
    "## Step 2: Configure Model Names\n",
    "\n",
    "We define our model names as constants for easy configuration. This makes it simple to swap models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e83f2a-b56f-41b9-a192-a28dfc9f18ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model: all-MiniLM-L6-v2\n",
      "Chat Model: Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Model Configuration\n",
    "# ============================================================================\n",
    "# Define the models used in our RAG pipeline. These are easily swappable.\n",
    "#\n",
    "# IMPORTANT: The embedding model used for QUERIES must match the model used\n",
    "# for DOCUMENT INGESTION. Using different models will result in mismatched\n",
    "# vector spaces and poor retrieval quality.\n",
    "# ============================================================================\n",
    "\n",
    "# Embedding model: converts text to dense vectors\n",
    "EMBEDDER_MODEL = 'all-MiniLM-L6-v2'\n",
    "\n",
    "# Chat/LLM model: generates responses from prompts\n",
    "CHAT_MODEL = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "\n",
    "print(f\"Embedding Model: {EMBEDDER_MODEL}\")\n",
    "print(f\"Chat Model: {CHAT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16h5baojz43",
   "metadata": {},
   "source": [
    "**Understanding the Models**:\n",
    "\n",
    "- **`all-MiniLM-L6-v2`**: A lightweight embedding model (23M parameters) that converts text into 384-dimensional vectors. It's fast and efficient, making it ideal for demos and smaller deployments. Runs well on CPU.\n",
    "\n",
    "- **`Qwen/Qwen2.5-0.5B-Instruct`**: A small but capable instruction-tuned LLM (500M parameters). We use a smaller model here for efficiency, but you can swap it for larger models in production.\n",
    "\n",
    "**Note on Model Selection**:\n",
    "- The embedding model must be the SAME for both document ingestion and query embedding\n",
    "- Using mismatched models results in incompatible vector spaces and poor retrieval\n",
    "- For production, consider larger models like `hkunlp/instructor-large` (with GPU) for better quality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6872e11",
   "metadata": {},
   "source": [
    "## Step 3: Load the Input Data\n",
    "\n",
    "We use `ray.data.read_parquet()` to load our prompts. Ray Data creates a **lazy dataset** that only reads data when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa90229-7289-4bc8-929b-252a0f0ac0b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:02:59,123\tINFO worker.py:1821 -- Connecting to existing Ray cluster at address: 10.0.9.248:6379...\n",
      "2026-01-22 01:02:59,139\tINFO worker.py:1998 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-v4klp1kjtnk9yrxwdcz5ah11ub.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-01-22 01:02:59,174\tINFO packaging.py:463 -- Pushing file package 'gcs://_ray_pkg_0b93c89d52ed901a67a4cef3e5464d9a908cdc11.zip' (10.62MiB) to Ray cluster...\n",
      "2026-01-22 01:02:59,231\tINFO packaging.py:476 -- Successfully pushed file package 'gcs://_ray_pkg_0b93c89d52ed901a67a4cef3e5464d9a908cdc11.zip'.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "2026-01-22 01:02:59,679\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_138_0\n",
      "2026-01-22 01:02:59,703\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_138_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 01:02:59,704\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_138_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> LimitOperator[limit=4]\n",
      "2026-01-22 01:02:59,705\tINFO streaming_executor.py:687 -- [dataset]: A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True` and `ray.data.DataContext.get_current().use_ray_tqdm = False`.\n",
      "2026-01-22 01:02:59,706\tINFO progress_bar.py:155 -- Progress bar disabled because stdout is a non-interactive terminal.\n",
      "2026-01-22 01:02:59,708\tWARNING resource_manager.py:136 -- ⚠️  Ray's object store is configured to use only 27.8% of available memory (40.0GiB out of 144.0GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "2026-01-22 01:02:59,752\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 01:02:59,753\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:02:59,755\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 01:02:59,756\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:02:59,757\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=4} ===\n",
      "2026-01-22 01:02:59,757\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:02:59,758\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 01:02:59,759\tINFO progress_bar.py:215 -- Running Dataset: dataset_138_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:04,834\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 61.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:03:04,835\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:04,836\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:04,836\tINFO progress_bar.py:215 -- Running Dataset: dataset_138_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:10,394\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 61.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:03:10,395\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 3.0KiB object store: Progress Completed 23 / ?\n",
      "2026-01-22 01:03:10,396\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 189.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:10,397\tINFO progress_bar.py:215 -- Running Dataset: dataset_138_0. Active & requested resources: 1/32 CPU, 4.6KiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:10,411\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_138_0 execution finished in 10.71 seconds\n",
      "INFO:openlineage.client.client:OpenLineageClient will use `composite` transport\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt': array(['Describe the body of water in Utah?',\n",
       "        'Tell as much as you can about the robbery?',\n",
       "        'Did Phileas Fogg really rob the bank?',\n",
       "        'Who is the main protagonist of Around the World in 80 Days?'],\n",
       "       dtype=object)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ray.data.read_parquet('/mnt/cluster_storage/prompts.parquet')\n",
    "data.take_batch(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nve1tlr61ue",
   "metadata": {},
   "source": [
    "The output shows us sample prompts from our dataset. Notice these are questions about various topics - this is what users might ask in a RAG system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2e23a",
   "metadata": {},
   "source": [
    "## Step 4: Create the Embedding Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6affa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "Embeddings are dense vector representations of text that capture semantic meaning:\n",
    "- Similar texts produce similar vectors (close in vector space)\n",
    "- We can measure similarity using cosine similarity or Euclidean distance\n",
    "- This allows us to find relevant documents by comparing query embeddings to document embeddings\n",
    "\n",
    "\n",
    "> **Why this pattern?** Loading a 100MB+ model for every batch would be extremely slow. By loading once in `__init__`, we amortize the cost across many batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0248c8-7f6c-4c56-b0d8-2dde486c27df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Embedder Class: Converting Text to Vector Embeddings\n",
    "# ============================================================================\n",
    "# This class demonstrates the \"callable class\" pattern required by Ray Data\n",
    "# for stateful operations like ML model inference.\n",
    "#\n",
    "# Why use a class instead of a function?\n",
    "# - The model only needs to be loaded ONCE (in __init__)\n",
    "# - The model is reused across MANY batches (in __call__)\n",
    "# - Loading a model for every batch would be extremely slow!\n",
    "#\n",
    "# Key Pattern:\n",
    "#   __init__: Expensive one-time setup (load model, connect to DB, etc.)\n",
    "#   __call__: Fast per-batch processing (inference, queries, etc.)\n",
    "# ============================================================================\n",
    "\n",
    "class Embedder:\n",
    "    \"\"\"\n",
    "    Converts text prompts into dense vector embeddings using a sentence transformer.\n",
    "    \n",
    "    The embedding model is loaded once when the actor starts and reused for all batches.\n",
    "    This is much more efficient than loading the model for each batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: str):\n",
    "        self._device = 'cpu'\n",
    "        self._model = SentenceTransformer(model, device=self._device)\n",
    "        print(f\"Embedder initialized with model '{model}' on device: {self._device}\")\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Process a batch of prompts and add their embeddings.\n",
    "        \n",
    "        Args:\n",
    "            batch: Dict with 'prompt' key containing array of text strings\n",
    "            \n",
    "        Returns:\n",
    "            Dict with original data plus 'prompt_embedding' containing vectors\n",
    "        \"\"\"\n",
    "        # Generate embeddings for all prompts in the batch\n",
    "        # The encode() method handles batching internally for efficiency\n",
    "        batch['prompt_embedding'] = self._model.encode(\n",
    "            batch['prompt'], \n",
    "            batch_size=32\n",
    "        )\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vzsh4grxqjr",
   "metadata": {},
   "source": [
    "### Running the Embedder with Ray Data\n",
    "\n",
    "Now let's run our Embedder using `map_batches`. Key parameters explained:\n",
    "\n",
    "| Parameter | Value | Explanation |\n",
    "|-----------|-------|-------------|\n",
    "| `fn_constructor_args` | `[EMBEDDER_MODEL]` | Arguments passed to `__init__` |\n",
    "| `compute` | `ActorPoolStrategy(size=2)` | Use 2 persistent actors (not tasks) |\n",
    "| `batch_size` | `4` | Process 4 prompts at a time |\n",
    "\n",
    "### Why ActorPoolStrategy?\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────────┐\n",
    "│                 TASKS vs ACTORS COMPARISON                             │\n",
    "├────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                        │\n",
    "│  TASKS (stateless):                  ACTORS (stateful):                │\n",
    "│  ┌─────┐ ┌─────┐ ┌─────┐            ┌─────────────────────┐            │\n",
    "│  │Load │ │Load │ │Load │            │ Load Model ONCE     │            │\n",
    "│  │Model│ │Model│ │Model│            │     ↓               │            │\n",
    "│  │  ↓  │ │  ↓  │ │  ↓  │            │ Process Batch 1     │            │\n",
    "│  │Batch│ │Batch│ │Batch│            │ Process Batch 2     │            │\n",
    "│  │  1  │ │  2  │ │  3  │            │ Process Batch 3     │            │\n",
    "│  └─────┘ └─────┘ └─────┘            │     ...             │            │\n",
    "│     ↑        ↑       ↑              └─────────────────────┘            │\n",
    "│  SLOW! Model loaded 3x              FAST! Model loaded 1x              │\n",
    "│                                                                        │\n",
    "└────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "- **Actors** are long-lived processes that maintain state (our loaded model)\n",
    "- **Tasks** would load the model for each batch - very inefficient!\n",
    "- `size=2` means we run 2 actors in parallel for throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6146158a-fa8e-4102-a568-4c64ee60956f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:03:10,544\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_140_0\n",
      "2026-01-22 01:03:10,549\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_140_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 01:03:10,550\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_140_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> LimitOperator[limit=4] -> ActorPoolMapOperator[MapBatches(Embedder)]\n",
      "2026-01-22 01:03:10,722\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 01:03:10,724\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:10,724\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 01:03:10,726\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:10,727\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=4} ===\n",
      "2026-01-22 01:03:10,728\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:10,728\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(Embedder)} ===\n",
      "2026-01-22 01:03:10,729\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:10,730\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 01:03:10,731\tINFO progress_bar.py:215 -- Running Dataset: dataset_140_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store (pending: 2 CPU): Progress Completed 0 / ?\n",
      "2026-01-22 01:03:15,743\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:03:15,744\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:03:15,745\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 189.0B object store: Progress Completed 4 / 4\n",
      "2026-01-22 01:03:15,746\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 1 (189.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:15,746\tINFO progress_bar.py:215 -- Running Dataset: dataset_140_0. Active & requested resources: 0/32 CPU, 189.0B/26.9GiB object store (pending: 2 CPU): Progress Completed 0 / ?\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py:416: UserWarning: The minimum number of concurrent actors for 'MapBatches(Embedder)' is set to 2, but the operator only received 1 input(s). This means that the operator can launch at most 1 task(s), and won't fully utilize the available concurrency. You might be able to increase the number of concurrent tasks by configuring `override_num_blocks` earlier in the pipeline.\n",
      "  warnings.warn(\n",
      "2026-01-22 01:03:16,371\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_140_0 execution finished in 5.82 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(Embedder)) pid=26757, ip=10.0.45.231)\u001b[0m Embedder initialized with model 'all-MiniLM-L6-v2' on device: cpu\n",
      "Sample embeddings shape: (4, 384)\n",
      "Each prompt is converted to a 384 -dimensional vector\n",
      "\n",
      "First prompt: Describe the body of water in Utah?\n",
      "First 10 values of its embedding: [-0.01692119  0.05387887  0.02787627 -0.03565136 -0.01942815 -0.05900183\n",
      "  0.04629578 -0.01992714 -0.04378952 -0.01160184]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Demo: Running Just the Embedder Stage\n",
    "# ============================================================================\n",
    "# Let's test the Embedder in isolation to see what embeddings look like.\n",
    "#\n",
    "# Key Parameters Explained:\n",
    "# - fn_constructor_args: Arguments passed to __init__ (model name in this case)\n",
    "# - compute: ActorPoolStrategy creates persistent actors (NOT tasks)\n",
    "# - batch_size: Number of items to process at once\n",
    "#\n",
    "# Note: For the lightweight MiniLM model, we use CPU actors (no num_gpus needed)\n",
    "# ============================================================================\n",
    "\n",
    "embedder_output = data.map_batches(\n",
    "    Embedder, \n",
    "    fn_constructor_args=[EMBEDDER_MODEL],\n",
    "    compute=ray.data.ActorPoolStrategy(size=2),\n",
    "    batch_size=4      # Process 4 prompts per batch\n",
    ").take_batch(4)\n",
    "\n",
    "# Display the results\n",
    "print(\"Sample embeddings shape:\", embedder_output['prompt_embedding'].shape)\n",
    "print(\"Each prompt is converted to a\", embedder_output['prompt_embedding'].shape[1], \"-dimensional vector\")\n",
    "print(\"\\nFirst prompt:\", embedder_output['prompt'][0])\n",
    "print(\"First 10 values of its embedding:\", embedder_output['prompt_embedding'][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2b546-05b0-46f4-a458-7a0ed4d8bf17",
   "metadata": {},
   "source": [
    "The output shows our embeddings - 384-dimensional vectors (one for each prompt). These vectors capture the semantic meaning of our questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d72753",
   "metadata": {},
   "source": [
    "## Step 5: Set Up the Vector Database (ChromaDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e3c13",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### What is a Vector Database?\n",
    "\n",
    "A vector database is specialized storage for embeddings that enables:\n",
    "- **Fast similarity search**: Find similar vectors using approximate nearest neighbor (ANN) algorithms\n",
    "- **Persistence**: Store embeddings across sessions\n",
    "- **Scalability**: Handle millions or billions of vectors\n",
    "\n",
    "### ChromaDB Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Client** | Connection to the database (persistent or in-memory) |\n",
    "| **Collection** | A group of embeddings (like a table in SQL) |\n",
    "| **Documents** | The original text associated with each embedding |\n",
    "| **Query** | Find similar documents by comparing embeddings |\n",
    "\n",
    "> **Note**: In a real RAG system, you would pre-populate the vector database with your knowledge base. Here we create a fresh collection for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f77772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created collection 'persistent_text_chunks' at /mnt/cluster_storage/vector_store\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTANT: Initialize Vector Database with Fresh Collection\n",
    "# ============================================================================\n",
    "# Before running the RAG pipeline, we need a vector database populated with\n",
    "# document embeddings. This cell:\n",
    "# 1. Removes any existing vector store (for a clean demo)\n",
    "# 2. Creates a new persistent ChromaDB client\n",
    "# 3. Creates an empty collection that we'll populate in the next step\n",
    "#\n",
    "# Note: In production, you would NOT delete existing data. Instead, you'd\n",
    "# connect to an existing populated database or run an ingestion job separately.\n",
    "# ============================================================================\n",
    "\n",
    "import chromadb\n",
    "import shutil\n",
    "\n",
    "# Remove existing vector store for a clean start (just for the DEMO)\n",
    "shutil.rmtree(\"/mnt/cluster_storage/vector_store\", ignore_errors=True)\n",
    "\n",
    "# Create a persistent ChromaDB client\n",
    "# Using persistent storage ensures data survives cluster restarts\n",
    "client = chromadb.PersistentClient(path=\"/mnt/cluster_storage/vector_store\")\n",
    "\n",
    "# Create a new collection for our document chunks\n",
    "# The collection name should be descriptive of its contents\n",
    "collection = client.create_collection(\"persistent_text_chunks\")\n",
    "\n",
    "print(f\"Created collection 'persistent_text_chunks' at /mnt/cluster_storage/vector_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mw3mop0pb6",
   "metadata": {},
   "source": [
    "### Step 5.1: Populate the Vector Database with Document Embeddings\n",
    "\n",
    "**CRITICAL**: Before the RAG pipeline can retrieve relevant documents, we must first populate the vector database with embeddings of our knowledge base documents.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    DOCUMENT INGESTION FLOW                               │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "    around.txt                  DocEmbedder                   ChromaDB\n",
    "    ┌─────────┐                ┌───────────┐                ┌──────────┐\n",
    "    │ Line 1  │ ──────────────▶│           │──────────────▶│ ID: abc  │\n",
    "    │ Line 2  │     Filter     │ Generate  │    Upsert     │ Vec:[...]│\n",
    "    │ Line 3  │   (len > 10)   │ Embeddings│               │ Doc:\"...\"│\n",
    "    │   ...   │                │ + UUIDs   │               │          │\n",
    "    │ Line N  │                │           │               │ ID: xyz  │\n",
    "    └─────────┘                └───────────┘               │ Vec:[...]│\n",
    "                                                           │ Doc:\"...\"│\n",
    "                                                           └──────────┘\n",
    "```\n",
    "\n",
    "This step uses Ray Data to:\n",
    "1. **Read** the source text file\n",
    "2. **Generate embeddings** for each paragraph using the same embedding model\n",
    "3. **Store** the embeddings in ChromaDB with unique IDs\n",
    "\n",
    "**Why is this important?**\n",
    "- The RAG pipeline queries the vector database to find similar documents\n",
    "- Without pre-populated embeddings, there's nothing to retrieve!\n",
    "- This is typically a one-time ingestion process done before serving queries\n",
    "\n",
    "> **Production Note**: In production, you might run ingestion on a schedule to add new documents, use incremental updates, or monitor collection size and embedding quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qxb8zs2sccl",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/dataset.py:1560: UserWarning: Use 'expr' instead of 'fn' when possible for performant filters.\n",
      "  warnings.warn(\n",
      "2026-01-22 01:03:19,965\tINFO dataset.py:3641 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2026-01-22 01:03:19,969\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_146_0\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/anyscale/data/_internal/util/dependencies.py:42: UserWarning: Numba isn't available. Install numba>=0.61>=0.61 to get better performance for hash partitioning operations. Falling back to slower Python implementation for RayTurbo optimizations.\n",
      "  warnings.warn(\n",
      "2026-01-22 01:03:19,979\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_146_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 01:03:19,979\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_146_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[Filter(<lambda>)] -> ActorPoolMapOperator[MapBatches(DocEmbedder)] -> ActorPoolMapOperator[MapBatches(ChromaDBWriter)] -> HashAggregateOperator[HashAggregate(key_columns=(), num_partitions=1)] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document ingestion pipeline...\n",
      "Using embedding model: all-MiniLM-L6-v2\n",
      "This will read the source text, generate embeddings, and store them in ChromaDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:03:20,311\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 01:03:20,314\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:20,315\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 01:03:20,316\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:20,317\tINFO progress_bar.py:213 -- === Ray Data Progress {Filter(<lambda>)} ===\n",
      "2026-01-22 01:03:20,319\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:20,320\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(DocEmbedder)} ===\n",
      "2026-01-22 01:03:20,321\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 0; Actors: 4 (running=0, restarting=0, pending=4); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:20,322\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(ChromaDBWriter)} ===\n",
      "2026-01-22 01:03:20,324\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1 (running=0, restarting=0, pending=1); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:20,325\tINFO progress_bar.py:213 -- === Ray Data Progress {HashAggregate(key_columns=(), num_partitions=1)} ===\n",
      "2026-01-22 01:03:20,326\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:20,327\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=1} ===\n",
      "2026-01-22 01:03:20,328\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:20,330\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 01:03:20,331\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 0.25/32 CPU, 0.0B/26.9GiB object store (pending: 5 CPU): Progress Completed 0 / ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(ChromaDBWriter)) pid=29647, ip=10.0.50.252)\u001b[0m ChromaDBWriter connected to collection: persistent_text_chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:03:25,333\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:03:25,334\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:03:25,335\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.0KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:03:25,336\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 0; Actors: 4 (running=0, restarting=0, pending=4); Queued blocks: 1 (368.0KiB); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:25,337\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:25,338\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:25,339\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:25,340\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 1.25/32 CPU, 368.0KiB/26.9GiB object store (pending: 4 CPU): Progress Completed 0 / ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(DocEmbedder)) pid=26865, ip=10.0.45.231)\u001b[0m DocEmbedder initialized with model: all-MiniLM-L6-v2 on CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py:416: UserWarning: The minimum number of concurrent actors for 'MapBatches(DocEmbedder)' is set to 4, but the operator only received 1 input(s). This means that the operator can launch at most 1 task(s), and won't fully utilize the available concurrency. You might be able to increase the number of concurrent tasks by configuring `override_num_blocks` earlier in the pipeline.\n",
      "  warnings.warn(\n",
      "2026-01-22 01:03:30,432\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:03:30,433\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:03:30,434\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.0KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:03:30,435\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:30,436\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:30,437\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:30,437\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:30,438\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:35,525\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:03:35,526\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:03:35,527\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.0KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:03:35,528\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:35,528\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:35,529\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:35,530\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:35,531\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:40,630\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:03:40,631\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:03:40,632\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.0KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:03:40,632\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:40,633\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:40,634\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:40,635\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:40,635\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:45,729\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:03:45,730\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:03:45,731\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.0KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:03:45,732\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:45,733\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:45,734\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:45,734\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:45,735\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:50,820\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:03:50,821\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:03:50,821\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.5KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:03:50,822\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:50,822\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:50,823\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:50,824\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:50,825\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:55,910\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:03:55,911\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:03:55,912\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.5KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:03:55,912\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:55,913\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:55,914\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:55,915\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:03:55,915\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:01,005\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:04:01,006\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:04:01,007\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.5KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:04:01,008\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:01,008\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:01,009\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:01,010\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:01,010\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:06,094\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:04:06,094\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:04:06,095\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.5KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:04:06,096\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:06,097\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:06,097\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:06,098\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:06,099\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:11,188\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:04:11,188\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:04:11,189\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.5KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:04:11,190\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:11,191\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:11,191\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:11,192\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:11,193\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:16,290\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:04:16,291\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:04:16,292\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.5KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:04:16,293\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:16,294\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:16,295\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:16,296\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:16,297\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:21,383\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:04:21,384\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:04:21,385\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.5KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:04:21,385\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:21,386\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:21,387\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:21,387\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:21,388\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:26,482\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:04:26,483\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:04:26,484\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.5KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:04:26,485\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:26,486\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:26,487\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:26,487\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:26,488\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:31,577\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:04:31,578\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:04:31,579\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.5KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:04:31,580\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:31,580\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:31,581\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:31,582\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:31,582\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:36,664\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:04:36,665\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:04:36,666\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.5KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:04:36,667\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:36,668\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:36,669\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:36,670\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:36,671\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:41,752\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:04:41,753\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:04:41,754\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.5KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:04:41,755\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:41,756\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:41,756\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:41,757\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:41,758\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:46,843\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:04:46,844\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1654 / 1654\n",
      "2026-01-22 01:04:46,844\tINFO progress_bar.py:215 -- Filter(<lambda>): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 368.5KiB object store: Progress Completed 1614 / 1614\n",
      "2026-01-22 01:04:46,845\tINFO progress_bar.py:215 -- MapBatches(DocEmbedder): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:46,846\tINFO progress_bar.py:215 -- MapBatches(ChromaDBWriter): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:46,847\tINFO progress_bar.py:215 -- HashAggregate(key_columns=(), num_partitions=1): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.2 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:46,848\tINFO progress_bar.py:215 -- limit=1: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:46,848\tINFO progress_bar.py:215 -- Running Dataset: dataset_146_0. Active & requested resources: 2.25/32 CPU, 384.4MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:51,327\tINFO progress_bar.py:213 -- === Ray Data Progress {*- Shuffle} ===\n",
      "2026-01-22 01:04:51,328\tINFO progress_bar.py:215 -- *- Shuffle: Progress Completed 1 / 1\n",
      "2026-01-22 01:04:51,343\tINFO progress_bar.py:213 -- === Ray Data Progress {*- Aggregation} ===\n",
      "2026-01-22 01:04:51,344\tINFO progress_bar.py:215 -- *- Aggregation: Progress Completed 1 / ?\n",
      "2026-01-22 01:04:51,363\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_146_0 execution finished in 91.38 seconds\n",
      "INFO:openlineage.client.client:OpenLineageClient will use `composite` transport\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingestion complete! Total documents indexed: 1614\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Document Embedder for Ingestion\n",
    "# ============================================================================\n",
    "# This class generates embeddings for source documents and prepares them\n",
    "# for storage in the vector database. Unlike the query Embedder, this one:\n",
    "# - Generates UUIDs for each document chunk (required by ChromaDB)\n",
    "# - Renames 'text' column to 'doc' for storage\n",
    "#\n",
    "# IMPORTANT: Use the same embedding model (EMBEDDER_MODEL) as the query\n",
    "# embedder to ensure vector space compatibility!\n",
    "# ============================================================================\n",
    "\n",
    "class DocEmbedder:\n",
    "    \"\"\"\n",
    "    Generates embeddings for document chunks to be stored in the vector database.\n",
    "    \n",
    "    The __init__ method loads the model once per actor (efficient!).\n",
    "    The __call__ method processes batches of text, generating embeddings for each.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: str):\n",
    "        self._model = SentenceTransformer(model, device='cpu')\n",
    "        print(f\"DocEmbedder initialized with model: {model} on CPU\")\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Generate embeddings for all text in the batch\n",
    "        embeddings = self._model.encode(batch['text'], batch_size=32)\n",
    "        \n",
    "        # Generate unique IDs for each document (required by ChromaDB)\n",
    "        ids = np.array([uuid.uuid1().hex for _ in batch['text']])\n",
    "        \n",
    "        # Return dict with columns needed for ChromaDB: doc, vec, id\n",
    "        return {\n",
    "            'doc': batch['text'],      # Original text for retrieval\n",
    "            'vec': embeddings,          # Vector embeddings\n",
    "            'id': ids                   # Unique identifiers\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# ChromaDB Writer Class\n",
    "# ============================================================================\n",
    "# This class writes batches of embeddings to ChromaDB.\n",
    "# Using a class allows the connection to persist across batches.\n",
    "# ============================================================================\n",
    "\n",
    "class ChromaDBWriter:\n",
    "    \"\"\"\n",
    "    Writes document embeddings to ChromaDB in batches.\n",
    "    \n",
    "    Maintains a persistent connection to ChromaDB across batch operations.\n",
    "    \"\"\"\n",
    "    def __init__(self, collection_name: str):\n",
    "        # Connect to the persistent ChromaDB instance\n",
    "        self._client = chromadb.PersistentClient(path=\"/mnt/cluster_storage/vector_store\")\n",
    "        self._collection = self._client.get_collection(collection_name)\n",
    "        print(f\"ChromaDBWriter connected to collection: {collection_name}\")\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Upsert embeddings (insert or update if ID exists)\n",
    "        self._collection.upsert(\n",
    "            embeddings=batch['vec'].tolist(),\n",
    "            documents=batch['doc'].tolist(),\n",
    "            ids=batch['id'].tolist()\n",
    "        )\n",
    "        # Return count of documents written (for monitoring)\n",
    "        return {'docs_written': np.array([len(batch['id'])])}\n",
    "\n",
    "# ============================================================================\n",
    "# Run the Ingestion Pipeline\n",
    "# ============================================================================\n",
    "# This pipeline reads text, generates embeddings, and stores them in ChromaDB.\n",
    "# Using CPU workers for the lightweight MiniLM model is efficient and stable.\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Starting document ingestion pipeline...\")\n",
    "print(f\"Using embedding model: {EMBEDDER_MODEL}\")\n",
    "print(\"This will read the source text, generate embeddings, and store them in ChromaDB.\")\n",
    "\n",
    "# Read the source text file - each line becomes a \"document chunk\"\n",
    "# Filter out empty lines to avoid storing useless embeddings\n",
    "result = ray.data.read_text(\"/mnt/cluster_storage/around.txt\") \\\n",
    "    .filter(lambda row: len(row['text'].strip()) > 10) \\\n",
    "    .map_batches(\n",
    "        DocEmbedder, \n",
    "        fn_constructor_args=[EMBEDDER_MODEL],\n",
    "        compute=ray.data.ActorPoolStrategy(size=4),\n",
    "        batch_size=64      # Process 64 documents at a time\n",
    "    ) \\\n",
    "    .map_batches(\n",
    "        ChromaDBWriter,\n",
    "        fn_constructor_args=['persistent_text_chunks'],\n",
    "        compute=ray.data.ActorPoolStrategy(size=1),  \n",
    "        batch_size=100\n",
    "    ) \\\n",
    "    .sum('docs_written')\n",
    "\n",
    "print(f\"\\nIngestion complete! Total documents indexed: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "kwdjhdfjf1j",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in collection: 1614\n",
      "\n",
      "Test Query: 'What is the Great Salt Lake?'\n",
      "Retrieved 2 documents:\n",
      "\n",
      "  Document 1:\n",
      "    The Salt Lake, seventy miles long and thirty-five wide, is situated three miles eight hundred feet above the sea. Quite different from Lake Asphaltite, whose depression is twelve hundred feet below th...\n",
      "\n",
      "  Document 2:\n",
      "    The track up to this time had reached its highest elevation at the Great Salt Lake. From this point it described a long curve, descending towards Bitter Creek Valley, to rise again to the dividing rid...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Verify the Ingestion\n",
    "# ============================================================================\n",
    "# Let's confirm the vector database was populated correctly by:\n",
    "# 1. Checking the document count\n",
    "# 2. Running a test query to see if retrieval works\n",
    "# ============================================================================\n",
    "\n",
    "# Connect to ChromaDB and check collection stats\n",
    "verify_client = chromadb.PersistentClient(path=\"/mnt/cluster_storage/vector_store\")\n",
    "verify_collection = verify_client.get_collection(\"persistent_text_chunks\")\n",
    "\n",
    "doc_count = verify_collection.count()\n",
    "print(f\"Total documents in collection: {doc_count}\")\n",
    "\n",
    "# Run a test query to verify retrieval works\n",
    "# First, create an embedding for a test question using the SAME model\n",
    "test_model = SentenceTransformer(EMBEDDER_MODEL)\n",
    "test_query = test_model.encode(\"What is the Great Salt Lake?\").tolist()\n",
    "\n",
    "# Query ChromaDB for similar documents\n",
    "test_results = verify_collection.query(\n",
    "    query_embeddings=[test_query],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Query: 'What is the Great Salt Lake?'\")\n",
    "print(f\"Retrieved {len(test_results['documents'][0])} documents:\")\n",
    "for i, doc in enumerate(test_results['documents'][0]):\n",
    "    print(f\"\\n  Document {i+1}:\")\n",
    "    print(f\"    {doc[:200]}...\" if len(doc) > 200 else f\"    {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xx8x2ndagz",
   "metadata": {},
   "source": [
    "### The ChromaDBReader Class\n",
    "\n",
    "This class queries ChromaDB to find documents similar to our query embeddings. It follows the same pattern as `Embedder`:\n",
    "\n",
    "- **`__init__`**: Connect to ChromaDB and get the collection (done once per actor)\n",
    "- **`__call__`**: Query for similar documents for each batch of embeddings\n",
    "\n",
    "The `top_n` parameter controls how many similar documents to retrieve for each query.\n",
    "\n",
    "**Key Method**: `collection.query(query_embeddings=vecs, n_results=self._top_n)` \n",
    "- Takes embedding vectors and returns the `n` most similar documents\n",
    "- Uses approximate nearest neighbor search for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb111474-cb01-44ce-bbfb-e3a2af48e785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ChromaDBReader: Vector Database Retrieval\n",
    "# ============================================================================\n",
    "# This class queries ChromaDB to find documents similar to the query embeddings.\n",
    "# It follows the same callable class pattern as Embedder.\n",
    "#\n",
    "# How similarity search works:\n",
    "# 1. We have pre-computed embeddings for all documents (done during ingestion)\n",
    "# 2. We compute an embedding for the user's query\n",
    "# 3. ChromaDB finds the top-N documents with most similar embeddings\n",
    "# 4. Similarity is measured using cosine similarity or L2 distance\n",
    "# ============================================================================\n",
    "\n",
    "class ChromaDBReader:\n",
    "    \"\"\"\n",
    "    Retrieves similar documents from ChromaDB based on query embeddings.\n",
    "    \n",
    "    The database connection is established once per actor and reused.\n",
    "    \"\"\"\n",
    "    def __init__(self, collection: str, top_n: int):\n",
    "        \"\"\"\n",
    "        Initialize the ChromaDB reader.\n",
    "        \n",
    "        Args:\n",
    "            collection: Name of the ChromaDB collection to query\n",
    "            top_n: Number of similar documents to retrieve per query\n",
    "        \"\"\"\n",
    "        # Connect to the persistent ChromaDB instance\n",
    "        chroma_client = chromadb.PersistentClient(path=\"/mnt/cluster_storage/vector_store\")\n",
    "        self._coll = chroma_client.get_collection(collection)\n",
    "        self._top_n = top_n\n",
    "        print(f\"ChromaDBReader connected to collection '{collection}', retrieving top {top_n} docs\")\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Query ChromaDB for similar documents for each embedding in the batch.\n",
    "        \n",
    "        Args:\n",
    "            batch: Dict with 'prompt_embedding' containing query vectors\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'responsive_documents' containing retrieved documents\n",
    "        \"\"\"\n",
    "        # Convert numpy array to list for ChromaDB API\n",
    "        vecs = list(batch['prompt_embedding'])\n",
    "        \n",
    "        # Query ChromaDB for similar documents\n",
    "        # Returns: {'ids': [...], 'documents': [...], 'distances': [...], ...}\n",
    "        results = self._coll.query(\n",
    "            query_embeddings=vecs, \n",
    "            n_results=self._top_n\n",
    "        )\n",
    "        \n",
    "        # Add retrieved documents to the batch\n",
    "        # results['documents'] is a list of lists (one list per query)\n",
    "        batch['responsive_documents'] = results['documents']\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4hz39p0d4a",
   "metadata": {},
   "source": [
    "### Chaining Pipeline Stages\n",
    "\n",
    "Now we chain `Embedder` and `ChromaDBReader` together using multiple `map_batches` calls:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    DATA FLOW THROUGH PIPELINE                            │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "    Input Batch                   After Embedder              After ChromaDBReader\n",
    "    ┌──────────────┐             ┌──────────────────┐        ┌────────────────────────┐\n",
    "    │ {            │             │ {                │        │ {                      │\n",
    "    │   'prompt':  │  ────────▶  │   'prompt': [...],────────▶│   'prompt': [...],     │\n",
    "    │     [...]    │   Stage 1   │   'prompt_embedding':     │   'prompt_embedding':  │\n",
    "    │ }            │             │     [[0.1, ...], ...]     │     [[0.1, ...], ...], │\n",
    "    └──────────────┘             │ }                │        │   'responsive_documents':\n",
    "                                 └──────────────────┘        │     [[\"doc1\", ...], ...]\n",
    "                                                             │ }                      │\n",
    "                                                             └────────────────────────┘\n",
    "```\n",
    "\n",
    "**How it works**:\n",
    "1. Each batch flows through `Embedder`, which adds `prompt_embedding` to the batch\n",
    "2. The enriched batch then flows to `ChromaDBReader`, which adds `responsive_documents`\n",
    "3. Ray Data handles all the data transfer, parallelization, and backpressure automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e37c874-6981-40a9-b17d-fba0df7cd5d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:04:53,628\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_149_0\n",
      "2026-01-22 01:04:53,634\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_149_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 01:04:53,634\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_149_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> LimitOperator[limit=4] -> ActorPoolMapOperator[MapBatches(Embedder)] -> ActorPoolMapOperator[MapBatches(ChromaDBReader)]\n",
      "2026-01-22 01:04:53,852\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 01:04:53,853\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:53,854\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 01:04:53,855\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:53,856\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=4} ===\n",
      "2026-01-22 01:04:53,857\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:53,858\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(Embedder)} ===\n",
      "2026-01-22 01:04:53,858\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:53,859\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(ChromaDBReader)} ===\n",
      "2026-01-22 01:04:53,860\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:53,861\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 01:04:53,861\tINFO progress_bar.py:215 -- Running Dataset: dataset_149_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store (pending: 4 CPU): Progress Completed 0 / ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(ChromaDBReader)) pid=30369, ip=10.0.50.252)\u001b[0m ChromaDBReader connected to collection 'persistent_text_chunks', retrieving top 3 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:04:58,859\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:04:58,860\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:04:58,861\tINFO progress_bar.py:215 -- limit=4: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 189.0B object store: Progress Completed 4 / 4\n",
      "2026-01-22 01:04:58,862\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 1 (189.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:58,863\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 2; Queued blocks: 0 (0.0B); Resources: 2.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:04:58,864\tINFO progress_bar.py:215 -- Running Dataset: dataset_149_0. Active & requested resources: 2/32 CPU, 189.0B/26.9GiB object store (pending: 2 CPU): Progress Completed 0 / ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(Embedder)) pid=30368, ip=10.0.50.252)\u001b[0m Embedder initialized with model 'all-MiniLM-L6-v2' on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py:416: UserWarning: The minimum number of concurrent actors for 'MapBatches(ChromaDBReader)' is set to 2, but the operator only received 1 input(s). This means that the operator can launch at most 1 task(s), and won't fully utilize the available concurrency. You might be able to increase the number of concurrent tasks by configuring `override_num_blocks` earlier in the pipeline.\n",
      "  warnings.warn(\n",
      "2026-01-22 01:04:59,891\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_149_0 execution finished in 6.25 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-stage pipeline output:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Question: Describe the body of water in Utah?\n",
      "Retrieved 3 documents:\n",
      "  1. The Salt Lake, seventy miles long and thirty-five wide, is situated three miles eight hundred feet a...\n",
      "  2. During the lecture the train had been making good progress, and towards half-past twelve it reached ...\n",
      "\n",
      "Question: Tell as much as you can about the robbery?\n",
      "Retrieved 3 documents:\n",
      "  1. “Listen. On the 28th of last September a robbery of fifty-five thousand pounds was committed at the ...\n",
      "  2. “Well, Ralph,” said Thomas Flanagan, “what about that robbery?”\n",
      "\n",
      "Question: Did Phileas Fogg really rob the bank?\n",
      "Retrieved 3 documents:\n",
      "  1. Phileas Fogg had won his wager of twenty thousand pounds!\n",
      "  2. Phileas Fogg did not betray the least disappointment; but the situation was a grave one. It was not ...\n",
      "\n",
      "Question: Who is the main protagonist of Around the World in 80 Days?\n",
      "Retrieved 3 documents:\n",
      "  1. Around the World in Eighty Days\n",
      "  2. “The journey round the world in eighty days?”\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Demo: Embedder + ChromaDBReader (Two-Stage Pipeline)\n",
    "# ============================================================================\n",
    "# Now let's chain two stages together to see document retrieval in action.\n",
    "#\n",
    "# Data Flow:\n",
    "# 1. Prompts -> Embedder: Converts text to vectors (CPU)\n",
    "# 2. Vectors -> ChromaDBReader: Finds similar documents in the database (CPU)\n",
    "#\n",
    "# Note: This won't return useful documents if the database is empty!\n",
    "# Make sure you ran the ingestion step (Step 5.1) first.\n",
    "# ============================================================================\n",
    "\n",
    "two_stage_output = data.map_batches(\n",
    "    Embedder, \n",
    "    fn_constructor_args=[EMBEDDER_MODEL], \n",
    "    compute=ray.data.ActorPoolStrategy(size=2),\n",
    "    batch_size=4\n",
    ").map_batches(\n",
    "    ChromaDBReader, \n",
    "    fn_constructor_args=['persistent_text_chunks', 3],  # Retrieve top 3 documents\n",
    "    compute=ray.data.ActorPoolStrategy(size=2)\n",
    ").take_batch(4)\n",
    "\n",
    "# Display results\n",
    "print(\"Two-stage pipeline output:\")\n",
    "print(\"-\" * 60)\n",
    "for i, (prompt, docs) in enumerate(zip(two_stage_output['prompt'], two_stage_output['responsive_documents'])):\n",
    "    print(f\"\\nQuestion: {prompt}\")\n",
    "    print(f\"Retrieved {len(docs)} documents:\")\n",
    "    for j, doc in enumerate(docs[:2]):  # Show first 2 docs\n",
    "        print(f\"  {j+1}. {doc[:100]}...\" if len(doc) > 100 else f\"  {j+1}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd094c",
   "metadata": {},
   "source": [
    "**Note**: The `responsive_documents` field may be empty if no matching documents were found. In a production system, you would have pre-populated the vector database with relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d72f93f",
   "metadata": {},
   "source": [
    "## Step 6: Enhance Prompts with Retrieved Context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfb37f0-edec-4e32-a87f-bd4fffd50a6f",
   "metadata": {},
   "source": [
    "\n",
    "### The \"Augmentation\" in RAG\n",
    "\n",
    "This is where RAG gets its name - we **augment** the user's question with retrieved context:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    PROMPT AUGMENTATION                                   │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "    Original Question              Retrieved Documents          Enhanced Prompt\n",
    "    ┌──────────────────┐          ┌──────────────────┐        ┌─────────────────────┐\n",
    "    │ \"What is the     │          │ \"The Salt Lake,  │        │ System: You are a   │\n",
    "    │  Great Salt      │    +     │  seventy miles   │   =    │  helpful assistant  │\n",
    "    │  Lake?\"          │          │  long...\"        │        │                     │\n",
    "    └──────────────────┘          │ \"It reached the  │        │ User: Context:      │\n",
    "                                  │  Great Salt Lake │        │ - \"The Salt Lake...\"│\n",
    "                                  │  towards...\"     │        │ - \"It reached...\"   │\n",
    "                                  └──────────────────┘        │                     │\n",
    "                                                              │ Question: What is   │\n",
    "                                                              │ the Great Salt Lake?│\n",
    "                                                              └─────────────────────┘\n",
    "```\n",
    "\n",
    "### Prompt Engineering Best Practices\n",
    "\n",
    "The enhanced prompt includes:\n",
    "- **System message**: Tells the LLM its role (\"You are a helpful assistant\")\n",
    "- **Context**: The retrieved documents that may help answer the question\n",
    "- **Instructions**: Tells the LLM to admit when it doesn't know something\n",
    "- **User question**: The original question\n",
    "\n",
    "> **Tip**: Experiment with different system prompts to see how they affect output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff59368e-068b-4b41-9975-96168e5e3cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PromptEnhancer: The \"Augmentation\" in RAG\n",
    "# ============================================================================\n",
    "# This class combines:\n",
    "# 1. The user's original question\n",
    "# 2. Retrieved relevant documents from the vector database\n",
    "# 3. System instructions for the LLM\n",
    "#\n",
    "# This creates a \"context-aware\" prompt that helps the LLM answer questions\n",
    "# about specific documents it wasn't trained on.\n",
    "# ============================================================================\n",
    "\n",
    "class PromptEnhancer:\n",
    "    \"\"\"\n",
    "    Enhances user prompts with retrieved document context for RAG.\n",
    "    \n",
    "    The enhanced prompt follows a chat format:\n",
    "    - System message: Sets the LLM's behavior\n",
    "    - User message: Contains retrieved context + original question\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Base template for the user message\n",
    "        # The {context} placeholder will be replaced with retrieved documents\n",
    "        self._user_template = \"\"\"You are a helpful assistant who can answer questions about a text based on your existing knowledge and documents supplied here.\n",
    "\n",
    "When answering questions, use the following relevant excerpts from the text:\n",
    "{context}\n",
    "\n",
    "If you don't have information to answer a question, please say you don't know. Don't make up an answer.\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        original_prompts = batch['prompt']\n",
    "        enhanced_prompts = []\n",
    "        \n",
    "        for ix, original_prompt in enumerate(original_prompts):\n",
    "            # Get retrieved documents for this prompt\n",
    "            docs = batch['responsive_documents'][ix]\n",
    "            \n",
    "            # Format the context from retrieved documents\n",
    "            # Handle both list and numpy array cases safely\n",
    "            if docs is not None and len(docs) > 0:\n",
    "                context = \"\\n\".join([f\"- {doc}\" for doc in docs])\n",
    "            else:\n",
    "                context = \"No relevant documents found.\"\n",
    "            \n",
    "            # Build the enhanced prompt using safe string formatting \n",
    "            user_content = self._user_template.format(\n",
    "                context=context,\n",
    "                question=original_prompt\n",
    "            )\n",
    "            \n",
    "            # Create chat-formatted message for the LLM\n",
    "            enhanced_prompts.append([\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ])\n",
    "\n",
    "        batch['enhanced_prompt'] = enhanced_prompts\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5xq7wgdyb8",
   "metadata": {},
   "source": [
    "### Running the Three-Stage Pipeline\n",
    "\n",
    "Now we chain three stages together:\n",
    "1. **Embedder** → Generates embeddings\n",
    "2. **ChromaDBReader** → Retrieves relevant documents\n",
    "3. **PromptEnhancer** → Creates augmented prompts\n",
    "\n",
    "The output will include an `enhanced_prompt` field with the complete prompt ready for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d529d5f8-f963-4618-b63e-766c6b147d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:05:00,040\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_153_0\n",
      "2026-01-22 01:05:00,044\tINFO limit_pushdown.py:140 -- Skipping push down of limit 3 through map MapBatches[MapBatches(Embedder)] because it requires 4 rows to produce stable outputs\n",
      "2026-01-22 01:05:00,046\tINFO limit_pushdown.py:140 -- Skipping push down of limit 3 through map MapBatches[MapBatches(Embedder)] because it requires 4 rows to produce stable outputs\n",
      "2026-01-22 01:05:00,052\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_153_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 01:05:00,054\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_153_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> ActorPoolMapOperator[MapBatches(Embedder)] -> LimitOperator[limit=3] -> ActorPoolMapOperator[MapBatches(ChromaDBReader)] -> ActorPoolMapOperator[MapBatches(PromptEnhancer)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:05:00,379\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 01:05:00,380\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:00,381\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 01:05:00,382\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:00,382\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(Embedder)} ===\n",
      "2026-01-22 01:05:00,383\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:00,384\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=3} ===\n",
      "2026-01-22 01:05:00,384\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:00,385\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(ChromaDBReader)} ===\n",
      "2026-01-22 01:05:00,386\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:00,386\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(PromptEnhancer)} ===\n",
      "2026-01-22 01:05:00,388\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:00,388\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 01:05:00,389\tINFO progress_bar.py:215 -- Running Dataset: dataset_153_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store (pending: 6 CPU): Progress Completed 0 / ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(ChromaDBReader)) pid=27430, ip=10.0.45.231)\u001b[0m ChromaDBReader connected to collection 'persistent_text_chunks', retrieving top 3 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:05:05,439\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:05:05,439\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.5KiB object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:05,440\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 1 (1.5KiB); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:05,441\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:05,441\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 2; Queued blocks: 0 (0.0B); Resources: 2.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:05,442\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 2; Queued blocks: 0 (0.0B); Resources: 2.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:05,443\tINFO progress_bar.py:215 -- Running Dataset: dataset_153_0. Active & requested resources: 4/32 CPU, 1.5KiB/26.9GiB object store (pending: 2 CPU): Progress Completed 0 / ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(Embedder)) pid=27431, ip=10.0.45.231)\u001b[0m Embedder initialized with model 'all-MiniLM-L6-v2' on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py:416: UserWarning: The minimum number of concurrent actors for 'MapBatches(PromptEnhancer)' is set to 2, but the operator only received 1 input(s). This means that the operator can launch at most 1 task(s), and won't fully utilize the available concurrency. You might be able to increase the number of concurrent tasks by configuring `override_num_blocks` earlier in the pipeline.\n",
      "  warnings.warn(\n",
      "2026-01-22 01:05:06,980\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_153_0 execution finished in 6.93 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three-stage pipeline output (showing enhanced prompt structure):\n",
      "============================================================\n",
      "\n",
      "--- Example 1 ---\n",
      "Original Question: Describe the body of water in Utah?\n",
      "\n",
      "Enhanced Prompt Structure:\n",
      "  Role: system\n",
      "  Content: You are a helpful assistant that answers questions based on provided context.\n",
      "  Role: user\n",
      "  Content: You are a helpful assistant who can answer questions about a text based on your existing knowledge and documents supplied here.\n",
      "\n",
      "When answering questions, use the following relevant excerpts from the ...\n",
      "\n",
      "\n",
      "--- Example 2 ---\n",
      "Original Question: Tell as much as you can about the robbery?\n",
      "\n",
      "Enhanced Prompt Structure:\n",
      "  Role: system\n",
      "  Content: You are a helpful assistant that answers questions based on provided context.\n",
      "  Role: user\n",
      "  Content: You are a helpful assistant who can answer questions about a text based on your existing knowledge and documents supplied here.\n",
      "\n",
      "When answering questions, use the following relevant excerpts from the ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Demo: Three-Stage Pipeline (Embedder + ChromaDBReader + PromptEnhancer)\n",
    "# ============================================================================\n",
    "# Let's see what the enhanced prompts look like before sending to the LLM.\n",
    "# This shows the \"augmentation\" step - combining user questions with context.\n",
    "# ============================================================================\n",
    "\n",
    "three_stage_output = data.map_batches(\n",
    "                            Embedder, \n",
    "                            fn_constructor_args=[EMBEDDER_MODEL], \n",
    "                            compute=ray.data.ActorPoolStrategy(size=2),\n",
    "                            batch_size=4\n",
    "                        ).map_batches(\n",
    "                            ChromaDBReader, \n",
    "                            fn_constructor_args=['persistent_text_chunks', 3], \n",
    "                            compute=ray.data.ActorPoolStrategy(size=2)\n",
    "                        ).map_batches(\n",
    "                            PromptEnhancer, \n",
    "                            compute=ray.data.ActorPoolStrategy(size=2)\n",
    "                        ).take_batch(3)\n",
    "\n",
    "# Display the enhanced prompt structure\n",
    "print(\"Three-stage pipeline output (showing enhanced prompt structure):\")\n",
    "print(\"=\" * 60)\n",
    "for i, (prompt, enhanced) in enumerate(zip(three_stage_output['prompt'][:2], \n",
    "                                            three_stage_output['enhanced_prompt'][:2])):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Original Question: {prompt}\")\n",
    "    print(f\"\\nEnhanced Prompt Structure:\")\n",
    "    for msg in enhanced:\n",
    "        print(f\"  Role: {msg['role']}\")\n",
    "        content_preview = msg['content'][:200] + \"...\" if len(msg['content']) > 200 else msg['content']\n",
    "        print(f\"  Content: {content_preview}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f8088",
   "metadata": {},
   "source": [
    "## Step 7: Add LLM Inference to the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448e778-85fd-49c1-acf4-e432e7ac409d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### The Chat Class\n",
    "\n",
    "Now we add the final piece - the LLM that generates responses. The Chat class:\n",
    "\n",
    "- Uses Hugging Face's `pipeline` API for text generation\n",
    "- Loads the model to GPU for fast inference\n",
    "- Processes batches of enhanced prompts\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "| Parameter | Value | Explanation |\n",
    "|-----------|-------|-------------|\n",
    "| `max_new_tokens` | `200` | Maximum length of generated response |\n",
    "| `truncation` | `True` | Truncate long inputs to fit model context |\n",
    "| `cache_dir` | `/mnt/local_storage` | Cache model weights locally |\n",
    "\n",
    "> **Performance Note**: `max_new_tokens` affects both response quality and latency. More tokens = longer responses but slower generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c793f29-5292-4fe4-8e5d-6050b9f535e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Chat Class: LLM Response Generation\n",
    "# ============================================================================\n",
    "# This class handles the final stage - generating responses using an LLM.\n",
    "# The enhanced prompts (with retrieved context) are sent to the model.\n",
    "#\n",
    "# Key Considerations:\n",
    "# - We use a small model (Qwen2.5-0.5B) for efficiency in this demo\n",
    "# - In production, you might use larger models (7B, 13B, 70B+)\n",
    "# - GPU memory and latency are the main constraints\n",
    "# ============================================================================\n",
    "\n",
    "class Chat:\n",
    "    \"\"\"\n",
    "    Generates LLM responses for enhanced prompts using Hugging Face pipelines.\n",
    "    \n",
    "    The model is loaded once on initialization and reused for all batches.\n",
    "    This avoids the significant overhead of loading a model for each batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: str):\n",
    "        \"\"\"\n",
    "        Initialize the chat model.\n",
    "        \n",
    "        Args:\n",
    "            model: Hugging Face model ID (e.g., 'Qwen/Qwen2.5-0.5B-Instruct')\n",
    "        \"\"\"\n",
    "        # Create a text generation pipeline with the specified model\n",
    "        # device='cuda:0' ensures we use the GPU for faster inference\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\", \n",
    "            model=model, \n",
    "            device='cuda:0',\n",
    "            # Cache model weights locally to avoid re-downloading\n",
    "            model_kwargs={\"cache_dir\": \"/mnt/local_storage\"}\n",
    "        )\n",
    "        print(f\"Chat model '{model}' loaded on GPU\")\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Generate responses for a batch of enhanced prompts.\n",
    "        \n",
    "        Args:\n",
    "            batch: Dict with 'enhanced_prompt' containing chat-formatted messages\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'responses' containing the LLM outputs\n",
    "        \"\"\"\n",
    "        # Convert enhanced prompts from arrays to lists for the pipeline\n",
    "        # Each prompt is a list of messages [{\"role\": \"system\", ...}, {\"role\": \"user\", ...}]\n",
    "        enhanced_prompts = [[msg for msg in prompt] for prompt in batch['enhanced_prompt']]\n",
    "        \n",
    "        # Generate responses using the pipeline\n",
    "        # max_new_tokens limits response length (controls latency and cost)\n",
    "        # truncation=True handles cases where input exceeds model's context window\n",
    "        batch['responses'] = self.pipe(\n",
    "            enhanced_prompts, \n",
    "            max_new_tokens=200,    # Max tokens to generate\n",
    "            truncation=True        # Truncate long inputs\n",
    "        )\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408393f-fd91-41b3-a9fb-1bc193620dee",
   "metadata": {},
   "source": [
    "### Running the Complete RAG Pipeline\n",
    "\n",
    "Now we run the full 4-stage pipeline:\n",
    "\n",
    "```\n",
    "Embedder → ChromaDBReader → PromptEnhancer → Chat\n",
    "```\n",
    "\n",
    "**Note on `concurrency` vs `ActorPoolStrategy`**: \n",
    "- `concurrency=4` is shorthand for `ActorPoolStrategy(size=4)` with autoscaling\n",
    "- Use explicit `ActorPoolStrategy` when you need precise control over actor count\n",
    "\n",
    "We store the output in a Python variable for inspection. In production, you would typically stream results or write to storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a05c7970-8680-4363-90f4-3d92e99d989b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m Failed to convert column 'responsive_documents' into pyarrow array due to: Error converting data to Arrow: column: 'responsive_documents', shape: (3,), dtype: object, data: [array(['The Salt Lake, seventy miles long and thirty-five wide, is situated three miles eight hundred feet above the sea. Quite diffe...; falling back to serialize as pickled python objects\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 774, in from_numpy\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m     return cls._from_numpy(arr)\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 794, in _from_numpy\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m     return ArrowVariableShapedTensorArray.from_numpy(arr)\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 1218, in from_numpy\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m     data_buffer = _concat_ndarrays(raveled)\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 1471, in _concat_ndarrays\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m     byte_view = base.view(np.uint8).reshape(-1)\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m                 ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/numpy/core/_internal.py\", line 551, in _view_is_safe\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m     raise TypeError(\"Cannot change data-type for object array.\")\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m TypeError: Cannot change data-type for object array.\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m \n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m \n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 233, in convert_to_pyarrow_array\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m     return ArrowTensorArray.from_numpy(\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 780, in from_numpy\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m     raise ArrowConversionError(data_str) from e\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=27432, ip=10.0.45.231)\u001b[0m ray.air.util.tensor_extensions.arrow.ArrowConversionError: Error converting data to Arrow: column: 'responsive_documents', shape: (3,), dtype: object, data: [array(['The Salt Lake, seventy miles long and thirty-five wide, is situated three miles eight hundred feet above the sea. Quite diffe...\n",
      "2026-01-22 01:05:07,106\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_158_0\n",
      "2026-01-22 01:05:07,111\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_158_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 01:05:07,112\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_158_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> LimitOperator[limit=23] -> ActorPoolMapOperator[MapBatches(Embedder)] -> ActorPoolMapOperator[MapBatches(ChromaDBReader)] -> ActorPoolMapOperator[MapBatches(PromptEnhancer)] -> ActorPoolMapOperator[MapBatches(Chat)]\n",
      "2026-01-22 01:05:07,442\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 01:05:07,444\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:07,445\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 01:05:07,446\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:07,448\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=23} ===\n",
      "2026-01-22 01:05:07,448\tINFO progress_bar.py:215 -- limit=23: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:07,449\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(Embedder)} ===\n",
      "2026-01-22 01:05:07,450\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 4 (running=0, restarting=0, pending=4); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:07,451\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(ChromaDBReader)} ===\n",
      "2026-01-22 01:05:07,453\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:07,454\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(PromptEnhancer)} ===\n",
      "2026-01-22 01:05:07,456\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:07,457\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(Chat)} ===\n",
      "2026-01-22 01:05:07,458\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 0; Actors: 1 (running=0, restarting=0, pending=1); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:07,459\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 01:05:07,460\tINFO progress_bar.py:215 -- Running Dataset: dataset_158_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store (pending: 8 CPU, 1 GPU): Progress Completed 0 / ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(ChromaDBReader)) pid=27661, ip=10.0.45.231)\u001b[0m ChromaDBReader connected to collection 'persistent_text_chunks', retrieving top 3 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:05:12,527\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:05:12,528\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:12,529\tINFO progress_bar.py:215 -- limit=23: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.5KiB object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:12,530\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 4 (running=0, restarting=0, pending=4); Queued blocks: 1 (1.5KiB); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:12,531\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 2; Queued blocks: 0 (0.0B); Resources: 2.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:12,532\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 2; Queued blocks: 0 (0.0B); Resources: 2.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:12,532\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 0; Actors: 1 (running=0, restarting=0, pending=1); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:12,533\tINFO progress_bar.py:215 -- Running Dataset: dataset_158_0. Active & requested resources: 4/32 CPU, 1.5KiB/26.9GiB object store (pending: 4 CPU, 1 GPU): Progress Completed 0 / ?\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py:416: UserWarning: The minimum number of concurrent actors for 'MapBatches(Embedder)' is set to 4, but the operator only received 1 input(s). This means that the operator can launch at most 1 task(s), and won't fully utilize the available concurrency. You might be able to increase the number of concurrent tasks by configuring `override_num_blocks` earlier in the pipeline.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(Embedder)) pid=27660, ip=10.0.45.231)\u001b[0m Embedder initialized with model 'all-MiniLM-L6-v2' on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m Device set to use cuda:0\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m Failed to convert column 'responsive_documents' into pyarrow array due to: Error converting data to Arrow: column: 'responsive_documents', shape: (23,), dtype: object, data: [array(['The Salt Lake, seventy miles long and thirty-five wide, is situated three miles eight hundred feet above the sea. Quite diff...; falling back to serialize as pickled python objects\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 774, in from_numpy\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m     return cls._from_numpy(arr)\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 794, in _from_numpy\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m     return ArrowVariableShapedTensorArray.from_numpy(arr)\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 1218, in from_numpy\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m     data_buffer = _concat_ndarrays(raveled)\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 1471, in _concat_ndarrays\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m     byte_view = base.view(np.uint8).reshape(-1)\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m                 ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/numpy/core/_internal.py\", line 551, in _view_is_safe\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m     raise TypeError(\"Cannot change data-type for object array.\")\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m TypeError: Cannot change data-type for object array.\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m \n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m \n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 233, in convert_to_pyarrow_array\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m     return ArrowTensorArray.from_numpy(\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 780, in from_numpy\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m     raise ArrowConversionError(data_str) from e\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=30945, ip=10.0.50.252)\u001b[0m ray.air.util.tensor_extensions.arrow.ArrowConversionError: Error converting data to Arrow: column: 'responsive_documents', shape: (23,), dtype: object, data: [array(['The Salt Lake, seventy miles long and thirty-five wide, is situated three miles eight hundred feet above the sea. Quite diff...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m Chat model 'Qwen/Qwen2.5-0.5B-Instruct' loaded on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:05:17,559\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:05:17,560\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:17,561\tINFO progress_bar.py:215 -- limit=23: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:17,562\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:17,562\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:17,563\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 81.9KiB object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:17,564\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:17,565\tINFO progress_bar.py:215 -- Running Dataset: dataset_158_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.1MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:22,586\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:05:22,587\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:22,588\tINFO progress_bar.py:215 -- limit=23: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:22,589\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:22,590\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:22,591\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 81.9KiB object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:22,592\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:22,593\tINFO progress_bar.py:215 -- Running Dataset: dataset_158_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.1MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m Failed to convert column 'responsive_documents' into pyarrow array due to: Error converting data to Arrow: column: 'responsive_documents', shape: (4,), dtype: object, data: [array(['The Salt Lake, seventy miles long and thirty-five wide, is situated three miles eight hundred feet above the sea. Quite diffe...; falling back to serialize as pickled python objects\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m     pa_scalar_type = pa.from_numpy_dtype(dtype)\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m   File \"pyarrow/types.pxi\", line 6039, in pyarrow.lib.from_numpy_dtype\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m   File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m   File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m pyarrow.lib.ArrowNotImplementedError: Unsupported numpy type 17\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m \n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m \n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m ray.air.util.tensor_extensions.arrow.ArrowConversionError: Error converting data to Arrow: column: 'responsive_documents', shape: (4,), dtype: object, data: [array(['The Salt Lake, seventy miles long and thirty-five wide, is situated three miles eight hundred feet above the sea. Quite diffe...\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 780, in from_numpy\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m     return cls._from_numpy(arr)\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 794, in _from_numpy\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m     return ArrowVariableShapedTensorArray.from_numpy(arr)\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 233, in convert_to_pyarrow_array\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m     return ArrowTensorArray.from_numpy(\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=30944, ip=10.0.50.252)\u001b[0m     raise ArrowConversionError(data_str) from e\n",
      "2026-01-22 01:05:27,625\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:05:27,626\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:27,626\tINFO progress_bar.py:215 -- limit=23: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:27,627\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:27,628\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:27,629\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 81.9KiB object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:27,629\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:27,630\tINFO progress_bar.py:215 -- Running Dataset: dataset_158_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.1MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:32,651\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:05:32,653\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:32,653\tINFO progress_bar.py:215 -- limit=23: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:32,654\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:32,655\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:32,656\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 81.9KiB object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:32,657\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:32,657\tINFO progress_bar.py:215 -- Running Dataset: dataset_158_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.1MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:37,692\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:05:37,693\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:37,694\tINFO progress_bar.py:215 -- limit=23: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:37,694\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:37,695\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:37,696\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:37,696\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:37,697\tINFO progress_bar.py:215 -- Running Dataset: dataset_158_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:42,719\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:05:42,720\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:42,720\tINFO progress_bar.py:215 -- limit=23: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:42,721\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:42,722\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:42,723\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:42,723\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:42,724\tINFO progress_bar.py:215 -- Running Dataset: dataset_158_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:47,754\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:05:47,755\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:47,755\tINFO progress_bar.py:215 -- limit=23: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:47,756\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:47,757\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:47,758\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:47,759\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:47,760\tINFO progress_bar.py:215 -- Running Dataset: dataset_158_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:52,785\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:05:52,786\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:52,787\tINFO progress_bar.py:215 -- limit=23: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:52,787\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:52,788\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:52,789\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:52,790\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:52,791\tINFO progress_bar.py:215 -- Running Dataset: dataset_158_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:57,818\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:05:57,818\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:57,819\tINFO progress_bar.py:215 -- limit=23: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:57,820\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:57,820\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:57,822\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:05:57,823\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:05:57,823\tINFO progress_bar.py:215 -- Running Dataset: dataset_158_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:02,844\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:06:02,845\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:02,846\tINFO progress_bar.py:215 -- limit=23: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:02,847\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:02,847\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:02,848\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:02,849\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:02,850\tINFO progress_bar.py:215 -- Running Dataset: dataset_158_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:05,200\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_158_0 execution finished in 58.08 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline complete! Retrieved 23 responses.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Complete RAG Pipeline Execution\n",
    "# ============================================================================\n",
    "# This runs the full 4-stage pipeline and stores results in memory for inspection.\n",
    "#\n",
    "# Pipeline Flow:\n",
    "# [Prompts] -> [Embedder] -> [ChromaDBReader] -> [PromptEnhancer] -> [Chat] -> [Output]\n",
    "#\n",
    "# Resource Allocation:\n",
    "# - Embedder: CPU only (MiniLM is efficient on CPU)\n",
    "# - ChromaDBReader: CPU only (vector DB queries)\n",
    "# - PromptEnhancer: CPU only (string operations)\n",
    "# - Chat: GPU required (LLM inference)\n",
    "#\n",
    "# IMPORTANT: We use compute=ray.data.ActorPoolStrategy() instead of the deprecated\n",
    "# 'concurrency' parameter. This is the recommended approach in Ray 2.53.0+.\n",
    "# ============================================================================\n",
    "\n",
    "output = data \\\n",
    "    .map_batches(\n",
    "        Embedder, \n",
    "        fn_constructor_args=[EMBEDDER_MODEL], \n",
    "        # CPU-based embedding with the lightweight MiniLM model\n",
    "        compute=ray.data.ActorPoolStrategy(size=4),\n",
    "        batch_size=4\n",
    "    ) \\\n",
    "    .map_batches(\n",
    "        ChromaDBReader, \n",
    "        fn_constructor_args=['persistent_text_chunks', 3],  # Collection name, top_n results\n",
    "        compute=ray.data.ActorPoolStrategy(size=2)\n",
    "    ) \\\n",
    "    .map_batches(\n",
    "        PromptEnhancer, \n",
    "        compute=ray.data.ActorPoolStrategy(size=2)\n",
    "    ) \\\n",
    "    .map_batches(\n",
    "        Chat, \n",
    "        compute=ray.data.ActorPoolStrategy(size=1),  # Single GPU actor\n",
    "        fn_constructor_args=[CHAT_MODEL], \n",
    "        num_gpus=1,         # LLM inference needs GPU\n",
    "        batch_size=4        # Keep batch small to avoid OOM\n",
    "    ) \\\n",
    "    .take_batch(23)         # Retrieve all 23 prompts for inspection\n",
    "\n",
    "print(f\"Pipeline complete! Retrieved {len(output['prompt'])} responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p48to57bf7g",
   "metadata": {},
   "source": [
    "### Inspecting the Results\n",
    "\n",
    "Let's create a helper function to visualize the LLM's responses. This extracts and prints:\n",
    "- The user's original question\n",
    "- The LLM's generated response\n",
    "\n",
    "> **Quality Check**: Analyze the responses to see if they're accurate. Does the model make things up when it doesn't have information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6be3b7fd-251e-4a2c-880b-2e32537b5821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Helper Function: Visualize RAG Results\n",
    "# ============================================================================\n",
    "# This function formats the LLM responses for easy reading.\n",
    "# It extracts and displays:\n",
    "# 1. The original user question\n",
    "# 2. The LLM's generated answer\n",
    "# ============================================================================\n",
    "\n",
    "def print_visual_eval(batch, max_responses=5):\n",
    "    \"\"\"\n",
    "    Pretty-print the RAG pipeline results for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        batch: Output batch containing 'prompt' and 'responses'\n",
    "        max_responses: Maximum number of responses to display (default 5)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"RAG PIPELINE RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, (prompt, response) in enumerate(zip(batch['prompt'][:max_responses], \n",
    "                                                batch['responses'][:max_responses])):\n",
    "        print(f\"\\n{'─' * 80}\")\n",
    "        print(f\"QUESTION {i+1}: {prompt}\")\n",
    "        print(f\"{'─' * 80}\")\n",
    "        \n",
    "        # Extract the generated answer from the response structure\n",
    "        # The response format is: [{'generated_text': [{'role': 'system', ...}, {'role': 'user', ...}, {'role': 'assistant', 'content': ...}]}]\n",
    "        try:\n",
    "            generated_text = response[0]['generated_text']\n",
    "            # Find the assistant's response (last message in the conversation)\n",
    "            assistant_response = None\n",
    "            for msg in generated_text:\n",
    "                if isinstance(msg, dict) and msg.get('role') == 'assistant':\n",
    "                    assistant_response = msg.get('content', 'No content')\n",
    "            \n",
    "            if assistant_response:\n",
    "                print(f\"ANSWER: {assistant_response}\")\n",
    "            else:\n",
    "                # Fallback: show the last message content\n",
    "                print(f\"ANSWER: {generated_text[-1].get('content', str(generated_text[-1]))}\")\n",
    "        except (KeyError, IndexError, TypeError) as e:\n",
    "            print(f\"ANSWER: [Error parsing response: {e}]\")\n",
    "            print(f\"Raw response: {response}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Displayed {min(max_responses, len(batch['prompt']))} of {len(batch['prompt'])} results\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44780ef2-ff3e-493b-b089-c3bcfb6b43b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RAG PIPELINE RESULTS\n",
      "================================================================================\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "QUESTION 1: Describe the body of water in Utah?\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER: The body of water described in the excerpt is the Great Salt Lake. It is a large saline lake located in western Utah, situated approximately seven miles west of Salt Lake City and thirty-five miles north of Salt Lake City. The lake's surface elevation is thirty-five feet above sea level, making it slightly higher than the surrounding areas. \n",
      "\n",
      "The lake has a distinct shape, resembling a bowl or a saucer, due to the presence of numerous salt crystals embedded within its waters. This unique feature gives the lake its name, \"Great Salt Lake.\" The lake's salt content is substantial, containing more than 98% of its volume as salt, primarily sodium chloride (NaCl). Its specific gravity is approximately 1,170 kg/m³, indicating that it weighs 1,170 kilograms per cubic meter.\n",
      "\n",
      "The Great Salt Lake is surrounded by extensive agricultural lands, particularly in the region known as the Salt Valley. This fertile land supports diverse crops such as wheat, corn,\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "QUESTION 2: Tell as much as you can about the robbery?\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER: The robbery took place on the 28th of last September at the Bank of England. It involved a robbery worth fifty-five thousand pounds. The description given in the excerpt refers to the robber, which matches the identity of Phileas Fogg mentioned earlier in the passage. The police eventually apprehended the robber, but they did so after months of investigation. The incident sparked discussions among reformers about the possibility of pursuing Fogg's case further.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "QUESTION 3: Did Phileas Fogg really rob the bank?\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER: I'm sorry, but I do not have enough information to determine whether Phileas Fogg actually robbed the bank or not. The excerpt provided does not mention anything about him robbing any bank or other financial institution. Therefore, I cannot confirm or deny if Phileas Fogg truly did rob the bank.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "QUESTION 4: Who is the main protagonist of Around the World in 80 Days?\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER: The main protagonist of Around the World in 80 Days is Harry Houdini. The story follows his daring escape from prison, where he performs acrobatic feats while wearing a fake body suit.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "QUESTION 5: What is the name of Phileas Fogg’s loyal servant?\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER: Phileas Fogg's loyal servant is called \"Phineas.\"\n",
      "\n",
      "================================================================================\n",
      "Displayed 5 of 23 results\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_visual_eval(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d3ac3f",
   "metadata": {},
   "source": [
    "## Step 8: Production Deployment - Writing to Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb06cc-1121-489b-9108-5504e471f326",
   "metadata": {},
   "source": [
    "\n",
    "In production, you typically want to:\n",
    "1. Process larger datasets\n",
    "2. Write results to persistent storage (Parquet, databases, data lakes)\n",
    "3. Run the pipeline as a batch job\n",
    "\n",
    "### Writing Results to Parquet\n",
    "\n",
    "Instead of `.take_batch()` (which loads results into memory), we use `.write_parquet()` to stream results directly to storage. This:\n",
    "- Handles datasets larger than memory\n",
    "- Provides durability (results won't be lost if something fails)\n",
    "- Enables downstream processing\n",
    "\n",
    "> **Other Options**: `write_json()`, `write_csv()`, or writing to databases are also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7670db3-2cd4-455c-98f0-e87258db44bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:06:05,409\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_165_0\n",
      "2026-01-22 01:06:05,414\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_165_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 01:06:05,414\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_165_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> ActorPoolMapOperator[MapBatches(Embedder)] -> ActorPoolMapOperator[MapBatches(ChromaDBReader)] -> ActorPoolMapOperator[MapBatches(PromptEnhancer)] -> ActorPoolMapOperator[MapBatches(Chat)] -> TaskPoolMapOperator[Write]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running complete RAG pipeline and writing results to Parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:06:05,747\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 01:06:05,748\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:05,749\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 01:06:05,750\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:05,751\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(Embedder)} ===\n",
      "2026-01-22 01:06:05,752\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 4 (running=0, restarting=0, pending=4); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:05,753\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(ChromaDBReader)} ===\n",
      "2026-01-22 01:06:05,754\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:05,755\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(PromptEnhancer)} ===\n",
      "2026-01-22 01:06:05,755\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:05,756\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(Chat)} ===\n",
      "2026-01-22 01:06:05,757\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 0; Actors: 1 (running=0, restarting=0, pending=1); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:05,758\tINFO progress_bar.py:213 -- === Ray Data Progress {Write} ===\n",
      "2026-01-22 01:06:05,758\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:05,759\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 01:06:05,760\tINFO progress_bar.py:215 -- Running Dataset: dataset_165_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store (pending: 8 CPU, 1 GPU): Progress Completed 0 / ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(ChromaDBReader)) pid=28083, ip=10.0.45.231)\u001b[0m ChromaDBReader connected to collection 'persistent_text_chunks', retrieving top 3 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:06:10,847\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:06:10,847\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.5KiB object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:10,848\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 4 (running=0, restarting=0, pending=4); Queued blocks: 1 (1.5KiB); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:10,849\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 2; Queued blocks: 0 (0.0B); Resources: 2.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:10,850\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 2; Queued blocks: 0 (0.0B); Resources: 2.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:10,851\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 0; Actors: 1 (running=0, restarting=0, pending=1); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:10,851\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:10,852\tINFO progress_bar.py:215 -- Running Dataset: dataset_165_0. Active & requested resources: 4/32 CPU, 1.5KiB/26.9GiB object store (pending: 4 CPU, 1 GPU): Progress Completed 0 / ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(Embedder)) pid=28081, ip=10.0.45.231)\u001b[0m Embedder initialized with model 'all-MiniLM-L6-v2' on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m Device set to use cuda:0\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m Failed to convert column 'responsive_documents' into pyarrow array due to: Error converting data to Arrow: column: 'responsive_documents', shape: (23,), dtype: object, data: [array(['The Salt Lake, seventy miles long and thirty-five wide, is situated three miles eight hundred feet above the sea. Quite diff...; falling back to serialize as pickled python objects\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 774, in from_numpy\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m     return cls._from_numpy(arr)\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 794, in _from_numpy\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m     return ArrowVariableShapedTensorArray.from_numpy(arr)\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 1218, in from_numpy\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m     data_buffer = _concat_ndarrays(raveled)\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 1471, in _concat_ndarrays\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m     byte_view = base.view(np.uint8).reshape(-1)\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m                 ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/numpy/core/_internal.py\", line 551, in _view_is_safe\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m     raise TypeError(\"Cannot change data-type for object array.\")\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m TypeError: Cannot change data-type for object array.\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m \n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m \n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 233, in convert_to_pyarrow_array\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m     return ArrowTensorArray.from_numpy(\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 780, in from_numpy\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m     raise ArrowConversionError(data_str) from e\n",
      "\u001b[36m(MapWorker(MapBatches(PromptEnhancer)) pid=31532, ip=10.0.50.252)\u001b[0m ray.air.util.tensor_extensions.arrow.ArrowConversionError: Error converting data to Arrow: column: 'responsive_documents', shape: (23,), dtype: object, data: [array(['The Salt Lake, seventy miles long and thirty-five wide, is situated three miles eight hundred feet above the sea. Quite diff...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m Chat model 'Qwen/Qwen2.5-0.5B-Instruct' loaded on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:06:15,935\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:06:15,936\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:15,937\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:15,938\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:15,940\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 81.9KiB object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:15,941\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:15,941\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:15,942\tINFO progress_bar.py:215 -- Running Dataset: dataset_165_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.1MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m Failed to convert column 'responsive_documents' into pyarrow array due to: Error converting data to Arrow: column: 'responsive_documents', shape: (4,), dtype: object, data: [array(['The Salt Lake, seventy miles long and thirty-five wide, is situated three miles eight hundred feet above the sea. Quite diffe...; falling back to serialize as pickled python objects\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m     pa_scalar_type = pa.from_numpy_dtype(dtype)\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m   File \"pyarrow/types.pxi\", line 6039, in pyarrow.lib.from_numpy_dtype\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m   File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m   File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m pyarrow.lib.ArrowNotImplementedError: Unsupported numpy type 17\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m \n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m \n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m ray.air.util.tensor_extensions.arrow.ArrowConversionError: Error converting data to Arrow: column: 'responsive_documents', shape: (4,), dtype: object, data: [array(['The Salt Lake, seventy miles long and thirty-five wide, is situated three miles eight hundred feet above the sea. Quite diffe...\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 780, in from_numpy\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m     return cls._from_numpy(arr)\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 794, in _from_numpy\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m     return ArrowVariableShapedTensorArray.from_numpy(arr)\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/air/util/tensor_extensions/arrow.py\", line 233, in convert_to_pyarrow_array\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m     return ArrowTensorArray.from_numpy(\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MapWorker(MapBatches(Chat)) pid=31533, ip=10.0.50.252)\u001b[0m     raise ArrowConversionError(data_str) from e\n",
      "2026-01-22 01:06:20,981\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:06:20,982\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:20,982\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:20,983\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:20,984\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 81.9KiB object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:20,985\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:20,985\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:20,986\tINFO progress_bar.py:215 -- Running Dataset: dataset_165_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.1MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:26,041\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:06:26,043\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:26,044\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:26,045\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:26,046\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 81.9KiB object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:26,047\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:26,047\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:26,048\tINFO progress_bar.py:215 -- Running Dataset: dataset_165_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.1MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:31,136\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:06:31,138\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:31,139\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:31,140\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:31,141\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 81.9KiB object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:31,141\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:31,143\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:31,144\tINFO progress_bar.py:215 -- Running Dataset: dataset_165_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.1MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:36,213\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:06:36,214\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:36,215\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:36,217\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:36,217\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:36,218\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:36,220\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:36,221\tINFO progress_bar.py:215 -- Running Dataset: dataset_165_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:41,294\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:06:41,295\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:41,296\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:41,297\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:41,297\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:41,298\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:41,299\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:41,300\tINFO progress_bar.py:215 -- Running Dataset: dataset_165_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:46,349\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:06:46,351\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:46,351\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:46,352\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:46,353\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:46,354\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:46,355\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:46,357\tINFO progress_bar.py:215 -- Running Dataset: dataset_165_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:51,416\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:06:51,417\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:51,418\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:51,418\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:51,419\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:51,420\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:51,421\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:51,421\tINFO progress_bar.py:215 -- Running Dataset: dataset_165_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:56,492\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:06:56,493\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:56,494\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:56,495\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:56,495\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:06:56,496\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:56,497\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:06:56,498\tINFO progress_bar.py:215 -- Running Dataset: dataset_165_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:07:01,553\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:07:01,554\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:07:01,554\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:07:01,555\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:07:01,556\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:07:01,557\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 1; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 384.0MiB object store; [0/1 objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:07:01,558\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:07:01,559\tINFO progress_bar.py:215 -- Running Dataset: dataset_165_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:07:06,587\tINFO progress_bar.py:215 -- Write: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:07:06,589\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:07:06,590\tINFO progress_bar.py:215 -- Running Dataset: dataset_165_0. Active & requested resources: 0/32 CPU, 1/2 GPU, 172.0B/26.9GiB object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:07:06,592\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:07:06,591\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 23 / 23\n",
      "2026-01-22 01:07:06,593\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:07:06,594\tINFO progress_bar.py:215 -- MapBatches(ChromaDBReader): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:07:06,594\tINFO progress_bar.py:215 -- MapBatches(Embedder): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:07:06,595\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:07:06,597\tINFO progress_bar.py:215 -- MapBatches(PromptEnhancer): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / 23\n",
      "2026-01-22 01:07:06,597\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / ?\n",
      "2026-01-22 01:07:06,598\tINFO progress_bar.py:215 -- MapBatches(Chat): Tasks: 0; Actors: 1; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.0 GPU, 0.0B object store; [0/1 objects local]: Progress Completed 23 / ?\n",
      "2026-01-22 01:07:06,623\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_165_0 execution finished in 61.20 seconds\n",
      "INFO:openlineage.client.client:OpenLineageClient will use `composite` transport\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n",
      "2026-01-22 01:07:06,684\tINFO dataset.py:5344 -- Data sink Parquet finished. 23 rows and 123.7KiB data written.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline complete! Results written to /mnt/cluster_storage/batch_output_1.parquet\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Production Deployment: Write Results to Parquet\n",
    "# ============================================================================\n",
    "# In production, you typically want to persist results to storage rather than\n",
    "# loading them into memory. This approach:\n",
    "#\n",
    "# 1. Handles datasets larger than memory (streaming)\n",
    "# 2. Provides durability (results survive failures)\n",
    "# 3. Enables downstream processing (other jobs can read the output)\n",
    "#\n",
    "# write_parquet() streams data directly to storage without materializing\n",
    "# the entire dataset in memory.\n",
    "# ============================================================================\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Clean up previous output (for demo purposes)\n",
    "shutil.rmtree('/mnt/cluster_storage/batch_output_1.parquet', ignore_errors=True)\n",
    "\n",
    "print(\"Running complete RAG pipeline and writing results to Parquet...\")\n",
    "\n",
    "# Run the complete pipeline and write to Parquet\n",
    "ray.data.read_parquet('/mnt/cluster_storage/prompts.parquet') \\\n",
    "    .map_batches(\n",
    "        Embedder, \n",
    "        fn_constructor_args=[EMBEDDER_MODEL], \n",
    "        compute=ray.data.ActorPoolStrategy(size=4),\n",
    "        batch_size=4\n",
    "    ) \\\n",
    "    .map_batches(\n",
    "        ChromaDBReader, \n",
    "        fn_constructor_args=['persistent_text_chunks', 3], \n",
    "        compute=ray.data.ActorPoolStrategy(size=2)\n",
    "    ) \\\n",
    "    .map_batches(\n",
    "        PromptEnhancer, \n",
    "        compute=ray.data.ActorPoolStrategy(size=2)\n",
    "    ) \\\n",
    "    .map_batches(\n",
    "        Chat, \n",
    "        compute=ray.data.ActorPoolStrategy(size=1), \n",
    "        fn_constructor_args=[CHAT_MODEL], \n",
    "        num_gpus=1, \n",
    "        batch_size=4\n",
    "    ) \\\n",
    "    .write_parquet('/mnt/cluster_storage/batch_output_1.parquet')\n",
    "\n",
    "print(\"\\nPipeline complete! Results written to /mnt/cluster_storage/batch_output_1.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94818c98",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c1a145-04b8-4e07-a547-6b79bfc6a751",
   "metadata": {},
   "source": [
    "\n",
    "### What We Built\n",
    "\n",
    "We implemented a complete RAG (Retrieval-Augmented Generation) pipeline using Ray Data:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         COMPLETE RAG PIPELINE                                   │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐\n",
    "  │ Prompts  │───▶│ Embedder │───▶│ ChromaDB │───▶│  Prompt  │───▶│   Chat   │\n",
    "  │ (Parquet)│    │  (CPU)   │    │  Reader  │    │ Enhancer │    │  (GPU)   │\n",
    "  └──────────┘    └──────────┘    │  (CPU)   │    │  (CPU)   │    └──────────┘\n",
    "                                  └──────────┘    └──────────┘         │\n",
    "                                                                       ▼\n",
    "                                                                 ┌──────────┐\n",
    "                                                                 │  Output  │\n",
    "                                                                 │ (Parquet)│\n",
    "                                                                 └──────────┘\n",
    "```\n",
    "\n",
    "| Stage | Class | Purpose | Resource |\n",
    "|-------|-------|---------|----------|\n",
    "| 0 | `DocEmbedder` + `ChromaDBWriter` | Ingest documents into vector DB | CPU |\n",
    "| 1 | `Embedder` | Convert user queries to vector embeddings | CPU |\n",
    "| 2 | `ChromaDBReader` | Retrieve similar documents from vector DB | CPU |\n",
    "| 3 | `PromptEnhancer` | Combine query with retrieved context | CPU |\n",
    "| 4 | `Chat` | Generate responses using an LLM | **GPU** |\n",
    "\n",
    "### Key Ray Data Concepts\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    KEY CONCEPTS SUMMARY                                  │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  1. map_batches()          → Apply transformations to data batches      │\n",
    "│  2. Callable Classes       → __init__ for setup, __call__ for batches   │\n",
    "│  3. ActorPoolStrategy      → Keep workers alive across batches          │\n",
    "│  4. Resource Allocation    → num_gpus for GPU, defaults to CPU          │\n",
    "│  5. Chaining               → .map_batches(...).map_batches(...)         │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### API Note (Ray 2.53.0+)\n",
    "\n",
    "- **DEPRECATED**: `concurrency=N` parameter\n",
    "- **USE INSTEAD**: `compute=ray.data.ActorPoolStrategy(size=N)`\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "| Practice | Why It Matters |\n",
    "|----------|----------------|\n",
    "| Load models in `__init__` | Avoids loading model for every batch |\n",
    "| Use same embedding model for ingestion and queries | Ensures vector space compatibility |\n",
    "| Use `ActorPoolStrategy` for stateful operations | Actors persist between batches |\n",
    "| Use fractional GPUs when appropriate | Allows multiple models to share GPU |\n",
    "| Write to storage with `write_parquet()` | Handles datasets larger than memory |\n",
    "| Safe string formatting (not `eval()`) | Security and reliability |\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| CUDA out of memory | Reduce batch_size, use fractional GPUs, or smaller models |\n",
    "| Empty retrieval results | Check ingestion used same embedding model |\n",
    "| Slow embedding | Use GPU for larger models, or more CPU actors |\n",
    "| `concurrency` deprecation warning | Use `compute=ray.data.ActorPoolStrategy()` |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Experiment with models**: Try `instructor-large` (with GPU) for better quality\n",
    "- **Improve chunking**: Use smarter document chunking (semantic boundaries)\n",
    "- **Add metadata**: Store chapter numbers, page references in ChromaDB\n",
    "- **Implement caching**: Cache embeddings for frequently asked questions\n",
    "- **Add evaluation**: Implement automated response quality evaluation\n",
    "\n",
    "### References\n",
    "\n",
    "- [Ray Data Documentation](https://docs.ray.io/en/latest/data/batch_inference.html)\n",
    "- [Ray Data map_batches API](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
