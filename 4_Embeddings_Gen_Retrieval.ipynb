{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1d247-cb22-4115-89c1-e5fbdc0dc633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import ray\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f00640-1a22-492a-85eb-638f8f30c1e1",
   "metadata": {},
   "source": [
    "# Generating, storing, and retrieving embeddings with Ray Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2a18d-abf3-4342-894e-8154fdba2a92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"hkunlp/instructor-large\"\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5a05e-1f74-479f-a63c-bd14783c697b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = [\"What are some top attractions in Seattle?\", \"What are some top attractions in Los Angeles?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6bfa41-1a60-42da-bc56-48006ac2a86c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectors = model.encode(items)\n",
    "\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00820505-d734-40e8-aeb9-ee02db88c55c",
   "metadata": {},
   "source": [
    "We move the data to shared storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a090751-a222-4f7a-b20b-af586fc777fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cp around.txt /mnt/cluster_storage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8314c12-e9ed-4fd4-a146-8120c19086fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paras_ds = ray.data.read_text(\"/mnt/cluster_storage/around.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f87ec-a0bd-434c-ad85-b4db342b7787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paras_ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95898c5f-a051-4335-89ad-92e94eac2f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paras_ds.take_batch(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df05d1-eebc-4a20-b8bd-e6fd6f100033",
   "metadata": {
    "tags": []
   },
   "source": [
    "To generate our emeddings, we'll use two steps\n",
    "\n",
    "1. Create a class that performs the embedding operation\n",
    "    1. We use a class because we'll want to hold on to a large, valuable piece of state -- the embedding model itself\n",
    "    1. For use with our vector databases, we'll need unique IDs to go with each document and embedding -- we'll generate UUIDs\n",
    "    1. the output from the `__call__` method will be similar to the input: a dict with the column names as keys, and vectorized types for values\n",
    "1. Call `dataset.map_batches(...)` where we connect the dataset to the processing class as well as specify resources like the number of class instances (actors) and GPUs\n",
    "    1. Specify an autoscaling actor pool -- to demo how Ray could autoscale to handle large, uneven workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66789137-7b23-4a70-aed4-e3745000df80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DocEmbedder:\n",
    "    def __init__(self):\n",
    "        self._model = SentenceTransformer(\"hkunlp/instructor-large\")\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        inputs = batch['text']\n",
    "        embeddings = self._model.encode(inputs, device='cuda:0')\n",
    "        ids = np.array([uuid.uuid1().hex for i in inputs])\n",
    "        return { 'doc' : inputs, 'vec' : embeddings, 'id' : ids }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150cfd9-f22f-4137-a1c9-75c05f5e7685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vecs = paras_ds.map_batches(DocEmbedder, compute=ray.data.ActorPoolStrategy(min_size=2, max_size=8), num_gpus=0.125, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13df2c-56bc-4105-8f73-a5aa419003d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_batch = vecs.take_batch(4)\n",
    "\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20851404-9548-4d14-b48e-2eed6dfb2705",
   "metadata": {},
   "source": [
    "### Vector storage example: ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b3da0-1512-45a7-88b7-10918373ed19",
   "metadata": {},
   "source": [
    "Ray focuses on compute and is orthogonal to data storage, so many data stores can be used.\n",
    "\n",
    "For a simple example, we'll use ChromaDB, starting with a minimal in-memory demo so that we can see the prorgamming pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a09e60-b3fa-4e12-9915-174b6c9a0198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"my_text_chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88cecc-2943-4448-b114-7bb58da320c8",
   "metadata": {},
   "source": [
    "Insert the vectors, documents, and IDs\n",
    "\n",
    "> Note that Chroma can also accept arbitrary metadata dictionaries for each document, which you can then use in your queries (along with semantic similarity) and see in results. Metadata allows you to easily add powerful features like \"search only in chapter 3\" or \"cite source URLs for data returned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aeaa33-d54e-4574-9286-07a0b80145e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection.upsert(\n",
    "    embeddings=sample_batch['vec'].tolist(),\n",
    "    documents=sample_batch['doc'].tolist(),\n",
    "    ids=sample_batch['id'].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de3493-1c3a-4978-a4c2-d21dbdb28443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_query = model.encode(\"tell me about money\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0467e1-4045-47ba-bf58-20b0512fb51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_embeddings=[test_query],\n",
    "    n_results=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc2a19-ca1e-4232-a973-04079625fcf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c078d82-2a9a-4b21-97a0-d730b1b3758d",
   "metadata": {},
   "source": [
    "### Scaling queries with Chroma\n",
    "\n",
    "Now that we have the basics of Chroma down, let's look at scaling to large datasets.\n",
    "\n",
    "We'll create a Ray Core Actor that provides access to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e86ca-2690-422c-8cc3-fa261c016584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ray.remote(concurrency_groups={\"write\": 4, \"read\": 16})\n",
    "class ChromaWrapper:\n",
    "    def __init__(self):\n",
    "        self.chroma_client = chromadb.PersistentClient(path=\"/mnt/cluster_storage/vector_store\")\n",
    "        self.collection = self.chroma_client.get_or_create_collection(name=\"persistent_text_chunks\")\n",
    "\n",
    "    @ray.method(concurrency_group=\"write\")\n",
    "    def upsert(self, batch):\n",
    "        self.collection.upsert(\n",
    "            embeddings=batch['vec'].tolist(),\n",
    "            documents=batch['doc'].tolist(),\n",
    "            ids=batch['id'].tolist()\n",
    "        )\n",
    "        return len(batch['id'])\n",
    "\n",
    "    @ray.method(concurrency_group=\"read\")\n",
    "    def query(self, q):\n",
    "        return self.collection.query(query_embeddings=[q], n_results=3)\n",
    "\n",
    "chroma_server = ChromaWrapper.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867eb1e-cc4b-41dd-9852-7f04a8f1aaf7",
   "metadata": {},
   "source": [
    "We're using `map_batches` with a side-effect to write the vectors to the database. Alternative approach would include\n",
    "* writing a custom sink so we could use code like `my_dataset_with_vectors.write_cool_vectordb('collection')`\n",
    "* writing to a standard storage format like parquet and using another scripted workflow step to bulk load the database\n",
    "\n",
    "In this code example, we also demo using `map_batches` in (stateless) task form, and using a lambda, to access a running actor.\n",
    "\n",
    "> As an exercise, rewrite this to use `map_batches` with an actor (callable) class, the way we've done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f705c52c-e9d0-45b9-bf7a-064327e569a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vecs.map_batches(lambda batch: {'batch_count': [ray.get(chroma_server.upsert.remote(batch))]}).sum('batch_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134d7ec-f01c-48b1-beb4-8e142f54dcf2",
   "metadata": {},
   "source": [
    "Since our service is running as an actor, we can quickly test out a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f13cb-c2fb-4c0b-b72f-9929984a934c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utah_query_vec = model.encode(\"Describe the body of water in Utah\").tolist()\n",
    "\n",
    "ray.get(chroma_server.query.remote(utah_query_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea421838-616a-4182-bc11-2ff7f3fa8122",
   "metadata": {},
   "source": [
    "> Exercise 1: Rewrite the test code here to use Ray Data, `map_batches`, and a dedicated ChromaDB retrieval actor. Hint: for testing purposes you can create a small Ray Dataset from Python strings with `.from_items`\n",
    ">\n",
    "> Exercise 2: Define a Ray Serve deployment that queries the data using Chroma. Such a service would be useful for online/low-latency recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489741a7-ecb7-46db-a861-6c7869902d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
