{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee788c7-5783-4162-b1c8-00fe2a30de76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import ray\n",
    "import requests\n",
    "from ray import serve\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f9ef9-7f9e-446d-8876-40ca04f40b38",
   "metadata": {},
   "source": [
    "# Running a LLM with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63008356-89cd-41e3-b9f4-003f228d739b",
   "metadata": {},
   "source": [
    "__Road Map__:\n",
    "* Ray Core + Huggingface\n",
    "* Motivating Actors\n",
    "* Ray Core Actor\n",
    "* Using Actors in Ray AI Libraries\n",
    "* Ray Data\n",
    "* Ray Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5aaee4-e3cd-4119-aad3-74a9768549d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHAT_MODEL = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "\n",
    "prompt = \"Tell me something about large language models.\"\n",
    "\n",
    "@ray.remote(num_gpus=1) \n",
    "# Ray accounts for resources for schedule/load purposes and sets CUDA_VISIBLE_DEVICES but does not enforce resource usage quotas\n",
    "def basic_hf(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    pipe = pipeline(\"text-generation\", model=CHAT_MODEL, device='cuda', model_kwargs={\"cache_dir\": \"/mnt/local_storage\"})\n",
    "    return pipe(messages, max_new_tokens=200, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37467332-4761-4b55-9aba-e0cd9235e22e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref = basic_hf.remote(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ab5fa-0bc2-453c-9f67-dde3bb774296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e3c077-8721-47e8-b149-c82ab4a9ad87",
   "metadata": {
    "tags": []
   },
   "source": [
    "What's wrong with this code? We're loading the model from storage on every call.\n",
    "\n",
    "What's the solution? Ray Actors!\n",
    "\n",
    "Define an actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b73177-cc78-4f10-995b-f7bdc91d65e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=0.15)\n",
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        self.pipe = pipeline(\"text-generation\", model=model, device='cuda', model_kwargs={\"cache_dir\": \"/mnt/local_storage\"})\n",
    "\n",
    "    def reply(self, prompts):\n",
    "        messages = [ [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ] for prompt in prompts]\n",
    "        return self.pipe(messages, max_new_tokens=200, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdad79b-6f13-42ae-82ae-831654ddbdd5",
   "metadata": {},
   "source": [
    "Instantiate one or more actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba72ce7-604a-4a2c-9c07-d3de0e16a68a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = Chat.remote(CHAT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556cbdb-a986-4b4d-90c8-d068a9ab6c92",
   "metadata": {},
   "source": [
    "Make multiple calls to the same actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1251d-6684-4be1-a028-cc3130668f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = [\"What are some top attractions in Seattle?\", \"What are some top attractions in Los Angeles?\"]\n",
    "\n",
    "ref = chat.reply.remote(queries) \n",
    "\n",
    "ray.get(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b8b36-b70e-42cc-88e9-ea87a438eafe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "more_queries = [\"What are some top attractions in Vancouver BC?\", \"What are some top attractions in Portland OR?\"]\n",
    "\n",
    "ray.get(chat.reply.remote(more_queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132de9b-ab5c-417d-8976-304c00444c27",
   "metadata": {},
   "source": [
    "## Ray Data + Actors: LLM Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e63ec-03a3-491c-9214-e1b87bcfe9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cp prompts.parquet /mnt/cluster_storage/prompts.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c8deb-288a-4881-89e3-c153793f43e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = ray.data.read_parquet('/mnt/cluster_storage/prompts.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda641d7-f001-4072-83ea-9c66dcad3bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts.limit(5).take_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dded6a5-66e4-444e-bbef-2e6b2e1351f1",
   "metadata": {},
   "source": [
    "Recall that the workhorse of Ray Data processing pipelines is the `map_batches` API call on a `Dataset`\n",
    "\n",
    "`map_batches` supports stateless (tasks) and statefull (actors) processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef39584-7d90-45b6-b6a8-8270ed7db9cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PromptEnhancer:\n",
    "    def __call__(self, batch):\n",
    "        inputs = batch['prompt']\n",
    "        outputs = [ [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt }\n",
    "        ] for prompt in inputs]\n",
    "        batch['enhanced_prompt'] = outputs\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81a933-cfd1-4eb3-a1c7-b42ec9853c93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts.limit(5).map_batches(PromptEnhancer, concurrency=2).take_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd684f64-e0c9-49a8-b78a-be093bdbded9",
   "metadata": {},
   "source": [
    "Similar result, different internal semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876bdff-8baa-41bc-9fd5-3a156dcaaf3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts.map_batches(PromptEnhancer, concurrency=2).take_batch(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e1b4e-8ef4-48cb-903b-fdbcfdb19ccf",
   "metadata": {},
   "source": [
    "Once we have prompts, we can extend the pipeline with our LLM Chat functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ddbc8-c2de-4bfa-a509-a90dd77a2a35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        self.pipe = pipeline(\"text-generation\", model=model, device='cuda', model_kwargs={\"cache_dir\": \"/mnt/local_storage\"})\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        enhanced_prompts = [[j for j in i] for i in batch['enhanced_prompt']]\n",
    "        batch['responses'] = self.pipe(enhanced_prompts, max_new_tokens=200, truncation=True)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfcbbc6-261e-4e21-b789-955e45b48c29",
   "metadata": {},
   "source": [
    "Note that in this usage, we specify the resources in the `map_batches` call rather than on the actor (class) definition itself.\n",
    "\n",
    "As we'll see later, this provides more flexibility for performance and scaling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b7305-79d6-414c-937e-ccba8fe379e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts \\\n",
    "    .limit(5) \\\n",
    "    .map_batches(PromptEnhancer, concurrency=2) \\\n",
    "    .map_batches(Chat, concurrency=(2,4), fn_constructor_args=[CHAT_MODEL], num_gpus=0.15, batch_size=4) \\\n",
    "    .take_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896bfb9e-185c-4069-9e2a-279672d71693",
   "metadata": {},
   "source": [
    "# Bonus: Ray Actor as a service for low-latency inference with Ray Serve\n",
    "## Moving toward production-grade hosting with Ray Serve\n",
    "\n",
    "Ray Actors can be used to host a service providing basic encapsulation and RPC for internal clients\n",
    "\n",
    "For more robust services Ray Serve adds scalability, load balancing, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19ce28-590f-44df-a67c-0441dff82e96",
   "metadata": {},
   "source": [
    "### What is Ray Serve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065e019-3e84-4868-b727-33d4dea6ec8e",
   "metadata": {},
   "source": [
    "Serve is a microservices framework for serving ML â€“ the model serving\n",
    "component of Ray AI Libraries.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/serve_architecture.png' width=700/>\n",
    "\n",
    "### Deployments\n",
    "\n",
    "`Deployment` is the fundamental developer-facing element of serve.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment.png' width=600/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2eebc-9c7b-4657-9f60-0ddbc7ddf9c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"num_gpus\": 0.15})\n",
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        self.pipe = pipeline(\"text-generation\", model=model, device='cuda', model_kwargs={\"cache_dir\": \"/mnt/local_storage\"})\n",
    "\n",
    "    def reply(self, prompts):\n",
    "        messages = [ [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ] for prompt in prompts]\n",
    "        return self.pipe(messages, max_new_tokens=200, truncation=True)\n",
    "\n",
    "handle = serve.run(Chat.bind(model=CHAT_MODEL), name='chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80552ef-6fb5-4a2d-aa40-007ef49affd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref = handle.reply.remote(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba41909-07f9-4a02-b427-3ad1423cf4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "await ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85e009-8a8f-4cac-8761-cc0cca3419c3",
   "metadata": {},
   "source": [
    "Ray Serve has various other capabilites, including gRPC/HTTP access, FastAPI compatibility, inplace upgrading of components, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159d5a9-ad49-41e1-9b41-19480bbf7483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff576e-84bd-4dac-bcce-8f5855b20b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
