{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Ray Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Datasets: Distributed Data Preprocessing\n",
    "\n",
    "Ray Datasets are the standard way to load and exchange data in Ray libraries and applications. They provide basic distributed data transformations such as maps ([`map_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")), global and grouped aggregations ([`GroupedDataset`](https://docs.ray.io/en/latest/data/api/doc/ray.data.grouped_dataset.GroupedDataset.html#ray.data.grouped_dataset.GroupedDataset \"ray.data.grouped_dataset.GroupedDataset\")), and shuffling operations ([`random_shuffle`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\"), [`sort`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.sort.html#ray.data.Dataset.sort \"ray.data.Dataset.sort\"), [`repartition`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")), and are compatible with a variety of file formats, data sources, and distributed frameworks.\n",
    "\n",
    "Here's an overview of the integrations with other processing frameworks, file formats, and supported operations, as well as a glimpse at the Ray Datasets API.\n",
    "\n",
    "Check the [Input/Output reference](https://docs.ray.io/en/latest/data/api/input_output.html#input-output) to see if your favorite format is already supported.\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset.svg' width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets for Parallel Compute\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Datasets also simplify general purpose parallel GPU and CPU compute in Ray; for instance, for [GPU batch inference](https://docs.ray.io/en/latest/ray-overview/use-cases.html#ref-use-cases-batch-infer). They provide a higher-level API for Ray tasks and actors for such embarrassingly parallel compute, internally handling operations like batching, pipelining, and memory management.\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/stream-example.png' width=60%/>\n",
    "\n",
    "As part of the Ray ecosystem, Ray Datasets can leverage the full functionality of Ray's distributed scheduler, e.g., using actors for optimizing setup time and GPU scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data[](https://docs.ray.io/en/latest/data/key-concepts.html#reading-data \"Permalink to this headline\")\n",
    "\n",
    "Datasets uses Ray tasks to read data from remote storage. When reading from a file-based datasource (e.g., S3, GCS), it creates a number of read tasks proportional to the number of CPUs in the cluster. Each read task reads its assigned files and produces an output block:\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-read.svg' width=60%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 20:26:53,722\tINFO worker.py:1821 -- Connecting to existing Ray cluster at address: 10.0.142.230:6379...\n",
      "2026-01-20 20:26:53,733\tINFO worker.py:1998 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-v4klp1kjtnk9yrxwdcz5ah11ub.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-01-20 20:26:53,758\tINFO packaging.py:463 -- Pushing file package 'gcs://_ray_pkg_0b7f0c167e67268a4d6bf905ca9322f25f293996.zip' (9.27MiB) to Ray cluster...\n",
      "2026-01-20 20:26:53,795\tINFO packaging.py:476 -- Successfully pushed file package 'gcs://_ray_pkg_0b7f0c167e67268a4d6bf905ca9322f25f293996.zip'.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025c058c679e48e1bad804d3332e59c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset(num_rows=?, schema=Unknown schema)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ds = ray.data.read_csv(\"s3://anyscale-public-materials/ray-ai-libraries/Iris.csv\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 20:27:01,141\tINFO dataset.py:3641 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2026-01-20 20:27:01,146\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_166_0\n",
      "2026-01-20 20:27:01,167\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_166_0. Full logs are in /tmp/ray/session_2026-01-20_18-18-31_241199_2386/logs/ray-data\n",
      "2026-01-20 20:27:01,167\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_166_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> LimitOperator[limit=3]\n",
      "2026-01-20 20:27:01,168\tINFO streaming_executor.py:687 -- [dataset]: A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True` and `ray.data.DataContext.get_current().use_ray_tqdm = False`.\n",
      "2026-01-20 20:27:01,169\tINFO progress_bar.py:155 -- Progress bar disabled because stdout is a non-interactive terminal.\n",
      "2026-01-20 20:27:01,171\tWARNING resource_manager.py:136 -- ⚠️  Ray's object store is configured to use only 27.8% of available memory (40.0GiB out of 144.0GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "2026-01-20 20:27:01,197\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-20 20:27:01,198\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:01,198\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-20 20:27:01,199\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:01,200\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=3} ===\n",
      "2026-01-20 20:27:01,200\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:01,201\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-20 20:27:01,202\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:06,291\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:06,292\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:06,293\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:06,294\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:11,384\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:11,384\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:11,385\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:11,386\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:16,467\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:16,468\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:16,469\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:16,470\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:21,553\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:21,554\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:21,554\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:21,555\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:26,637\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:26,638\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:26,639\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:26,641\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:31,722\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:31,723\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:31,723\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:31,724\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:36,803\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:36,804\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:36,805\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:36,806\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:41,890\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:41,891\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:41,892\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:41,892\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:46,973\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:46,974\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:46,975\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:46,976\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:52,060\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:52,060\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:52,061\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:52,062\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:57,143\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:57,143\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:57,144\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:27:57,145\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:02,228\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:02,229\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:02,229\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:02,230\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:07,317\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:07,318\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:07,318\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:07,319\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:12,404\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:12,405\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:12,406\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:12,407\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:17,489\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:17,490\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:17,490\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:17,491\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:22,575\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:22,576\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:22,577\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:22,578\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:27,662\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:27,663\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:27,664\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:27,664\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +1m40s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 20:28:34,990\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / 1\n",
      "2026-01-20 20:28:34,991\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 16.8KiB object store: Progress Completed 150 / ?\n",
      "2026-01-20 20:28:34,992\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:34,992\tINFO progress_bar.py:215 -- Running Dataset: dataset_166_0. Active & requested resources: 1/32 CPU, 25.3KiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:35,001\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_166_0 execution finished in 93.83 seconds\n",
      "INFO:openlineage.client.client:OpenLineageClient will use `composite` transport\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Id': 1, 'SepalLengthCm': 5.1, 'SepalWidthCm': 3.5, 'PetalLengthCm': 1.4, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa'}\n",
      "{'Id': 2, 'SepalLengthCm': 4.9, 'SepalWidthCm': 3.0, 'PetalLengthCm': 1.4, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa'}\n",
      "{'Id': 3, 'SepalLengthCm': 4.7, 'SepalWidthCm': 3.2, 'PetalLengthCm': 1.3, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa'}\n"
     ]
    }
   ],
   "source": [
    "ds.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 20:28:35,103\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_168_0\n",
      "2026-01-20 20:28:35,107\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_168_0. Full logs are in /tmp/ray/session_2026-01-20_18-18-31_241199_2386/logs/ray-data\n",
      "2026-01-20 20:28:35,108\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_168_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[Write]\n",
      "2026-01-20 20:28:35,125\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-20 20:28:35,126\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:35,126\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-20 20:28:35,127\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:35,128\tINFO progress_bar.py:213 -- === Ray Data Progress {Write} ===\n",
      "2026-01-20 20:28:35,129\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:35,130\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-20 20:28:35,131\tINFO progress_bar.py:215 -- Running Dataset: dataset_168_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:35,384\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_168_0 execution finished in 0.27 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n",
      "2026-01-20 20:28:35,561\tINFO dataset.py:5344 -- Data sink Parquet finished. 150 rows and 8.4KiB data written.\n"
     ]
    }
   ],
   "source": [
    "ds.write_parquet('/mnt/cluster_storage/parquet_iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b> \n",
    "\n",
    "We  write to `/mnt/cluster_storage`. This is a path that is available on all nodes in the cluster. If instead you use a path that is only local to one of the nodes in a multi-node cluster, you will see errors like `FileNotFoundError: [Errno 2] No such file or directory: '/path/to/file'`.\n",
    "\n",
    "This is because Ray Data is designed to work with distributed storage systems like S3, HDFS, etc. If you want to write to local storage, you can add a special prefix `local://` to the path. In this case, Ray will only schedule tasks on the head node of the cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "-rw-r--r-- 1 ray users 3722 Jan 20 18:30 0_1679641afb914a4d941be4a3b7a7cf3c_000000_000000-0.parquet\n",
      "-rw-r--r-- 1 ray users 3722 Jan 20 20:28 164_953e278c878a459e80c7f9a52b83212b_000000_000000-0.parquet\n"
     ]
    }
   ],
   "source": [
    "! ls -l /mnt/cluster_storage/parquet_iris/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data[](https://docs.ray.io/en/latest/data/key-concepts.html#transforming-data \"Permalink to this headline\")\n",
    "\n",
    "Datasets can use either Ray tasks or Ray actors to transform datasets. Using actors allows for expensive state initialization (e.g., for GPU-based tasks) to be cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61f9054d66748e7888ad973163b6d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repartition\n",
       "+- Dataset(num_rows=?, schema=Unknown schema)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.repartition(5)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 20:28:55,537\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_172_0\n",
      "2026-01-20 20:28:55,542\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_172_0. Full logs are in /tmp/ray/session_2026-01-20_18-18-31_241199_2386/logs/ray-data\n",
      "2026-01-20 20:28:55,543\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_172_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> AllToAllOperator[Repartition] -> LimitOperator[limit=5]\n",
      "2026-01-20 20:28:55,562\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-20 20:28:55,563\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:55,563\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-20 20:28:55,564\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:55,565\tINFO progress_bar.py:213 -- === Ray Data Progress {Repartition} ===\n",
      "2026-01-20 20:28:55,566\tINFO progress_bar.py:215 -- Repartition: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; 0 rows output: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:55,567\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=5} ===\n",
      "2026-01-20 20:28:55,568\tINFO progress_bar.py:215 -- limit=5: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:55,568\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-20 20:28:55,569\tINFO progress_bar.py:215 -- Running Dataset: dataset_172_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:28:55,795\tINFO progress_bar.py:213 -- === Ray Data Progress {*- Split Repartition} ===\n",
      "2026-01-20 20:28:55,795\tINFO progress_bar.py:215 -- *- Split Repartition: Progress Completed 150 / 1\n",
      "2026-01-20 20:28:59,449\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_172_0 execution finished in 3.91 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Id': array([1, 2, 3, 4, 5]),\n",
       " 'SepalLengthCm': array([5.1, 4.9, 4.7, 4.6, 5. ]),\n",
       " 'SepalWidthCm': array([3.5, 3. , 3.2, 3.1, 3.6]),\n",
       " 'PetalLengthCm': array([1.4, 1.4, 1.3, 1.5, 1.4]),\n",
       " 'PetalWidthCm': array([0.2, 0.2, 0.2, 0.2, 0.2]),\n",
       " 'Species': array(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "        'Iris-setosa'], dtype=object)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.take_batch(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling Data[](https://docs.ray.io/en/latest/data/key-concepts.html#shuffling-data \"Permalink to this headline\")\n",
    "\n",
    "Certain operations like *sort* or *groupby* require data blocks to be partitioned by value, or *shuffled*. Datasets uses tasks to implement distributed shuffles in a map-reduce style, using map tasks to partition blocks by value, and then reduce tasks to merge co-partitioned blocks together.\n",
    "\n",
    "You can also change just the number of blocks of a Dataset using [`repartition()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\"). Repartition has two modes:\n",
    "\n",
    "1.  `shuffle=False` - performs the minimal data movement needed to equalize block sizes __(the default)__\n",
    "\n",
    "2.  `shuffle=True` - performs a full distributed shuffle __(not the default because it's more expensive)__\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-shuffle.svg' width=60%/>\n",
    "\n",
    "Datasets shuffle can scale to processing hundreds of terabytes of data. See the [Performance Tips Guide](https://docs.ray.io/en/latest/data/performance-tips.html#shuffle-performance-tips) for an in-depth guide on shuffle performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_batch(batch):\n",
    "    areas = []\n",
    "    for ix in range(len(batch['Id'])):\n",
    "        areas.append(batch[\"PetalLengthCm\"][ix] * batch[\"PetalWidthCm\"][ix])        \n",
    "    batch['approximate_petal_area'] = areas\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 20:31:45,921\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_178_0\n",
      "2026-01-20 20:31:45,925\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_178_0. Full logs are in /tmp/ray/session_2026-01-20_18-18-31_241199_2386/logs/ray-data\n",
      "2026-01-20 20:31:45,926\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_178_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> AllToAllOperator[Repartition] -> LimitOperator[limit=20] -> TaskPoolMapOperator[MapBatches(transform_batch)]\n",
      "2026-01-20 20:31:45,943\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-20 20:31:45,944\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:31:45,945\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-20 20:31:45,946\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:31:45,947\tINFO progress_bar.py:213 -- === Ray Data Progress {Repartition} ===\n",
      "2026-01-20 20:31:45,948\tINFO progress_bar.py:215 -- Repartition: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; 0 rows output: Progress Completed 0 / ?\n",
      "2026-01-20 20:31:45,948\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=20} ===\n",
      "2026-01-20 20:31:45,949\tINFO progress_bar.py:215 -- limit=20: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:31:45,950\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(transform_batch)} ===\n",
      "2026-01-20 20:31:45,950\tINFO progress_bar.py:215 -- MapBatches(transform_batch): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:31:45,951\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-20 20:31:45,951\tINFO progress_bar.py:215 -- Running Dataset: dataset_178_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:31:46,300\tINFO progress_bar.py:213 -- === Ray Data Progress {*- Split Repartition} ===\n",
      "2026-01-20 20:31:46,300\tINFO progress_bar.py:215 -- *- Split Repartition: Progress Completed 150 / 1\n",
      "2026-01-20 20:31:50,040\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_178_0 execution finished in 4.11 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Id': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20]),\n",
       " 'SepalLengthCm': array([5.1, 4.9, 4.7, 4.6, 5. , 5.4, 4.6, 5. , 4.4, 4.9, 5.4, 4.8, 4.8,\n",
       "        4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1]),\n",
       " 'SepalWidthCm': array([3.5, 3. , 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3. ,\n",
       "        3. , 4. , 4.4, 3.9, 3.5, 3.8, 3.8]),\n",
       " 'PetalLengthCm': array([1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.6, 1.4,\n",
       "        1.1, 1.2, 1.5, 1.3, 1.4, 1.7, 1.5]),\n",
       " 'PetalWidthCm': array([0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1,\n",
       "        0.1, 0.2, 0.4, 0.4, 0.3, 0.3, 0.3]),\n",
       " 'Species': array(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "        'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "        'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "        'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "        'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa'],\n",
       "       dtype=object),\n",
       " 'approximate_petal_area': array([0.28, 0.28, 0.26, 0.3 , 0.28, 0.68, 0.42, 0.3 , 0.28, 0.15, 0.3 ,\n",
       "        0.32, 0.14, 0.11, 0.24, 0.6 , 0.52, 0.42, 0.51, 0.45])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.map_batches(transform_batch) \\\n",
    "  .take_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More example operations: transforming Datasets\n",
    "\n",
    "Datasets transformations take in datasets and produce new datasets. For example, *map* is a transformation that applies a user-defined function on each dataset record and returns a new dataset as the result. Datasets transformations can be composed to express a chain of computations.\n",
    "\n",
    "There are two main types of transformations:\n",
    "\n",
    "-   One-to-one: each input block will contribute to only one output block, such as [`ds.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\").\n",
    "\n",
    "-   All-to-all: input blocks can contribute to multiple output blocks, such as [`ds.random_shuffle()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\").\n",
    "\n",
    "Here is a table listing some common transformations supported by Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Ray Datasets transformations.[](https://docs.ray.io/en/latest/data/transforming-datasets.html#id2 \"Permalink to this table\")\n",
    "\n",
    "| Transformation | Type | Description |\n",
    "| --- | --- | --- |\n",
    "|[`ds.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")|One-to-one|Apply a given function to batches of records of this dataset.|\n",
    "|[`ds.add_column()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.add_column.html#ray.data.Dataset.add_column \"ray.data.Dataset.add_column\")|One-to-one|Apply a given function to batches of records to create a new column.|\n",
    "|[`ds.drop_columns()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.add_column.html#ray.data.Dataset.add_column \"ray.data.Dataset.add_column\")|One-to-one|Drop the given columns from the dataset.|\n",
    "|[`ds.split()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.split.html#ray.data.Dataset.split \"ray.data.Dataset.split\")|One-to-one|Split the dataset into N disjoint pieces.|\n",
    "|[`ds.repartition(shuffle=False)`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")|One-to-one|Repartition the dataset into N blocks, without shuffling the data.|\n",
    "|[`ds.repartition(shuffle=True)`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")|All-to-all|Repartition the dataset into N blocks, shuffling the data during repartition.|\n",
    "|[`ds.random_shuffle()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\")|All-to-all|Randomly shuffle the elements of this dataset.|\n",
    "|[`ds.sort()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.sort.html#ray.data.Dataset.sort \"ray.data.Dataset.sort\")|All-to-all|Sort the dataset by a sortkey.|\n",
    "|[`ds.groupby()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.groupby.html#ray.data.Dataset.groupby \"ray.data.Dataset.groupby\")|All-to-all|Group the dataset by a groupkey.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution mode\n",
    "\n",
    "Most transformations are **lazy** in Ray Data - i.e. they don't execute until you either:\n",
    "- **write a dataset to storage**\n",
    "- explicitly **materialize** the data\n",
    "- **iterate over the dataset** (usually when feeding data to model training).\n",
    "\n",
    "To explicitly *materialize* a very small subset of the data, you can use the `take_batch` method.\n",
    "Most transformations are lazy. They don't execute until you consume a dataset or call [`Dataset.materialize()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.materialize.html#ray.data.Dataset.materialize \"ray.data.Dataset.materialize\").\n",
    "\n",
    "The transformations are executed in a streaming way, incrementally on the data and with operators processed in parallel. For an in-depth guide on Datasets execution, read https://docs.ray.io/en/releases-2.8.0/data/data-internals.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To force computation and local caching of the entire dataset, you can used `materialize`. Consider the performance constraints and impacts of caching, though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 20:33:15,588\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_179_0\n",
      "2026-01-20 20:33:15,591\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_179_0. Full logs are in /tmp/ray/session_2026-01-20_18-18-31_241199_2386/logs/ray-data\n",
      "2026-01-20 20:33:15,592\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_179_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> AllToAllOperator[Repartition]\n",
      "2026-01-20 20:33:15,609\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-20 20:33:15,609\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:33:15,610\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-20 20:33:15,611\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:33:15,612\tINFO progress_bar.py:213 -- === Ray Data Progress {Repartition} ===\n",
      "2026-01-20 20:33:15,613\tINFO progress_bar.py:215 -- Repartition: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; 0 rows output: Progress Completed 0 / ?\n",
      "2026-01-20 20:33:15,613\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-20 20:33:15,614\tINFO progress_bar.py:215 -- Running Dataset: dataset_179_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:33:15,917\tINFO progress_bar.py:213 -- === Ray Data Progress {*- Split Repartition} ===\n",
      "2026-01-20 20:33:15,918\tINFO progress_bar.py:215 -- *- Split Repartition: Progress Completed 120 / 1\n",
      "2026-01-20 20:33:18,274\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_179_0 execution finished in 2.68 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c908431e4784da195b01ca600ff7521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MaterializedDataset(\n",
       "   num_blocks=5,\n",
       "   num_rows=150,\n",
       "   schema={\n",
       "      Id: int64,\n",
       "      SepalLengthCm: double,\n",
       "      SepalWidthCm: double,\n",
       "      PetalLengthCm: double,\n",
       "      PetalWidthCm: double,\n",
       "      Species: string\n",
       "   }\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.materialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming data with actors\n",
    "\n",
    "When using the actor compute strategy, per-row and per-batch UDFs can also be *callable classes*, i.e. classes that implement the `__call__` magic method. The constructor of the class can be used for stateful setup, and will be only invoked once per worker actor.\n",
    "\n",
    "To implement this, you can use the `map_batches` API with a \"Callable\" class method that implements:\n",
    "\n",
    "- `__init__`: Initialize any expensive state.\n",
    "- `__call__`: Perform the stateful transformation.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> These transformation APIs take the uninstantiated callable class as an argument, not an instance of the class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 20:35:13,500\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_182_0\n",
      "2026-01-20 20:35:13,505\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_182_0. Full logs are in /tmp/ray/session_2026-01-20_18-18-31_241199_2386/logs/ray-data\n",
      "2026-01-20 20:35:13,506\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_182_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> AllToAllOperator[Repartition] -> LimitOperator[limit=10] -> ActorPoolMapOperator[MapBatches(ModelExample)]\n",
      "2026-01-20 20:35:13,669\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-20 20:35:13,670\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:35:13,671\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-20 20:35:13,672\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:35:13,673\tINFO progress_bar.py:213 -- === Ray Data Progress {Repartition} ===\n",
      "2026-01-20 20:35:13,674\tINFO progress_bar.py:215 -- Repartition: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; 0 rows output: Progress Completed 0 / ?\n",
      "2026-01-20 20:35:13,675\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=10} ===\n",
      "2026-01-20 20:35:13,675\tINFO progress_bar.py:215 -- limit=10: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-20 20:35:13,676\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(ModelExample)} ===\n",
      "2026-01-20 20:35:13,676\tINFO progress_bar.py:215 -- MapBatches(ModelExample): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-20 20:35:13,677\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-20 20:35:13,678\tINFO progress_bar.py:215 -- Running Dataset: dataset_182_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store (pending: 2 CPU): Progress Completed 0 / ?\n",
      "2026-01-20 20:35:14,111\tINFO progress_bar.py:213 -- === Ray Data Progress {*- Split Repartition} ===\n",
      "2026-01-20 20:35:14,112\tINFO progress_bar.py:215 -- *- Split Repartition: Progress Completed 150 / 1\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py:416: UserWarning: The minimum number of concurrent actors for 'MapBatches(ModelExample)' is set to 2, but the operator only received 1 input(s). This means that the operator can launch at most 1 task(s), and won't fully utilize the available concurrency. You might be able to increase the number of concurrent tasks by configuring `override_num_blocks` earlier in the pipeline.\n",
      "  warnings.warn(\n",
      "2026-01-20 20:35:17,984\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_182_0 execution finished in 4.48 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Id': 1, 'SepalLengthCm': 5.1, 'SepalWidthCm': 3.5, 'PetalLengthCm': 1.4, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa', 'predictions': 0.29730177875068026}\n",
      "{'Id': 2, 'SepalLengthCm': 4.9, 'SepalWidthCm': 3.0, 'PetalLengthCm': 1.4, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa', 'predictions': 0.29730177875068026}\n",
      "{'Id': 3, 'SepalLengthCm': 4.7, 'SepalWidthCm': 3.2, 'PetalLengthCm': 1.3, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa', 'predictions': 0.29730177875068026}\n",
      "{'Id': 4, 'SepalLengthCm': 4.6, 'SepalWidthCm': 3.1, 'PetalLengthCm': 1.5, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa', 'predictions': 0.29730177875068026}\n",
      "{'Id': 5, 'SepalLengthCm': 5.0, 'SepalWidthCm': 3.6, 'PetalLengthCm': 1.4, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa', 'predictions': 0.29730177875068026}\n",
      "{'Id': 6, 'SepalLengthCm': 5.4, 'SepalWidthCm': 3.9, 'PetalLengthCm': 1.7, 'PetalWidthCm': 0.4, 'Species': 'Iris-setosa', 'predictions': 0.5356999058252557}\n",
      "{'Id': 7, 'SepalLengthCm': 4.6, 'SepalWidthCm': 3.4, 'PetalLengthCm': 1.4, 'PetalWidthCm': 0.3, 'Species': 'Iris-setosa', 'predictions': 0.40903897192829974}\n",
      "{'Id': 8, 'SepalLengthCm': 5.0, 'SepalWidthCm': 3.4, 'PetalLengthCm': 1.5, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa', 'predictions': 0.29730177875068026}\n",
      "{'Id': 9, 'SepalLengthCm': 4.4, 'SepalWidthCm': 2.9, 'PetalLengthCm': 1.4, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa', 'predictions': 0.29730177875068026}\n",
      "{'Id': 10, 'SepalLengthCm': 4.9, 'SepalWidthCm': 3.1, 'PetalLengthCm': 1.5, 'PetalWidthCm': 0.1, 'Species': 'Iris-setosa', 'predictions': 0.2011893487492697}\n"
     ]
    }
   ],
   "source": [
    "class ModelExample:\n",
    "    def __init__(self):\n",
    "        expensive_model_weights = [ 0.3, 1.75 ]\n",
    "        self.complex_model = lambda petal_width: (petal_width + expensive_model_weights[0])  ** expensive_model_weights[1]\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch[\"predictions\"] = self.complex_model(batch[\"PetalWidthCm\"])\n",
    "        return batch\n",
    "\n",
    "ds.map_batches(ModelExample, \n",
    "               compute=ray.data.ActorPoolStrategy(size=2)) \\\n",
    "  .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset details\n",
    "\n",
    "A Dataset consists of a list of Ray object references to *blocks*. Having multiple blocks in a dataset allows for parallel transformation and ingest.\n",
    "\n",
    "The following figure visualizes a tabular dataset with three blocks, each block holding 1000 rows each:\n",
    "\n",
    "|<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-intro/block.png\" width=\"700px\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|A Dataset when materialized is a distributed collection of blocks. This example illustrates a materialized dataset with three blocks, each block holding 1000 rows.|\n",
    "\n",
    "Since a Dataset is just a list of Ray object references, it can be freely passed between Ray tasks, actors, and libraries like any other object reference. This flexibility is a unique characteristic of Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "__Lab activity: Stateless transformation__\n",
    "    \n",
    "1. Create a Ray Dataset from the iris data in `/mnt/cluster_storage/parquet_iris`\n",
    "1. Create a \"sum of features\" transformation that calculates the sum of the Sepal Length, Sepal Width, Petal Length, and Petal Width features for the records\n",
    "    1. Design this transformation to take a Ray Dataset *batch* of records\n",
    "    1. Return the records without the ID column but with an additional column called \"sum\"\n",
    "    1. Hint: you do not need to use NumPy, but the calculation may be easier/simpler to code using NumPy vectorized operations with the records in the batch\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "__Lab activity: Stateful transformation__\n",
    "    \n",
    "1. Create a Ray Dataset from the iris data in `/mnt/cluster_storage/parquet_iris`\n",
    "1. Create an class that makes predictions on iris records using these steps:\n",
    "    1. in the class constructor, create an instance of the following \"model\" class:\n",
    "        ```python\n",
    "\n",
    "          class SillyModel():\n",
    "\n",
    "              def predict(self, petal_length):\n",
    "                  return petal_length + 0.42\n",
    "\n",
    "\n",
    "        ```\n",
    "    1. in the `__call__` method of the actor class\n",
    "        1. take a batch of records\n",
    "        1. create predictions for each record in the batch using the model instance\n",
    "            1. Hint: the code may be simpler using NumPy vectorized operations\n",
    "        1. add the predictions to the record batch\n",
    "        1. return the new, augmented batch\n",
    "1. Use that class to perform batch inference on the dataset using actors\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
