{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Ray Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Datasets: Distributed Data Preprocessing\n",
    "\n",
    "Ray Datasets are the standard way to load and exchange data in Ray libraries and applications. They provide basic distributed data transformations such as maps ([`map_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")), global and grouped aggregations ([`GroupedDataset`](https://docs.ray.io/en/latest/data/api/doc/ray.data.grouped_dataset.GroupedDataset.html#ray.data.grouped_dataset.GroupedDataset \"ray.data.grouped_dataset.GroupedDataset\")), and shuffling operations ([`random_shuffle`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\"), [`sort`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.sort.html#ray.data.Dataset.sort \"ray.data.Dataset.sort\"), [`repartition`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")), and are compatible with a variety of file formats, data sources, and distributed frameworks.\n",
    "\n",
    "Here's an overview of the integrations with other processing frameworks, file formats, and supported operations, as well as a glimpse at the Ray Datasets API.\n",
    "\n",
    "Check the [Input/Output reference](https://docs.ray.io/en/latest/data/api/input_output.html#input-output) to see if your favorite format is already supported.\n",
    "\n",
    "<img src='./assets/ray_data.png' width=70%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets for Parallel Compute\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Datasets also simplify general purpose parallel GPU and CPU compute in Ray; for instance, for [GPU batch inference](https://docs.ray.io/en/latest/ray-overview/use-cases.html#ref-use-cases-batch-infer). They provide a higher-level API for Ray tasks and actors for such embarrassingly parallel compute, internally handling operations like batching, pipelining, and memory management.\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/stream-example.png' width=60%/>\n",
    "\n",
    "As part of the Ray ecosystem, Ray Datasets can leverage the full functionality of Ray's distributed scheduler, e.g., using actors for optimizing setup time and GPU scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data[](https://docs.ray.io/en/latest/data/key-concepts.html#reading-data \"Permalink to this headline\")\n",
    "\n",
    "Datasets uses Ray tasks to read data from remote storage. When reading from a file-based datasource (e.g., S3, GCS), it creates a number of read tasks proportional to the number of CPUs in the cluster. Each read task reads its assigned files and produces an output block:\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-read.svg' width=60%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:15:45,851\tINFO worker.py:1821 -- Connecting to existing Ray cluster at address: 10.0.9.248:6379...\n",
      "2026-01-22 01:15:45,865\tINFO worker.py:1998 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-v4klp1kjtnk9yrxwdcz5ah11ub.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-01-22 01:15:45,894\tINFO packaging.py:463 -- Pushing file package 'gcs://_ray_pkg_12060dcf4c9f1881df591e677439e024f426b2f1.zip' (10.59MiB) to Ray cluster...\n",
      "2026-01-22 01:15:45,935\tINFO packaging.py:476 -- Successfully pushed file package 'gcs://_ray_pkg_12060dcf4c9f1881df591e677439e024f426b2f1.zip'.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8376715784df4ea9b8fbe88abce03f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset(num_rows=?, schema=Unknown schema)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ds = ray.data.read_csv(\"s3://anyscale-public-materials/ray-ai-libraries/Iris.csv\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:15:46,350\tINFO dataset.py:3641 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2026-01-22 01:15:46,355\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_169_0\n",
      "2026-01-22 01:15:46,382\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_169_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 01:15:46,383\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_169_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> LimitOperator[limit=3]\n",
      "2026-01-22 01:15:46,385\tINFO streaming_executor.py:687 -- [dataset]: A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True` and `ray.data.DataContext.get_current().use_ray_tqdm = False`.\n",
      "2026-01-22 01:15:46,386\tINFO progress_bar.py:155 -- Progress bar disabled because stdout is a non-interactive terminal.\n",
      "2026-01-22 01:15:46,389\tWARNING resource_manager.py:136 -- ⚠️  Ray's object store is configured to use only 27.8% of available memory (40.0GiB out of 144.0GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "2026-01-22 01:15:46,422\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 01:15:46,424\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:46,425\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 01:15:46,426\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:46,428\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=3} ===\n",
      "2026-01-22 01:15:46,428\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:46,431\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 01:15:46,431\tINFO progress_bar.py:215 -- Running Dataset: dataset_169_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:51,436\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 76.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:15:51,438\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:51,438\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:51,439\tINFO progress_bar.py:215 -- Running Dataset: dataset_169_0. Active & requested resources: 1/32 CPU, 384.0MiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:57,401\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 76.0B object store: Progress Completed 1 / 1\n",
      "2026-01-22 01:15:57,402\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 16.8KiB object store: Progress Completed 150 / ?\n",
      "2026-01-22 01:15:57,402\tINFO progress_bar.py:215 -- limit=3: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 165.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:57,403\tINFO progress_bar.py:215 -- Running Dataset: dataset_169_0. Active & requested resources: 1/32 CPU, 25.3KiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:57,418\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_169_0 execution finished in 11.03 seconds\n",
      "INFO:openlineage.client.client:OpenLineageClient will use `composite` transport\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Id': 1, 'SepalLengthCm': 5.1, 'SepalWidthCm': 3.5, 'PetalLengthCm': 1.4, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa'}\n",
      "{'Id': 2, 'SepalLengthCm': 4.9, 'SepalWidthCm': 3.0, 'PetalLengthCm': 1.4, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa'}\n",
      "{'Id': 3, 'SepalLengthCm': 4.7, 'SepalWidthCm': 3.2, 'PetalLengthCm': 1.3, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa'}\n"
     ]
    }
   ],
   "source": [
    "ds.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:15:57,477\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_171_0\n",
      "2026-01-22 01:15:57,495\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_171_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 01:15:57,495\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_171_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[Write]\n",
      "2026-01-22 01:15:57,515\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 01:15:57,516\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:57,518\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 01:15:57,520\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:57,521\tINFO progress_bar.py:213 -- === Ray Data Progress {Write} ===\n",
      "2026-01-22 01:15:57,521\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:57,522\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 01:15:57,523\tINFO progress_bar.py:215 -- Running Dataset: dataset_171_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:57,775\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_171_0 execution finished in 0.28 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n",
      "2026-01-22 01:15:57,837\tINFO dataset.py:5344 -- Data sink Parquet finished. 150 rows and 8.4KiB data written.\n"
     ]
    }
   ],
   "source": [
    "ds.write_parquet('/mnt/cluster_storage/parquet_iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b> \n",
    "\n",
    "We  write to `/mnt/cluster_storage`. This is a path that is available on all nodes in the cluster. If instead you use a path that is only local to one of the nodes in a multi-node cluster, you will see errors like `FileNotFoundError: [Errno 2] No such file or directory: '/path/to/file'`.\n",
    "\n",
    "This is because Ray Data is designed to work with distributed storage systems like S3, HDFS, etc. If you want to write to local storage, you can add a special prefix `local://` to the path. In this case, Ray will only schedule tasks on the head node of the cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\n",
      "-rw-r--r-- 1 ray users 3722 Jan 20 18:30 0_1679641afb914a4d941be4a3b7a7cf3c_000000_000000-0.parquet\n",
      "-rw-r--r-- 1 ray users 3722 Jan 21 21:07 0_cde6b538914747e9b1167b8cec70afff_000000_000000-0.parquet\n",
      "-rw-r--r-- 1 ray users 3722 Jan 21 22:51 12_7a92a40c35f840949915b0a824056735_000000_000000-0.parquet\n",
      "-rw-r--r-- 1 ray users 3722 Jan 20 20:28 164_953e278c878a459e80c7f9a52b83212b_000000_000000-0.parquet\n",
      "-rw-r--r-- 1 ray users 3722 Jan 22 01:15 167_140b2f9010b34264830054417b186873_000000_000000-0.parquet\n"
     ]
    }
   ],
   "source": [
    "! ls -l /mnt/cluster_storage/parquet_iris/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data[](https://docs.ray.io/en/latest/data/key-concepts.html#transforming-data \"Permalink to this headline\")\n",
    "\n",
    "Datasets can use either Ray tasks or Ray actors to transform datasets. Using actors allows for expensive state initialization (e.g., for GPU-based tasks) to be cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92e58da24bd48f5855564260b3617ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Repartition\n",
       "+- Dataset(num_rows=?, schema=Unknown schema)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.repartition(5)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:15:58,602\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_175_0\n",
      "2026-01-22 01:15:58,609\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_175_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 01:15:58,609\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_175_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> AllToAllOperator[Repartition] -> LimitOperator[limit=5]\n",
      "2026-01-22 01:15:58,641\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 01:15:58,642\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:58,643\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 01:15:58,644\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:58,646\tINFO progress_bar.py:213 -- === Ray Data Progress {Repartition} ===\n",
      "2026-01-22 01:15:58,646\tINFO progress_bar.py:215 -- Repartition: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; 0 rows output: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:58,647\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=5} ===\n",
      "2026-01-22 01:15:58,648\tINFO progress_bar.py:215 -- limit=5: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:58,648\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 01:15:58,649\tINFO progress_bar.py:215 -- Running Dataset: dataset_175_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:15:58,903\tINFO progress_bar.py:213 -- === Ray Data Progress {*- Split Repartition} ===\n",
      "2026-01-22 01:15:58,903\tINFO progress_bar.py:215 -- *- Split Repartition: Progress Completed 150 / 1\n",
      "2026-01-22 01:16:03,237\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_175_0 execution finished in 4.63 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Id': array([1, 2, 3, 4, 5]),\n",
       " 'SepalLengthCm': array([5.1, 4.9, 4.7, 4.6, 5. ]),\n",
       " 'SepalWidthCm': array([3.5, 3. , 3.2, 3.1, 3.6]),\n",
       " 'PetalLengthCm': array([1.4, 1.4, 1.3, 1.5, 1.4]),\n",
       " 'PetalWidthCm': array([0.2, 0.2, 0.2, 0.2, 0.2]),\n",
       " 'Species': array(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "        'Iris-setosa'], dtype=object)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.take_batch(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling Data[](https://docs.ray.io/en/latest/data/key-concepts.html#shuffling-data \"Permalink to this headline\")\n",
    "\n",
    "Certain operations like *sort* or *groupby* require data blocks to be partitioned by value, or *shuffled*. Datasets uses tasks to implement distributed shuffles in a map-reduce style, using map tasks to partition blocks by value, and then reduce tasks to merge co-partitioned blocks together.\n",
    "\n",
    "You can also change just the number of blocks of a Dataset using [`repartition()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\"). Repartition has two modes:\n",
    "\n",
    "1.  `shuffle=False` - performs the minimal data movement needed to equalize block sizes __(the default)__\n",
    "\n",
    "2.  `shuffle=True` - performs a full distributed shuffle __(not the default because it's more expensive)__\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-shuffle.svg' width=60%/>\n",
    "\n",
    "Datasets shuffle can scale to processing hundreds of terabytes of data. See the [Performance Tips Guide](https://docs.ray.io/en/latest/data/performance-tips.html#shuffle-performance-tips) for an in-depth guide on shuffle performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_batch(batch):\n",
    "    areas = []\n",
    "    for ix in range(len(batch['Id'])):\n",
    "        areas.append(batch[\"PetalLengthCm\"][ix] * batch[\"PetalWidthCm\"][ix])        \n",
    "    batch['approximate_petal_area'] = areas\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:16:03,338\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_177_0\n",
      "2026-01-22 01:16:03,344\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_177_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 01:16:03,346\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_177_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> AllToAllOperator[Repartition] -> LimitOperator[limit=20] -> TaskPoolMapOperator[MapBatches(transform_batch)]\n",
      "2026-01-22 01:16:03,367\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 01:16:03,368\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:03,369\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 01:16:03,371\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:03,372\tINFO progress_bar.py:213 -- === Ray Data Progress {Repartition} ===\n",
      "2026-01-22 01:16:03,373\tINFO progress_bar.py:215 -- Repartition: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; 0 rows output: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:03,375\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=20} ===\n",
      "2026-01-22 01:16:03,376\tINFO progress_bar.py:215 -- limit=20: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:03,376\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(transform_batch)} ===\n",
      "2026-01-22 01:16:03,378\tINFO progress_bar.py:215 -- MapBatches(transform_batch): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:03,380\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 01:16:03,381\tINFO progress_bar.py:215 -- Running Dataset: dataset_177_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:03,684\tINFO progress_bar.py:213 -- === Ray Data Progress {*- Split Repartition} ===\n",
      "2026-01-22 01:16:03,685\tINFO progress_bar.py:215 -- *- Split Repartition: Progress Completed 60 / 1\n",
      "2026-01-22 01:16:10,131\tINFO progress_bar.py:215 -- ListFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 1 / ?\n",
      "2026-01-22 01:16:10,132\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 150 / ?\n",
      "2026-01-22 01:16:10,133\tINFO progress_bar.py:215 -- Repartition: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; 150 rows output: Progress Completed 150 / 150\n",
      "2026-01-22 01:16:10,134\tINFO progress_bar.py:215 -- limit=20: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 1.1KiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:10,135\tINFO progress_bar.py:215 -- MapBatches(transform_batch): Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:10,136\tINFO progress_bar.py:215 -- Running Dataset: dataset_177_0. Active & requested resources: 0/32 CPU, 8.4KiB/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:10,183\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_177_0 execution finished in 6.83 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Id': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20]),\n",
       " 'SepalLengthCm': array([5.1, 4.9, 4.7, 4.6, 5. , 5.4, 4.6, 5. , 4.4, 4.9, 5.4, 4.8, 4.8,\n",
       "        4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1]),\n",
       " 'SepalWidthCm': array([3.5, 3. , 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3. ,\n",
       "        3. , 4. , 4.4, 3.9, 3.5, 3.8, 3.8]),\n",
       " 'PetalLengthCm': array([1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.6, 1.4,\n",
       "        1.1, 1.2, 1.5, 1.3, 1.4, 1.7, 1.5]),\n",
       " 'PetalWidthCm': array([0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1,\n",
       "        0.1, 0.2, 0.4, 0.4, 0.3, 0.3, 0.3]),\n",
       " 'Species': array(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "        'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "        'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "        'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "        'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa'],\n",
       "       dtype=object),\n",
       " 'approximate_petal_area': array([0.28, 0.28, 0.26, 0.3 , 0.28, 0.68, 0.42, 0.3 , 0.28, 0.15, 0.3 ,\n",
       "        0.32, 0.14, 0.11, 0.24, 0.6 , 0.52, 0.42, 0.51, 0.45])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.map_batches(transform_batch) \\\n",
    "  .take_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More example operations: transforming Datasets\n",
    "\n",
    "Datasets transformations take in datasets and produce new datasets. For example, *map* is a transformation that applies a user-defined function on each dataset record and returns a new dataset as the result. Datasets transformations can be composed to express a chain of computations.\n",
    "\n",
    "There are two main types of transformations:\n",
    "\n",
    "-   One-to-one: each input block will contribute to only one output block, such as [`ds.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\").\n",
    "\n",
    "-   All-to-all: input blocks can contribute to multiple output blocks, such as [`ds.random_shuffle()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\").\n",
    "\n",
    "Here is a table listing some common transformations supported by Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Ray Datasets transformations.[](https://docs.ray.io/en/latest/data/transforming-datasets.html#id2 \"Permalink to this table\")\n",
    "\n",
    "| Transformation | Type | Description |\n",
    "| --- | --- | --- |\n",
    "|[`ds.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches \"ray.data.Dataset.map_batches\")|One-to-one|Apply a given function to batches of records of this dataset.|\n",
    "|[`ds.add_column()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.add_column.html#ray.data.Dataset.add_column \"ray.data.Dataset.add_column\")|One-to-one|Apply a given function to batches of records to create a new column.|\n",
    "|[`ds.drop_columns()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.add_column.html#ray.data.Dataset.add_column \"ray.data.Dataset.add_column\")|One-to-one|Drop the given columns from the dataset.|\n",
    "|[`ds.split()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.split.html#ray.data.Dataset.split \"ray.data.Dataset.split\")|One-to-one|Split the dataset into N disjoint pieces.|\n",
    "|[`ds.repartition(shuffle=False)`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")|One-to-one|Repartition the dataset into N blocks, without shuffling the data.|\n",
    "|[`ds.repartition(shuffle=True)`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.repartition.html#ray.data.Dataset.repartition \"ray.data.Dataset.repartition\")|All-to-all|Repartition the dataset into N blocks, shuffling the data during repartition.|\n",
    "|[`ds.random_shuffle()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle \"ray.data.Dataset.random_shuffle\")|All-to-all|Randomly shuffle the elements of this dataset.|\n",
    "|[`ds.sort()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.sort.html#ray.data.Dataset.sort \"ray.data.Dataset.sort\")|All-to-all|Sort the dataset by a sortkey.|\n",
    "|[`ds.groupby()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.groupby.html#ray.data.Dataset.groupby \"ray.data.Dataset.groupby\")|All-to-all|Group the dataset by a groupkey.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution mode\n",
    "\n",
    "Most transformations are **lazy** in Ray Data - i.e. they don't execute until you either:\n",
    "- **write a dataset to storage**\n",
    "- explicitly **materialize** the data\n",
    "- **iterate over the dataset** (usually when feeding data to model training).\n",
    "\n",
    "To explicitly *materialize* a very small subset of the data, you can use the `take_batch` method.\n",
    "Most transformations are lazy. They don't execute until you consume a dataset or call [`Dataset.materialize()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.materialize.html#ray.data.Dataset.materialize \"ray.data.Dataset.materialize\").\n",
    "\n",
    "The transformations are executed in a streaming way, incrementally on the data and with operators processed in parallel. For an in-depth guide on Datasets execution, read https://docs.ray.io/en/releases-2.8.0/data/data-internals.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To force computation and local caching of the entire dataset, you can used `materialize`. Consider the performance constraints and impacts of caching, though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:16:10,251\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_178_0\n",
      "2026-01-22 01:16:10,255\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_178_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 01:16:10,256\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_178_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> AllToAllOperator[Repartition]\n",
      "2026-01-22 01:16:10,279\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 01:16:10,280\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:10,281\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 01:16:10,282\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:10,283\tINFO progress_bar.py:213 -- === Ray Data Progress {Repartition} ===\n",
      "2026-01-22 01:16:10,283\tINFO progress_bar.py:215 -- Repartition: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; 0 rows output: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:10,284\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 01:16:10,286\tINFO progress_bar.py:215 -- Running Dataset: dataset_178_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:10,880\tINFO progress_bar.py:213 -- === Ray Data Progress {*- Split Repartition} ===\n",
      "2026-01-22 01:16:10,881\tINFO progress_bar.py:215 -- *- Split Repartition: Progress Completed 150 / 1\n",
      "2026-01-22 01:16:10,898\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_178_0 execution finished in 0.64 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de88e1cf66c4a76a09511e3f23e9f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MaterializedDataset(\n",
       "   num_blocks=5,\n",
       "   num_rows=150,\n",
       "   schema={\n",
       "      Id: int64,\n",
       "      SepalLengthCm: double,\n",
       "      SepalWidthCm: double,\n",
       "      PetalLengthCm: double,\n",
       "      PetalWidthCm: double,\n",
       "      Species: string\n",
       "   }\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.materialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming data with actors\n",
    "\n",
    "When using the actor compute strategy, per-row and per-batch UDFs can also be *callable classes*, i.e. classes that implement the `__call__` magic method. The constructor of the class can be used for stateful setup, and will be only invoked once per worker actor.\n",
    "\n",
    "To implement this, you can use the `map_batches` API with a \"Callable\" class method that implements:\n",
    "\n",
    "- `__init__`: Initialize any expensive state.\n",
    "- `__call__`: Perform the stateful transformation.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> These transformation APIs take the uninstantiated callable class as an argument, not an instance of the class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:16:10,984\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_181_0\n",
      "2026-01-22 01:16:10,990\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_181_0. Full logs are in /tmp/ray/session_2026-01-21_22-31-19_329458_2360/logs/ray-data\n",
      "2026-01-22 01:16:10,991\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_181_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> AllToAllOperator[Repartition] -> LimitOperator[limit=10] -> ActorPoolMapOperator[MapBatches(ModelExample)]\n",
      "2026-01-22 01:16:11,159\tINFO progress_bar.py:213 -- === Ray Data Progress {ListFiles} ===\n",
      "2026-01-22 01:16:11,160\tINFO progress_bar.py:215 -- ListFiles: Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:11,160\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadFiles} ===\n",
      "2026-01-22 01:16:11,161\tINFO progress_bar.py:215 -- ReadFiles: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:11,163\tINFO progress_bar.py:213 -- === Ray Data Progress {Repartition} ===\n",
      "2026-01-22 01:16:11,164\tINFO progress_bar.py:215 -- Repartition: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; 0 rows output: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:11,165\tINFO progress_bar.py:213 -- === Ray Data Progress {limit=10} ===\n",
      "2026-01-22 01:16:11,165\tINFO progress_bar.py:215 -- limit=10: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:11,166\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(ModelExample)} ===\n",
      "2026-01-22 01:16:11,166\tINFO progress_bar.py:215 -- MapBatches(ModelExample): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-22 01:16:11,167\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-22 01:16:11,168\tINFO progress_bar.py:215 -- Running Dataset: dataset_181_0. Active & requested resources: 0/32 CPU, 0.0B/26.9GiB object store (pending: 2 CPU): Progress Completed 0 / ?\n",
      "2026-01-22 01:16:11,611\tINFO progress_bar.py:213 -- === Ray Data Progress {*- Split Repartition} ===\n",
      "2026-01-22 01:16:11,612\tINFO progress_bar.py:215 -- *- Split Repartition: Progress Completed 150 / 1\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py:416: UserWarning: The minimum number of concurrent actors for 'MapBatches(ModelExample)' is set to 2, but the operator only received 1 input(s). This means that the operator can launch at most 1 task(s), and won't fully utilize the available concurrency. You might be able to increase the number of concurrent tasks by configuring `override_num_blocks` earlier in the pipeline.\n",
      "  warnings.warn(\n",
      "2026-01-22 01:16:15,941\tINFO streaming_executor.py:305 -- ✔️  Dataset dataset_181_0 execution finished in 4.95 seconds\n",
      "INFO:openlineage.client.transport.composite:Stopping OpenLineage CompositeTransport emission after the first successful delivery because `continue_on_success=False`. Transport that emitted the event: <HttpTransport(name=first, kind=http, priority=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Id': 1, 'SepalLengthCm': 5.1, 'SepalWidthCm': 3.5, 'PetalLengthCm': 1.4, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa', 'predictions': 0.29730177875068026}\n",
      "{'Id': 2, 'SepalLengthCm': 4.9, 'SepalWidthCm': 3.0, 'PetalLengthCm': 1.4, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa', 'predictions': 0.29730177875068026}\n",
      "{'Id': 3, 'SepalLengthCm': 4.7, 'SepalWidthCm': 3.2, 'PetalLengthCm': 1.3, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa', 'predictions': 0.29730177875068026}\n",
      "{'Id': 4, 'SepalLengthCm': 4.6, 'SepalWidthCm': 3.1, 'PetalLengthCm': 1.5, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa', 'predictions': 0.29730177875068026}\n",
      "{'Id': 5, 'SepalLengthCm': 5.0, 'SepalWidthCm': 3.6, 'PetalLengthCm': 1.4, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa', 'predictions': 0.29730177875068026}\n",
      "{'Id': 6, 'SepalLengthCm': 5.4, 'SepalWidthCm': 3.9, 'PetalLengthCm': 1.7, 'PetalWidthCm': 0.4, 'Species': 'Iris-setosa', 'predictions': 0.5356999058252557}\n",
      "{'Id': 7, 'SepalLengthCm': 4.6, 'SepalWidthCm': 3.4, 'PetalLengthCm': 1.4, 'PetalWidthCm': 0.3, 'Species': 'Iris-setosa', 'predictions': 0.40903897192829974}\n",
      "{'Id': 8, 'SepalLengthCm': 5.0, 'SepalWidthCm': 3.4, 'PetalLengthCm': 1.5, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa', 'predictions': 0.29730177875068026}\n",
      "{'Id': 9, 'SepalLengthCm': 4.4, 'SepalWidthCm': 2.9, 'PetalLengthCm': 1.4, 'PetalWidthCm': 0.2, 'Species': 'Iris-setosa', 'predictions': 0.29730177875068026}\n",
      "{'Id': 10, 'SepalLengthCm': 4.9, 'SepalWidthCm': 3.1, 'PetalLengthCm': 1.5, 'PetalWidthCm': 0.1, 'Species': 'Iris-setosa', 'predictions': 0.2011893487492697}\n"
     ]
    }
   ],
   "source": [
    "class ModelExample:\n",
    "    def __init__(self):\n",
    "        expensive_model_weights = [ 0.3, 1.75 ]\n",
    "        self.complex_model = lambda petal_width: (petal_width + expensive_model_weights[0])  ** expensive_model_weights[1]\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch[\"predictions\"] = self.complex_model(batch[\"PetalWidthCm\"])\n",
    "        return batch\n",
    "\n",
    "ds.map_batches(ModelExample, \n",
    "               compute=ray.data.ActorPoolStrategy(size=2)) \\\n",
    "  .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset details\n",
    "\n",
    "A Dataset consists of a list of Ray object references to *blocks*. Having multiple blocks in a dataset allows for parallel transformation and ingest.\n",
    "\n",
    "The following figure visualizes a tabular dataset with three blocks, each block holding 1000 rows each:\n",
    "\n",
    "|<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-intro/block.png\" width=\"700px\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|A Dataset when materialized is a distributed collection of blocks. This example illustrates a materialized dataset with three blocks, each block holding 1000 rows.|\n",
    "\n",
    "Since a Dataset is just a list of Ray object references, it can be freely passed between Ray tasks, actors, and libraries like any other object reference. This flexibility is a unique characteristic of Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "__Lab activity: Stateless transformation__\n",
    "    \n",
    "1. Create a Ray Dataset from the iris data in `/mnt/cluster_storage/parquet_iris`\n",
    "1. Create a \"sum of features\" transformation that calculates the sum of the Sepal Length, Sepal Width, Petal Length, and Petal Width features for the records\n",
    "    1. Design this transformation to take a Ray Dataset *batch* of records\n",
    "    1. Return the records without the ID column but with an additional column called \"sum\"\n",
    "    1. Hint: you do not need to use NumPy, but the calculation may be easier/simpler to code using NumPy vectorized operations with the records in the batch\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "__Lab activity: Stateful transformation__\n",
    "    \n",
    "1. Create a Ray Dataset from the iris data in `/mnt/cluster_storage/parquet_iris`\n",
    "1. Create an class that makes predictions on iris records using these steps:\n",
    "    1. in the class constructor, create an instance of the following \"model\" class:\n",
    "        ```python\n",
    "\n",
    "          class SillyModel():\n",
    "\n",
    "              def predict(self, petal_length):\n",
    "                  return petal_length + 0.42\n",
    "\n",
    "\n",
    "        ```\n",
    "    1. in the `__call__` method of the actor class\n",
    "        1. take a batch of records\n",
    "        1. create predictions for each record in the batch using the model instance\n",
    "            1. Hint: the code may be simpler using NumPy vectorized operations\n",
    "        1. add the predictions to the record batch\n",
    "        1. return the new, augmented batch\n",
    "1. Use that class to perform batch inference on the dataset using actors\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
